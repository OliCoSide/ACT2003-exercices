\chapter{\textsf{R} et la régression linéaire}
\index{regression@régression|(}
\label{chap:regression}


Comme tous les grands logiciels statistiques --- et même plusieurs
calculatrices scientifiques --- \textsf{R} comporte des fonctions
permettant de calculer les coefficients d'une régression simple ou
multiple. Les outils disponibles vont toutefois bien au-delà de ce
calcul relativement simple. Ainsi, par l'entremise de quelques
fonctions génériques simples à utiliser, il est possible de générer
différents graphiques relatifs à la régression, d'en calculer le
tableau ANOVA et d'en extraire les informations principales, de
calculer des prévisions ainsi que des intervalles de confiance. Bref,
l'analyse complète d'un ensemble de données tient en quelques lignes
de code; il suffit de connaître les fonctions à utiliser.

Cette annexe présente les principales fonctions --- dont la liste se
trouve au tableau \ref{tab:regression:fonctions} --- utiles lors de
l'analyse de données et la modélisation par régression. Il n'a
cependant aucune prétention d'exhaustivité. Consulter l'aide en ligne
de \textsf{R}, ainsi que \citet{MASS} pour de plus amples détails.

\begin{table}[t]
  \centering
  \begin{threeparttable}
    \begin{tabular}{ll}
      \toprule
      \textbf{Phase de l'analyse} & \textbf{Fonctions} \\
      \midrule
      Création et manipulation de \emph{data frames}
        & \fonction{data.frame} \\
        & \fonction{as.data.frame} \\
        & \fonction{read.table} \\
        & \fonction{cbind} \\
        & \fonction{rbind} \\
        & \fonction{names}, \fonction{colnames} \\
        & \fonction{row.names}, \fonction{rownames} \\
        & \fonction{attach} \\
        & \fonction{detach} \\
      \midrule
      Modélisation
        & \fonction{lm} \\
        & \fonction{add1}, \fonction{addterm}\tnote{1} \\
        & \fonction{drop1}, \fonction{dropterm}\tnote{1} \\
        & \fonction{step}, \fonction{stepAIC}\tnote{1} \\
      \midrule
      Analyse des résultats et diagnostics
        & \fonction{summary} \\
        & \fonction{anova} \\
        & \fonction{coef}, \fonction{coefficients} \\
        & \fonction{confint} \\
        & \fonction{residuals} \\
        & \fonction{fitted} \\
        & \fonction{deviance} \\
        & \fonction{df.residual} \\
      \midrule
      Mise à jour et prévisions
        & \fonction{update} \\
        & \fonction{predict} \\
      \midrule
      Graphiques
        & \fonction{plot} \\
        & \fonction{abline} \\
        & \fonction{matplot} \\
        & \fonction{matlines} \\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item[1] Dans le package \texttt{MASS}\index{package!MASS@\texttt{MASS}}.
    \end{tablenotes}
  \end{threeparttable}
  \caption{Principales fonctions \textsf{R} pour la
    régression linéaire}
  \label{tab:regression:fonctions}
\end{table}


\section{Importation de données}
\index{regression@régression!importation de données}
\label{chap:regression:importation}

La modélisation statistique en \textsf{R} --- par exemple, l'analyse
de régression --- repose souvent sur l'utilisation de \emph{data
  frames}\index{data frame} pour le stockage des données. On se
reportera à la section 2.7 de {\shorthandoff{:}
  \citet{Goulet:introS:2007}} pour une présentation de ce type
d'objet.

La principale fonction utilisée pour importer des données dans
\textsf{R} en vue d'une analyse de régression est
\Fonction{read.table}. Celle-ci retourne un \emph{data frame}. Les
arguments de \code{read.table} les plus souvent utilisés sont:
\begin{ttscript}{comment.char}
\item[\code{file}] le nom ou l'URL du fichier de données à importer;
\item[\code{header}] \texttt{TRUE} si la première ligne du fichier à être lue
  contient les étiquettes des colonnes;
\item[\code{comment.char}] le caractère (\texttt{\#} par défaut)
  représentant le début d'un commentaire dans le fichier;
\item[\code{skip}] le nombre de lignes à sauter au début du fichier.
\end{ttscript}


\section{Formules}
\index{regression@régression!formules}
\label{chap:regression:formules}

Lorsque l'on fait une régression, il faut informer \textsf{R} des
variables que l'on entend inclure dans celle-ci et leurs relations
entre elles. La convention utilisée dans le langage S est celle dite
des «formules»\Index{formule}. Le tableau \ref{tab:formules} présente
quelques exemples de formulation de modèles linéaires simples en S.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Modèle mathématique} & \textbf{Formule S} \\
    \midrule
    $y_t = \alpha + \beta x_t + \varepsilon_t$
      & \verb=y ~ x= \\
      & \verb=y ~ 1 + x= \\
    \midrule
    $y_t = \beta x_t + \varepsilon_t$
      & \verb=y ~ -1 + x= \\
      & \verb=y ~ x - 1= \\
    \midrule
    $y_t = \beta_0 + \beta_1 x_{t1} + \beta_2 x_{t2} + \varepsilon_t$
      & \verb=y ~ x1 + x2= \\
      & \verb=y ~ x= où \verb=x <- cbind(x1, x2)= \\
    \bottomrule
  \end{tabular}
  \caption{Modèles linéaires simples et leur formulation en S}
  \label{tab:formules}
\end{table}

Pour une utilisation de base des fonctions de régression, la
connaissance des règles suivantes suffit.
\begin{enumerate}
\item Les opérateurs \code{+} et \code{-} prennent une nouvelle
  signification dans les formules: \fonction{+} signifie «inclusion»
  et \fonction{-}, «exclusion».
\item Le terme constant d'une régression est inclus implicitement.
  Pour l'exclure explicitement (pour la régression passant par
  l'origine), il faut donc ajouter un terme \code{-1} du côté droit de
  la formule.
\item Dans une régression multiple, on peut soit lister toutes les
  variables à inclure du côté droit de la formule, soit ne spécifier
  qu'une matrice contenant ces variables (dans les colonnes).
\end{enumerate}

Consulter les sections 6.2 de \citet{MASS} et 11.1 de
{\shorthandoff{:} \citet{R:intro}} pour plus de détails.


\section{Modélisation des données}
\index{regression@régression!modélisation}
\label{chap:regression:modelisation}

Supposons que l'on souhaite étudier la relation entre la variable
indépendante \code{x1} et la variable dépendante (ou réponse)
\code{y1} du jeu de données \texttt{anscombe}. La première étape de la
modélisation des données en régression linéaire simple consiste
habituellement à représenter celles-ci graphiquement.

La fonction \fonction{plot} est une fonction générique comportant des
méthodes pour un grand nombre de classes d'objets différentes.
Puisqu'il existe une méthode pour les objets de classe
\classe{formula}, on peut tracer un graphique de \code{y1} en fonction
de \code{x1} avec
<<echo=TRUE, eval=FALSE>>=
plot(y1 ~ x1, data = anscombe)
@
ou, si les colonnes du \emph{data frame} \texttt{anscombe} sont visibles,
simplement avec
<<echo=TRUE, eval=FALSE>>=
plot(y1 ~ x1)
@
Le résultat de ces commandes se trouve à la figure \ref{fig:relation}.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
plot(y1 ~ x1, data=anscombe)
@
  \caption{Relation entre \code{y1} et \code{x1} des données
    \texttt{anscombe}}
  \label{fig:relation}
\end{figure}

Le graphique nous montre qu'il est raisonnable de postuler une
relation linéaire entre les éléments de \code{y1} et \code{x1}. On
pose donc le modèle
\begin{displaymath}
  y_t = \beta_0 + \beta_1 x_t + \varepsilon_t,
\end{displaymath}
où $y_t$ et $x_t$, $t = 1, \dots, 11$ sont les éléments des vecteurs
\code{y1} et \code{x1}, respectivement, et $\varepsilon_t$ est le
terme d'erreur.

C'est avec la fonction \Fonction{lm} (pour \emph{linear model}) que
l'on calcule les estimateurs des coefficients de la régression
$\beta_0$ et $\beta_1$. De façon simplifiée, cette fonction prend en
arguments une formule et un \emph{data frame} comprenant les données
relatives aux termes de la formule. La fonction \code{lm} retourne un
objet de classe \classe{lm}, classe pour laquelle il existe de
nombreuses méthodes.
<<echo=TRUE>>=
( fit <- lm(y1 ~ x1, data=anscombe) )
class(fit)
@

Lorsque plusieurs variables explicatives sont disponibles, l'analyste
doit souvent choisir les variables les plus significatives pour la
régression. Les techniques d'élimination successive, de sélection
successive et de sélection pas à pas, qui reposent toutes sur les
tests $F$ partiels, sont alors populaires pour parvenir au modèle le
plus utile. Ces techniques sont mises en {\oe}uvre, respectivement,
dans les fonctions \Fonction{dropterm}, \Fonction{addterm} et
\Fonction{stepAIC} du package
\texttt{MASS}\index{package!MASS@\texttt{MASS}} \citep{MASS}.


\section{Analyse des résultats}
\label{chap:regression:analyse}

Le résultat de la fonction \fonction{lm} est une liste dont on peut
extraire manuellement les différents éléments (consulter la rubrique
d'aide). Grâce à quelques fonctions génériques disposant d'une méthode
pour les objets de classe \classe{lm}, il est toutefois facile et
intuitif d'extraire les principaux résultats d'une régression:
\begin{enumerate}
\item \fonction{coef} ou \fonction{coefficients} extraient les
  coefficients $\hat{\beta}_0$ et $\hat{\beta}_1$ de la régression;
\item \fonction{fitted} extrait les valeurs ajustées $\hat{y}_t =
  \hat{\beta}_0 + \hat{\beta}_1 x_t$;
\item \fonction{residuals} extrait les résidus $y_t - \hat{y}_t$;
\item \fonction{deviance} retourne la somme des carrés des résidus
  $\mathrm{SSR} = \sum_{t = 1}^n (y_t - \hat{y}_t)^2$;
\item \fonction{df.residual} extrait le nombre de degrés de liberté
  de la somme des carrés des résidus.
\end{enumerate}

La fonction générique \fonction{summary} présente les informations
ci-dessus de manière facile à consulter. Plus précisément, le sommaire
de la régression contient, outre le modèle utilisé et les estimateurs
des coefficients de la régression: les résultats des tests $t$, la
valeur du coefficient de détermination
\begin{displaymath}
  R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n
    (y_i - \bar{y})^2}
\end{displaymath}
et celle du coefficient de détermination ajusté
\begin{displaymath}
  R_a^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1},
\end{displaymath}
ainsi que le résultat du test $F$ global.

La fonction \Fonction{confint} calcule les intervalles de confiance
des paramètres de la régression.

D'autre part, le tableau d'analyse de variance (séquentiel, en
régression multiple) est calculé avec la fonction générique
\fonction{anova}.

Pour ajouter la droite de régression au graphique créé au début de
l'analyse, utiliser la fonction \fonction{abline}, qui dispose elle
aussi d'une méthode pour les objets de classe \code{lm}.


\section{Diagnostics}
\index{regression@régression!diagnostics}
\label{chap:regression:diagnostics}

Les statistiques servant à mesurer la qualité d'un modèle de
régression ($R^2$, $R^2$ ajusté, statistiques $t$ et $F$) sont
calculées par les fonctions \fonction{summary} et \fonction{anova}.

La méthode de la fonction \fonction{plot} pour les objets de classe
\classe{lm} produit une série de six graphiques (quatre dans
\textsf{R} avant la version 2.2.0) permettant de juger de la qualité
d'une régression.  Consulter la rubrique d'aide de la fonction
\fonction{plot.lm} pour plus de détails.


\section{Mise à jour des résultats et prévisions}
\index{regression@régression!prévisions}
\label{chap:regression:prevision}

Il peut arriver que, une fois la modélisation d'un ensemble de données
effectuée, l'on doive ajouter ou modifier une ou plusieurs données ou
variables. Plutôt que de reprendre toute la modélisation avec la
fonction \code{lm}, il peut alors s'avérer plus simple et élégant
d'utiliser la fonction \fonction{update}:
<<echo=TRUE>>=
update(fit, ~ . + x4)
@

Le calcul de prévisions et d'intervalles de confiance pour la
régression et pour les prévisions se fait avec la fonction générique
\fonction{predict} et sa méthode pour les objets de classe \code{lm}.
Par défaut, \fonction{predict} calculera les prévisions pour les
valeurs $x_t$, $t = 1, \dots, n$.  Par conséquent, le résultat de
\fonction{predict} sera le même que celui de \fonction{fitted}:
<<echo=TRUE>>=
all.equal(predict(fit), fitted(fit))
@
Comme on souhaite généralement prévoir la réponse pour d'autres
valeurs de la variable indépendante, on spécifiera celles-ci par le
biais d'un \emph{data frame} passé à \fonction{predict} avec l'option
\argument{newdata}.

La fonction \fonction{predict} peut également servir au calcul des
bornes d'intervalles de confiance et de prévision. Pour calculer les
bornes d'un intervalle de confiance, on ajoutera l'argument
\verb!interval = "confidence"!\indexargument{interval}, alors que pour
les bornes d'un intervalle de prévision on utilise
\verb!interval = "prediction"!. Le niveau de confiance est déterminé
avec l'argument \argument{level} ($0,95$ par défaut). Le résultat est
une matrice de trois colonnes dont la première contient les prévisions
et les deux autres les bornes inférieures (\code{lwr}) et supérieures
(\code{upr}) des intervalles de confiance.

On ajoute les limites des intervalles de confiance au graphique des
données avec les fonctions \fonction{matlines} ou \fonction{matplot}.
Consulter les rubriques d'aide et les exemples pour de plus amples
détails.

\index{regression@régression|)}


\section{Exemples}
\label{chap:regression:exemples}

\lstinputlisting{regression.R}


\section{Exercices}
\label{chap:regression:exercices}

\Opensolutionfile{reponses}[reponses-regression]
\begin{Filesave}{reponses}
\section*{Annexe \ref{chap:regression}}
\addcontentsline{toc}{section}{Annexe \protect\ref{chap:regression}}

\end{Filesave}

\begin{exercice}
  Importer dans S-Plus ou \textsf{R} le jeu de données
  \texttt{steam.dat} à l'aide de la fonction \fonction{read.table}.
  Les trois première lignes du fichier sont des lignes de commentaires
  débutant par le caractère \code{\#}. La quatrième ligne contient les
  étiquettes des colonnes.
  \begin{rep}
<<eval=FALSE>>=
steam <- read.table("http://vgoulet.act.ulaval.ca/pub/data/steam.dat",
                    header = TRUE)
@
  \end{rep}
\end{exercice}

\begin{exercice}
  Rendre les colonnes individuelles de l'ensemble de données
  \texttt{steam} visibles dans l'espace de travail.
  \begin{rep}
<<eval=FALSE>>=
attach(steam)
@
  \end{rep}
\end{exercice}

\begin{exercice}
  Faire (même à l'aveuglette) l'analyse de régression de la variable
  \code{Y} en fonction de la variable \code{X1} des données
  \texttt{steam}.
  \begin{enumerate}
  \item Évaluer visuellement le type de relation pouvant exister entre
    \code{Y} et \code{X1}.
  \item Évaluer les coefficients d'une régression linéaire entre
    \code{Y} et \code{X1} et ajouter la droite de régression ainsi
    obtenue au graphique créé en a).
  \item Répéter la partie b) en forçant la droite de régression à
    passer par l'origine $(0, 0)$. Quel modèle semble le plus
    approprié?
  \item Le coefficient de détermination $R^2$ mesure la qualité de
    l'ajustement d'une droite de régression aux données. Calculer le
    $R^2$ pour les modèles en b) et c).  Obtient-on les mêmes
    résultats que ceux donnés par \fonction{summary}? Semble-t-il y
    avoir une anomalie?
  \item Calculer les bornes d'intervalles de confiance pour la droite
    de régression des deux modèles.
  \item Calculer les prévisions de chaque modèle pour toutes les
    valeurs de \texttt{X1} ainsi que les bornes d'intervalles de
    confiance pour ces prévisions.
  \item Ajouter au graphique créé précédemment les bornes inférieures
    et supérieures des intervalles de confiance calculées en e) et f).
    Utiliser des types de lignes (option \code{lty}) et des couleurs
    (option \code{col}) différents pour chaque ensemble de limites.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item
<<eval=FALSE>>=
plot(Y ~ X1)
@
    \item
<<eval=FALSE>>=
fit <- lm(Y ~ X1)
coef(fit)
abline(fit)
@
   \item
<<eval=FALSE>>=
fit.orig <- lm(Y ~ X1 - 1)
coef(fit.orig)
abline(fit.orig)
@
   \item
<<eval=FALSE>>=
summary(fit)$r.squared
summary(fit.orig)$r.squared
@
   \item
<<eval=FALSE>>=
pred.ci <- predict(fit, interval = "confidence")
pred.ci.orig <- predict(fit.orig, interval = "confidence")
@
   \item
<<eval=FALSE>>=
pred.pi <- predict(fit, newdata = data.frame(X1 = steam$X1),
                   interval = "prediction")
pred.pi.orig <- predict(fit.orig, newdata = data.frame(X1 = steam$X1),
                        interval = "prediction")
@
   \item
<<eval=FALSE>>=
par(mfrow = c(1, 2))
ord <- order(X1)
plot(Y ~ X1)
matplot(X1[ord], cbind(pred.ci, pred.pi[, -1])[ord,],
        type = "l", lty = c(1, 2, 2, 3, 3), lwd = 2,
        col = c("blue", "red", "red", "orange", "orange"),
        add = TRUE)
plot(Y ~ X1)
matplot(X1[ord], cbind(pred.ci.orig, pred.pi.orig[, -1])[ord,],
        type = "l", lty = c(1, 2, 2, 3, 3), lwd = 2,
        col = c("blue", "red", "red", "orange", "orange"),
        add = TRUE)
@
    \end{enumerate}
  \end{rep}
\end{exercice}

\begin{exercice}
  Répéter l'exercice précédent en ajoutant la variable \code{X5}
  à l'analyse, transformant ainsi le modèle de régression linéaire
  simple en un modèle de régression multiple.
\end{exercice}


\Closesolutionfile{reponses}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
