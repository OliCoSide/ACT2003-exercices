\chapter{Éléments d'algèbre matricielle}
\label{chap:elements}

Cette annexe présente quelques résultats d'algèbre matricielle utiles
en régression linéaire.

\section{Trace}

La \emph{trace} d'une matrice est la somme des éléments de la diagonale.

\begin{thm}
  \label{thm:elements:trace}
  Soient $\mat{A} = [a_{ij}]$ et $\mat{B} = [b_{ij}]$ des matrices
  carrées $k \times k$. Alors
  \begin{enumerate}[a)]
  \item $\tr(\mat{A}) = \sum_{i=1}^k a_{ii}$
  \item $\tr(\mat{A} + \mat{B}) = \tr(\mat{A}) + \tr(\mat{B})$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{thm:elements:symetrie_trace}
  Soient les matrices $\mat{A}_{p \times q}$ et $\mat{B}_{q \times
    p}$. Alors $\tr(\mat{AB}) = \tr(\mat{BA})$.
\end{thm}

\enlargethispage{\baselineskip}
\begin{proof}
  Posons $\mat{C} = \mat{AB}$ et $\mat{D} = \mat{BA}$. Par définition
  du produit matriciel, l'élément $c_{ij}$ de la matrice $\mat{C}$ est
  égal au produit scalaire entre la ligne $i$ de $\mat{A}$ et de la
  colonne $j$ de $\mat{B}$, soit
  \begin{displaymath}
    c_{ij} = \sum_{k=1}^q a_{ik} b_{kj}.
  \end{displaymath}
  Les éléments de la diagonale de $\mat{C}$ sont donc $c_{ii} =
  \sum_{j=1}^q a_{ij} b_{ji}$ et, par symétrie, ceux de la diagonale
  de $\mat{D}$ sont $d_{jj} = \sum_{i=1}^p b_{ji} a_{ij}$.  Or,
  \begin{align*}
    \tr(\mat{C})
    &= \sum_{i=1}^p c_{ii} \\
    &= \sum_{i=1}^p \sum_{j=1}^q a_{ij} b_{ji} \\
    &= \sum_{j=1}^q \sum_{i=1}^p b_{ji} a_{ij} \\
    &= \sum_{j=1}^p d_{jj} \\
    &= \tr(\mat{D}).
  \end{align*}
\end{proof}


\section{Formes quadratiques et dérivées}

Soit $\mat{A} = [a_{ij}]$ une matrice $k \times k$ symétrique et
$\mat{x} = (x_1, \dots, x_k)^\prime$ un vecteur. Alors
\begin{displaymath}
  \mat{x^\prime A x} = \sum_{i=1}^k \sum_{j=1}^k a_{ij} x_i x_j
\end{displaymath}
est une forme quadratique.

Par exemple, si
\begin{displaymath}
  \mat{x} =
  \begin{bmatrix}
    x_1 \\
    x_2
  \end{bmatrix}
  \quad
  \text{et}
  \quad
  \mat{A} =
  \begin{bmatrix}
    a_{11} & a_{12} \\
    a_{12} & a_{22}
  \end{bmatrix},
\end{displaymath}
alors
\begin{align*}
  \mat{x^\prime Ax}
  &= \sum_{i=1}^2 \sum_{j=1}^2 a_{ij} x_i x_j \\
  &= a_{11} x_1^2 + 2 a_{12} x_1 x_2 + a_{22} x_2^2.
\end{align*}

\begin{rem}
  Si $\mat{A}$ est diagonale, $\mat{x^\prime Ax} = \sum_{i=1}^k a_{ii} x_i^2$.
\end{rem}

\begin{thm}
  \label{thm:elements:derivee_produit_scalaire}
  Soient $\mat{x} = (x_1, \dots, x_k)^\prime$ et $\mat{a} = (a_1,
  \dots, a_k)^\prime$, d'où $\mat{x^\prime a} = a_1 x_1 + \dots + a_k x_k =
  \sum_{i=1}^k a_i x_i$. Alors
  \begin{align*}
    \frac{d}{d \mat{x}}\, \mat{x^\prime a}
    &= \frac{d}{d \mat{x}} \sum_{i=1}^k a_i x_i \\
    &=
    \begin{bmatrix}
      \frac{d}{d x_1}\, \sum_{i=1}^k a_i x_i \\
      \vdots \\
      \frac{d}{d x_k}\, \sum_{i=1}^k a_i x_i
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      a_1 \\
      \vdots \\
      a_k
    \end{bmatrix} \\
    &= \mat{a}.
  \end{align*}
\end{thm}

\begin{thm}
  \label{thm:elements:derivee_forme_quadratique}
  Soit $\mat{A}_{k \times k}$ une matrice symétrique. Alors
  \begin{displaymath}
    \frac{d}{d \mat{x}}\, \mat{x^\prime A x} = 2 \mat{A x}.
  \end{displaymath}
\end{thm}

\begin{proof}
  On a
  \begin{align*}
    \mat{x^\prime A x}
    &= \sum_{i=1}^k \sum_{j=1}^k a_{ij} x_i x_j \\
    &= \sum_{i=1}^k a_{ii} x_i^2 +
       \sum_{i=1}^k \sum_{\overset{j=1}{j \neq i}}^k a_{ij} x_i x_j.
  \end{align*}
  Par conséquent, pour $t = 1, \dots, k$ et puisque $a_{ij} = a_{ji}$,
  \begin{align*}
    \frac{\partial}{\partial x_t}\, \mat{x^\prime A x}
    &= 2 a_{tt} x_t + \sum_{\overset{i=1}{i \neq t}}^k a_{it} x_i +
       \sum_{\overset{j=1}{j \neq t}}^k a_{tj} x_j \\
    &= 2 \sum_{i=1}^k a_{it} x_t, \\
    \intertext{d'où}
    \frac{d}{d \mat{x}}\, \mat{x^\prime A x}
    &= 2 \mat{A x}.
  \end{align*}
\end{proof}

\begin{thm}
  \label{thm:elements:derivee_fonction}
  Si $f(\mat{x})$ est une fonction quelconque du vecteur $\mat{x}$, alors
  \begin{displaymath}
    \frac{d}{d \mat{x}}\, f(\mat{x})^\prime \mat{A} f(\mat{x}) =
    2 \left( \frac{d}{d \mat{x}} f(\mat{x}) \right)^\prime \mat{A} f(\mat{x}).
  \end{displaymath}
\end{thm}

Vérifier en exercice les résultats ci-dessus pour une matrice
$\mat{A}$ $3 \times 3$.


\section{Vecteurs et matrices aléatoires}

Soit $X_1, \dots, X_n$ des variables aléatoires. Alors
\begin{displaymath}
  \mat{x} =
  \begin{bmatrix}
    X_1 \\
    \vdots \\
    X_n
  \end{bmatrix}
\end{displaymath}
est un \emph{vecteur aléatoire}. On définit le vecteur espérance
\begin{align*}
  \esp{\mat{x}}
  &=
  \begin{bmatrix}
    \esp{X_1} \\
    \vdots \\
    \esp{X_n}
  \end{bmatrix}
\end{align*}
et la matrice de variance-covariance
\begin{align*}
  \varmat{\mat{x}}
  &= \esp{(\mat{x} - \esp{\mat{x}})(\mat{x} - \esp{\mat{x}})^\prime } \\
  &=
  \begin{bmatrix}
    \var{X_1} & \dots & \Cov(X_1, X_n) \\
    \vdots    & \ddots & \vdots \\
    \Cov(X_n, X_1) & \dots & \var{X_n}
  \end{bmatrix}
\end{align*}

\begin{thm}
  \label{thm:elements:esp_var}
  Soit $\mat{x}$ un vecteur aléatoire et $\mat{A}$ une matrice de
  constantes. Alors
  \begin{enumerate}[a)]
  \item $\esp{\mat{Ax}} = \mat{A} \esp{\mat{x}}$
    \label{thm:elements:esp_var:esp}
  \item $\varmat{\mat{Ax}} = \mat{A} \varmat{\mat{x}} \mat{A}^\prime $.
    \label{thm:elements:esp_var:var}
  \end{enumerate}
\end{thm}

\begin{proof}[Démonstration de b)]
  \begin{align*}
    \varmat{\mat{Ax}}
    &= \esp{(\mat{Ax} - \esp{\mat{Ax}})(\mat{Ax} - \esp{\mat{Ax}})^\prime } \\
    &= \esp{\mat{A}(\mat{x} - \esp{\mat{x}})(\mat{x} - \esp{\mat{x}})^\prime
       \mat{A}^\prime } \\
    &= \mat{A} \varmat{\mat{x}} \mat{A}^\prime .
  \end{align*}
\end{proof}

\begin{exemple}
  Soit $\mat{A} = [1\; 1]$, $\mat{x}^\prime = [X_1\; X_2]$ et $Y =
  \mat{Ax}$, donc $Y = X_1 + X_2$. Alors
  \begin{align*}
    \esp{Y}
    &= \mat{A} \esp{\mat{x}} \\
    &= [1\:\: 1]
    \begin{bmatrix}
    \esp{X_1} \\
    \esp{X_2}
    \end{bmatrix} \\
    &= \esp{X_1} + \esp{X_2}
    \intertext{et}
    \varmat{Y}
    &= \mat{A} \varmat{\mat{x}} \mat{A}^\prime \\
    &= [1\:\: 1]
    \begin{bmatrix}
      \var{X_1} & \Cov(X_1, X_2) \\
      \Cov(X_2, X_1) & \var{X_2}
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      1
    \end{bmatrix} \\
    &= \var{X_1} + \var{X_2} + 2\, \Cov(X_1, X_2).
  \end{align*}
\end{exemple}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
