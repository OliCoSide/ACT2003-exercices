\chapter{Modèles linéaires généralisés (GLM)}
\label{chap:glm}

\Opensolutionfile{reponses}[reponses-glm]
\Opensolutionfile{solutions}[solutions-glm]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:glm}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:glm}}

\end{Filesave}


\begin{exercice}
Est-ce que les distributions suivantes font partie de la famille exponentielle linéaire? Si oui, écrire la densité sous la forme exponentielle linéaire, donner le paramètre canonique, le paramètre de dispersion, l'espérance et la variance de $Y$ en termes de la fonction $b()$ et la relation $V()$ entre la moyenne et la variance.

\begin{enumerate}
\item Normale$(\mu,\sigma^2)$
\item Uniforme$(0,\beta)$
\item Poisson$(\lambda)$
\item Bernoulli$(\pi)$
\item Binomiale$(m, \pi)$, $m>0$ est un entier et est connu (On considère $Y^*=Y/m$).
\item Pareto$(\alpha,\lambda)$
\item Gamma$(\alpha,\beta)$
\item Binomiale négative$(r,\pi)$ avec $r$ connu (On considère $Y^*=Y/r$).
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Normale$(\mu,\sigma^2)$: oui, 
\begin{align*}
f_{Y}(y)&=\frac{1}{(2\pi\sigma^2)^1/2}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right),\, y\in\mathbb{R}\\
&=\exp\left(\frac{y\mu-\mu^2/2}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{\ln(2\pi\sigma^2)}{2}\right),\, y\in\mathbb{R}.
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\mu$
\item[$\bullet$] Paramètre de dispersion: $\phi=\sigma^2$
\item[$\bullet$] $b(\theta)=\frac{\theta^2}{2}$
\item[$\bullet$] $\esp{Y}=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\frac{\theta^2}{2}=\theta=\mu$
\item[$\bullet$] $\var{Y}=\phi\ddot{b}(\theta)=\sigma^2\frac{\partial}{\partial\theta}\theta=\sigma^2$
\item[$\bullet$] $V(\mu)=1$.
\end{itemize}

\item Uniforme$(0,\beta)$: non. Le domaine dépend du paramètre $\beta$.

\item Poisson$(\lambda)$: 
\begin{align*}
f_{Y}(y;\lambda)&=\frac{\lambda^{y}e^{-\lambda}}{y!} \mbox{, pour }y\in \mathbb{N}^{+}\\
&=\exp\{y\ln \lambda - \lambda - \ln y!\}\\
f_{Y}(y;\theta,\phi)&=\exp\left\{\frac{y\theta - e^{\theta}}{\phi} - \ln y!\right\}.
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln \lambda$
\item[$\bullet$] Paramètre de dispersion: $\phi=1$
\item[$\bullet$] $b(\theta)=e^{\theta}$
\item[$\bullet$] $\esp{Y}=\dot{b}(\theta)=\frac{\partial}{\partial\theta}e^{\theta}=e^{\theta}=\lambda$
\item[$\bullet$] $\var{Y}=\phi\ddot{b}(\theta)=\frac{\partial}{\partial\theta}e^{\theta}=e^{\theta}=\lambda$
\item[$\bullet$] $V(\mu)=\mu$.
\end{itemize}

\item Bernoulli$(\pi)$
\begin{align*}
f_{Y}(y;\pi)&=\pi^{y}(1-\pi)^{1-y}1(y \in \{0,1\})\\
&=\exp\left\{y\ln\left(\frac{\pi}{1-\pi}\right)+\ln(1-\pi)\right\}1(y \in \{0,1\}).\\
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln\left(\frac{\pi}{1-\pi}\right)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1$
\item[$\bullet$] $b(\theta)=\ln(1+e^{\theta})$
\item[$\bullet$] $\esp{Y}=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\ln(1+e^{\theta})=\frac{e^{\theta}}{1+e^\theta}=\pi$
\item[$\bullet$] $\var{Y}=\phi\ddot{b}(\theta)=\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1+e^\theta}=\frac{e^\theta}{(1+e^\theta)^2}=\pi(1-\pi)$
\item[$\bullet$] $V(\mu)=\mu(1-\mu)$.
\end{itemize}

\item Binomiale$(m, \pi)$, $m>0$ est un entier et est connu.
\begin{align*}
f_{Y}(y;\pi)&=\begin{pmatrix} m\\y\end{pmatrix}\pi^{y}(1-\pi)^{m-y}1(y \in \{0,1,...,m\})\\
&=\exp\left\{y\ln\left(\frac{\pi}{1-\pi}\right)+m\ln(1-\pi)+\ln\begin{pmatrix} m\\y\end{pmatrix}\right\}1(y \in \{0,1,...,m\}).\\
\end{align*}
Dans cette représentation, on a $$\esp{Y}=m\pi \mbox{ et } \var{Y}=m\pi(1-\pi).$$ Cette forme est moins utilisée car l'espérance de $Y$ dépend de $m$, le paramètre de dispersion. Souvent, on transforme les données. On utilise plutôt $Y^{*}=Y/m$. Alors, pour ces données transformées,
\begin{align*}
f_{Y^{*}}(y;\pi)&=\exp\left\{my\ln\left(\frac{\pi}{1-\pi}\right)+m\ln(1-\pi)+\ln\begin{pmatrix} m\\my\end{pmatrix}\right\}, \,y \in \{0,1/m,...,1\}\\
&=\exp\left\{\frac{y\ln\left(\frac{\pi}{1-\pi}\right)+\ln(1-\pi)}{1/m}+\ln\begin{pmatrix} m\\my\end{pmatrix}\right\}, \,y \in \{0,1/m,...,1\}.\\
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln\left(\frac{\pi}{1-\pi}\right)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1/m$
\item[$\bullet$] $b(\theta)=\ln(1+e^{\theta})$
\item[$\bullet$] $\esp{Y^*}=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\ln(1+e^{\theta})=\frac{e^{\theta}}{1+e^\theta}=\pi$
\item[$\bullet$] $\var{Y^*}=\phi\ddot{b}(\theta)=\frac{1}{m}\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1+e^\theta}=\frac{e^\theta}{m(1+e^\theta)^2}=\frac{\pi(1-\pi)}{m}$
\item[$\bullet$] $V(\mu)=\mu(1-\mu)$.
\end{itemize}

\item Pareto$(\alpha,\lambda)$: non.

\item Gamma$(\alpha,\beta)$
Soit $Y\sim Gamma(\alpha,\beta)$. Alors, avec un peu de travail, la densité peut être écrite sous la forme exponentielle linéaire.
$$f_{Y}(y;\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha-1}e^{-\beta y},$$ pour $y>0$. On reparamétrise: $\mu=\alpha/\beta=\esp{Y}$ et $\alpha$, on a donc $\beta=\alpha/\mu$ et $$f_{Y}(y;\alpha,\mu)=\frac{1}{y\Gamma(\alpha)}\left(\frac{\alpha y}{\mu}\right)^{\alpha}\exp\left\{-\frac{\alpha y}{\mu}\right\}.$$ Posons $\theta=-1/\mu$, et $a(\phi)=1/\alpha$, alors on trouve $$f_{Y}(y;\theta,\phi)=\exp\left\{\frac{y\theta +\ln(-\theta)}{\phi} +\alpha\ln\alpha+(\alpha-1)\ln y -\ln\Gamma(\alpha)\right\}.$$ Donc, $b(\theta)=-\ln(-\theta)$ et $a(\phi)=1/\alpha \Rightarrow \dot{b}(\theta)=\frac{-1}{\theta}=\mu$ et $\ddot{b}(\theta)=\frac{1}{\theta^{2}}=\mu^{2}$. Finalement, $$\esp{Y}=\frac{-1}{\theta}=\mu \mbox{ et } \var{Y}=\frac{1}{\alpha}\mu^{2}.$$

\item Binomiale négative$(r,\pi)$ avec $r$ connu. On considère $Y^*=Y/r$:
\begin{align*}
f_Y^*(y)&=\begin{pmatrix} r+ry-1\\ry\end{pmatrix}\pi^r(1-\pi)^{ry}, \mbox{ pour } y\in \{0,\frac{1}{r},\frac{2}{r},...\}\\
&=\exp\left(ry\ln(1-\pi)+r\ln\pi+\ln\begin{pmatrix} r+ry-1\\ry\end{pmatrix}\right).
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln(1-\pi)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1/r$
\item[$\bullet$] $b(\theta)=-\ln(1-e^\theta)$
\item[$\bullet$] $\esp{Y^*}=\dot{b}(\theta)=\frac{\partial}{\partial\theta}-\ln(1-e^\theta)=\frac{e^{\theta}}{1-e^\theta}=\frac{1-\pi}{\pi}$
\item[$\bullet$] $\var{Y^*}=\phi\ddot{b}(\theta)=\frac{1}{r}\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1-e^\theta}=\frac{e^\theta}{r(1-e^\theta)^2}=\frac{(1-\pi)}{r\pi^2}$
\item[$\bullet$] $V(\mu)=\mu(\mu+1)$.
\end{itemize}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Quelles fonctions de lien peut-on utiliser pour un GLM avec une loi de Poisson?

\begin{rep}
$\eta=\ln(\mu)$
\end{rep}

\begin{sol}
Le lien canonique est le lien log: $\eta=\ln(\mu)$. On pourrait aussi utiliser d'autres fonctions de lien, telle que le lien identité $\eta=\mu$, le lien inverse $\eta=\frac{1}{\mu}$, mais le lien log est le plus approprié parce que son utilisation garantit une moyenne $\mu$ positive, ce qui est nécessaire pour la loi de Poisson.
\end{sol}
\end{exercice}

\begin{exercice}
Quel est le lien canonique pour la loi gamma? Est-ce que ce lien est toujours approprié?

\begin{rep}
$\eta=1/\mu$
\end{rep}

\begin{sol}
Le lien canonique pour la loi Gamma est le lien inverse $\eta=1/\mu$. Comme la moyenne d'une loi Gamma est toujours positive, ce lien n'est pas toujours approprié parce qu'il ne restreint pas le domaine de $\mu$ aux réels positifs. Le lien log serait plus approprié dans certains cas.
\end{sol}
\end{exercice}

\begin{exercice}
On suppose que $Y_1,...,Y_n$ sont des v.a.s indépendantes et $Y_i\sim Poisson(\mu_i)$. Pour chaque observation, on a une seule variable explicative $x_i$.
\begin{enumerate}
\item Quel est le lien canonique?
\item Trouver les fonctions de score (à résoudre pour l'estimation des paramètres par maximum de vraisemblance) 
\end{enumerate}

\begin{rep}
\begin{inparaenum}
\item $\eta=g(\mu)=\ln(\mu)$
\item $\sum_{i=1}^n y_i-e^{\beta_0+\beta_1x_i}=0 \text{ et } \sum_{i=1}^n x_i(y_i-e^{\beta_0+\beta_1x_i})=0$
\end{inparaenum}
\end{rep}

\begin{sol}
\begin{enumerate}
\item $\eta=g(\mu)=\ln(\mu)$
\item On a $$\ln(\mu_i)=\eta_i=\beta_0+\beta_1x_i.$$ La densité de la loi Poisson est
\begin{align*}
f_{Y_i}(y_i;\mu_i)&=\exp\left(y_i\ln\mu_i-\mu_i-\ln y_i!\right)\\
f_{Y_i}(y_i;\beta_0,\beta_1)&=\exp\left(y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}-\ln y_i!\right).
\end{align*}
La fonction de vraisemblance et la log-vraisemblance sont donc:
\begin{align*}
\mathcal{L}(\beta_0,\beta_1)&=\prod_{i=1}^n f_{Y_i}(y_i;\beta_0,\beta_1)=\prod_{i=1}^n\exp\left(y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}-\ln y_i!\right)\\
\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}+\mbox{constante}.
\end{align*}
On maximise la log-vraisemblance:
\begin{align*}
\frac{\partial}{\partial\beta_0}\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_i-e^{\beta_0+\beta_1x_i}\\
\frac{\partial}{\partial\beta_1}\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_ix_i-x_i e^{\beta_0+\beta_1x_i}\\
\end{align*}
Donc, les équations à résoudre sont
\begin{align*}
\sum_{i=1}^n y_i-e^{\beta_0+\beta_1x_i}&=0\\
\sum_{i=1}^n x_i(y_i-e^{\beta_0+\beta_1x_i})&=0.\\
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Montrer que la déviance pour le modèle binomial est $$D(y,\hat{\mu})=2\sum_{i=1}^n m_i \left[y_i\ln\left(\frac{y_i}{\hat{\mu}_i}\right)+(1-y_i)\ln\left(\frac{1-y_i}{1-\hat{\mu}_i}\right)\right].$$

\begin{sol}
La déviance est $$D(y;\hat{\mu})=2(\ell_n(\tilde{\theta})-\ell_n(\hat{\theta})).$$ Pour le modèle Binomial, on a que
\begin{align*}
\ell_n(\theta)&=\sum_{i=1}^n\frac{Y_i\ln\left(\frac{\mu_i}{1-\mu_i}\right)+\ln(1-\mu_i)}{1/m_i}.
\end{align*} Alors, dans le modèle complet, $\mu_i=Y_i$ et on trouve
\begin{align*}
\ell_n(\tilde{\theta})&=\sum_{i=1}^n\frac{Y_i\ln\left(\frac{Y_i}{1-Y_i}\right)+\ln(1-Y_i)}{1/m_i}.
\end{align*}
Dans le modèle développé avec le lien log, $\mu_i=\hat{\mu}_i$ et on trouve
\begin{align*}
\ell_n(\hat{\theta})&=\sum_{i=1}^n\frac{Y_i\ln\left(\frac{\hat{\mu}_i}{1-\hat{\mu}_i}\right)+\ln(1-\hat{\mu}_i)}{1/m_i}.
\end{align*}
Finalement, la déviance est
\begin{align*}
D(y;\hat{\mu})&=\sum_{i=1}^n\frac{Y_i\ln\left(\frac{Y_i}{1-Y_i}\right)+\ln(1-Y_i)}{1/m_i}-\frac{Y_i\ln\left(\frac{\hat{\mu}_i}{1-\hat{\mu}_i}\right)+\ln(1-\hat{\mu}_i)}{1/m_i}\\
&=\sum_{i=1}^n m_i\left[Y_i\ln\left(\frac{Y_i}{\hat{\mu}_i}\right)+(1-Y_i)\ln\left(\frac{1-Y_i}{1-\hat{\mu}_i}\right)\right].
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
Trouver les expressions des résidus de Pearson, d'Anscombe et de déviance pour la loi Gamma.

\begin{rep}
Pearson : $r_{P_i} = (Y_i-\hat{\mu}_i)/\hat{\mu}_i $, 
Anscombe : $r_{A_i} = (3(Y_i^{1/3}-\hat{\mu}_i^{1/3}))(\hat{\mu}_i^{1/3})$,
Déviance : $r_{D_i}=sign(Y_i-\hat{\mu}_i)\sqrt{2\left(\ln\left(\frac{\hat{\mu}_i}{Y_i}\right)+\frac{Y_i-\hat{\mu}_i}{\hat{\mu}_i}\right)}$
\end{rep}

\begin{sol}
Pour la distribution Gamma, on a $V(t)=t^2$ et $b(t)=-\ln(-t)$.

Résidus de Pearson: $$r_{P_i}=\frac{Y_i-\hat{\mu}_i}{\sqrt{\hat{\mu}_i^2}}=\frac{Y_i-\hat{\mu}_i}{\hat{\mu}_i}.$$
Résidus d'Anscombe:
\begin{align*}
A(t)&=\int_{0}^{t} \frac{\mbox{d}s}{s^{2/3}}=3t^{1/3}\\
\dot{A}(t)&=\frac{1}{s^{2/3}}\\
r_{A_i}&=\frac{A(Y_i)-A(\hat{\mu}_i)}{\dot{A}(\hat{\mu}_i)\sqrt{V(\hat{\mu}_i)}}=\frac{3(Y_i^{1/3}-\hat{\mu}_i^{1/3})}{\hat{\mu}_i^{1/3}}.
\end{align*}
Résidus de déviance: 
\begin{align*}
D_i&=2\left(-\frac{Y_i}{Y_i}-\ln(Y_i)+\frac{Y_i}{\hat{\mu}_i}+\ln(\hat{\mu}_i)\right)\\
&=2\left(\ln\left(\frac{\hat{\mu}_i}{Y_i}\right)+\frac{Y_i-\hat{\mu}_i}{\hat{\mu}_i}\right)\\
r_{D_i}&=sign(Y_i-\hat{\mu}_i)\sqrt{2\left(\ln\left(\frac{\hat{\mu}_i}{Y_i}\right)+\frac{Y_i-\hat{\mu}_i}{\hat{\mu}_i}\right)}.
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
<<echo=FALSE>>=
stresstest <- read.csv("data/Stresstest.csv")
@
Les données suivantes représentent des données de comptage, du nombre d'échec pour trois appareils médicaux (M1, M2 et M3) lors de tests de résistance sur 1000 appareils de chaque type et pour quatre niveaux de résistance mécanique différents (I, II, III, IV).

\begin{center}
\begin{tabular}{c|cccc}
\texttt{Device}$\backslash$ \texttt{Stress Level} & \texttt{I} & \texttt{II} & \texttt{III} & \texttt{IV}\\ \hline
\texttt{M1} &6 &8& 18 &10\\
\texttt{M2}& 13& 18& 29& 20\\
\texttt{M3} &9& 8 &21& 19\\ 
\end{tabular}
\end{center}

À l'aide de la modélisation Poisson (lien canonique), évaluer s'il y a une différence significative entre les taux d'échec des appareils.
\begin{sol}
Cette solution est en anglais, vous pouvez poser vos questions sur le forum, s'il y a lieu.

This is a two-factor model, «Device» takes three levels (M1, M2 and M3) and «Stress» takes 4 levels. The baseline group is M1 device at stress level I. An analysis of deviance is carried out to assess if the parameters for the devices are significant.

<<>>=
glm <- glm(Failures~Level*Machine,family=poisson,data=stresstest)
anova(glm)
qchisq(0.95, 6)
qchisq(0.95, 2)
@

The model $$\mbox{Stress+Device+Stress.Device}$$ is fitted first. The change in deviance from the simpler model $\mbox{Stress+Device}$ is 2.7719 on 6 degrees of freedom, which is not significant when compared to $\chi^{2}_{(6,0.95)}=12.59$. Hence, the model $\mbox{Stress+Device}$ is an adequate simplification of the more complex model. If we then test for the significance of the Device parameters, we find that the change in deviance from the simpler model Stress is 12.2154 on 2 degrees of freedom, which is significant because $\chi^{2}_{(2,0.95)}=5.99$. From this analysis, we can conclude that there is a significant difference between the failure rates of the different devices. 
\end{sol}
\end{exercice}

\begin{exercice}
<<echo=FALSE>>=
Bcar <- read.table("data/BritishCar.csv",header=TRUE,sep=";")
@
Les données pour cet exercice sot contenues dans le fichier \texttt{BritishCar.csv (sep='';'')} disponible sur le site du cours. On y trouve les montants de réclamations moyens pour les dommages causés au véhicule du détenteur de la police pour les véhicules assurés au Royaume-Uni en 1975. Les moyennes sont en livres sterling ajustées pour l'inflation.

\begin{center}
\begin{tabular}{l|l}
\hline
Variable & Description\\ \hline
\texttt{OwnerAge} & Âge du détenteur de la police (8 catégories) \\
\texttt{Model} & Type de voiture (4 groupes) \\
\texttt{CarAge} & Âge du véhicule, en années (4 catégories) \\
\texttt{NClaims} & Nombre de réclamations\\
\texttt{AvCost} & Coût moyen par réclamation, en livres sterling\\ \hline
\end{tabular}
\end{center}

On s'intéresse à la modélisation du coût moyen par réclamation.

\begin{enumerate}
\item Ajuster un modèle de régression Gamma avec lien inverse pour la variable endogène \texttt{AvCost}. Inclure les effets principaux \texttt{OwnerAge}, \texttt{Model} et \texttt{CarAge}.

\item Quelle est l'espérance du coût moyen de la réclamation pour un détenteur de police âgé entre 17 et 20 ans, avec une auto de type A âgée de moins de 3 ans ?

\item Interpréter brièvement les coefficients pour la variable exogène \texttt{OwnerAge}.

\item Interpréter brièvement les coefficients pour la variable exogène \texttt{Model}.

\item Interpréter brièvement les coefficients pour la variable exogène \texttt{CarAge}.

\item Pour quelle combinaison de variables exogènes l'espérance du coût de réclamation est-elle la plus élevée? Calculer sa valeur.

\item Pour quelle combinaison de variables exogènes l'espérance du coût de réclamation est-elle la plus faible? Calculer sa valeur.

\item Quelle est la déviance pour ce modèle? Est-ce que le modèle semble adéquat?

\item Tracer le graphique des résidus de Pearson en fonction des valeurs prédites, des résidus d'Ascombe en fonction des valeurs prédites et des résidus de déviance en fonction des valeurs prédites.

\item Obtient-on les mêmes conclusions aux sous-questions a) à h) si on utilise un lien logarithmique plutôt que le lien inverse?
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item En \textsf{R}, on obtient
<<>>=
modinv <- glm(AvCost~OwnerAge+Model+CarAge,family=Gamma,data=Bcar)
summary(modinv)
@

\item On a utilisé un lien inverse, alors $\esp{Y_i}=\frac{1}{\eta_i}.$ Puisque les variables explicatives prennent toutes leur niveau de base, on a que $\hat{\eta}_i=\hat{\beta}_0=0.0033233$ et $$\widehat{\esp{Y_i}}=0.0033233^{-1}=300.91.$$

\item Puisqu'on a utilisé un lien inverse, un coefficient plus élevé implique une diminution de l'espérance du coût de la réclamation, alors qu'un coefficient négatif signifie une augmentation de cette espérance. Ici on observe que les sept coefficients sont positifs, alors la catégorie d'âge ayant une espérance de coût la plus élevée est la catégorie de base, 17-20 ans. Le coût moyen semble ensuite relativement élevé pour les jeunes entre 21 et 29 ans. La catégorie d'âge avec coût de réclamation minimal est 35-39 ans, puis la moyenne semble relativement stable pour les détenteurs de police plus âgés.

\item Les trois coefficients pour la variable modèle sont négatifs, ce qui signifie que les réclamations pour les véhicules de type A (niveau de base) sont moins élevées en moyenne que celles pour les autres types de véhicule. Les réclamatios pour les véhicules du modèle D semblent particulièrement coûteuse car le coefficient est beaucoup plus grand en valeur absolue que les autres.

\item De la même façon, on observe que d'augmenter l'âge du véhicule diminue le coût moyen des réclamations.

\item Pour un détenteur de police entre 17 et 20 ans, avec un véhicule de type D âgé de un à 3 ans, on trouve que $$\widehat{\esp{Y_i}}=\frac{1}{\hat{\beta}_0+\hat{\beta}^{MODEL}_D}=\frac{1}{0.0033233-0.0018235}=666.76.$$

\item Pour un détenteur de police entre 35 et 39 ans, avec un véhicule de type A âgé de plus de 10 ans, on trouve que $$\widehat{\esp{Y_i}}=\frac{1}{\hat{\beta}_0+\hat{\beta}^{OWNERAGE}_{35-39}+\hat{\beta}^{CARAGE}_{10+}}=\frac{1}{0.0033233+0.0016372+0.0033776}=119.93.$$

\item La déviance $D(y,\hat{\mu})=11.511$ est donnée dans la sortie \textsf{R} pour la sous-question a). On a que $$\frac{D(y,\hat{\mu})}{\hat{\phi}}=\frac{11.511}{0.1074529}=107.126,$$ ce qui est très près de $n-p'=109$. Le modèle semble donc adéquat.

\item Les résidus sont calculés avec les formules trouvées à la question 6. Il faut d'abord enlever les données manquantes du vecteur contenant les coûts moyens. On obtient les graphiques de la Figure~\ref{fig:glm:residuals}.

\begin{figure}
<<glm-residuals,echo=FALSE>>=
attach(Bcar)
pears <- (AvCost[-which(is.na(AvCost))]-fitted(modinv))/fitted(modinv)
ansc <- 3*(AvCost[-which(is.na(AvCost))]^(1/3)-fitted(modinv)^(1/3))/fitted(modinv)^(1/3)
dev <-sign(AvCost[-which(is.na(AvCost))]-fitted(modinv))*sqrt(2*(log(fitted(modinv)/AvCost[-which(is.na(AvCost))])+pears))

plot(fitted(modinv),pears,xlab="Valeurs prédites",ylab="Résidus Pearson",pch=16)
plot(fitted(modinv),ansc,xlab="Valeurs prédites",ylab="Résidus Anscombe",pch=16)
plot(fitted(modinv),dev,xlab="Valeurs prédites",ylab="Résidus de déviance",pch=16)
@
\caption{Résidus pour GLM Gamma}\label{fig:glm:residuals}
\end{figure}

\item a. Le modèle avec le lien logarithmique est 

<<>>=
modlog <- glm(AvCost~OwnerAge+Model+CarAge,family=Gamma(link=log),data=Bcar)
summary(modlog)
@

b. Avec ce modèle $\esp{Y_i}=e^{\eta_i}.$ Puisque les variables explicatives prennent toutes leur niveau de base, on a que $\hat{\eta}_i=\hat{\beta}_0=5.711739$ et $$\widehat{\esp{Y_i}}=e^{5.711739}=302.39.$$ Cela ne diffère pas beaucoup du résultat trouvé en b).

c-d-e. Puisqu'on a utilisé un lien logarithmique, on a un modèle multiplicatif. Si $e^\beta>1$, alors l'espérance du coût augmente, alors que si $e^\beta<1$ alors l'espérance du coût diminue. On peut donc tirer des conclusions similaires à celles en c), d) et e).

f. Pour un détenteur de police entre 25 et 29 ans, avec un véhicule de type D âgé de un à 3 ans, on trouve que $$\widehat{\esp{Y_i}}=\exp\left(\hat{\beta}_0+\hat{\beta}^{OWNERAGE}_{25-29}+\hat{\beta}^{MODEL}_D \right)=\exp(5.711739+0.005223+0.472290)=\Sexpr{round(exp(5.711739+0.005223+0.472290),2)}.$$ On note que cette valeur est beaucoup moins élevée que celle obtenue en f).

g. Pour un détenteur de police entre 35 et 39 ans, avec un véhicule de type A âgé de plus de 10 ans, on trouve que $$\widehat{\esp{Y_i}}=\exp\left(\hat{\beta}_0+\hat{\beta}^{OWNERAGE}_{35-39}+\hat{\beta}^{CARAGE}_{10+}\right)=\exp(5.711739-0.331420-0.735513)=104.04.$$

h. La déviance $D(y,\hat{\mu})=11.263$ est donnée dans la sortie \textsf{R} pour la sous-question a). On a que $$\frac{D(y,\hat{\mu})}{\hat{\phi}}=\frac{11.263}{0.0910768}=123.66,$$ ce qui est moins près de $n-p'=109$ que pour le modèle avec le lien inverse. Le modèle semble donc moins adéquat.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
<<echo=FALSE>>=
x <- c(400, seq(from = 1000, to = 4600, by = 400))
m <- c(39, 53, 33, 73, 30, 39, 42, 13, 34, 40, 36)
y <- c(2, 4, 3, 7, 5, 9, 9, 6, 22, 21, 21)
@
On considère les données suivantes, qui contiennent le nombre $Y_i$ de turbines sur $m_i$ qui ont été fissurées après $x_i$ heures d'opération.

\begin{center}
\begin{tabular}{rrr}
\hline
$x_i$ &$m_i$ & $Y_i$\\\hline
400& 39 &2\\
1000& 53 &4\\
1400 &33& 3\\
1800& 73 &7\\
2200 &30& 5\\
2600& 39 &9\\
3000 &42& 9\\
3400& 13 &6\\
3800 &34& 22\\
4200& 40 &21\\
4600 &36& 21\\\hline
\end{tabular}
\end{center}

\begin{enumerate}
\item En utilisant un GLM binomial avec lien canonique, dériver les estimateurs des paramètres lorsque $x_i$ est traité comme une variable exogène dichotomique avec 11 niveaux, et lorsque le prédicteur linéaire pour la donnée $i$ est $$\eta_i=\beta_0+\beta_i, \mbox{ pour } i=1,...,11,$$ avec la contrainte d'identifiabilité que $\beta_1=0$.

\item En utilisant \textsf{R} et un GLM binomial avec lien canonique, ajuster le modèle où le prédicteur linéaire est $$\eta_i=\beta_0+\beta_1 x_i, \mbox{ pour } i=1,...,11.$$ Donner les estimations des paramètres et leur écart-type.

\item Refaire (b) en utilisant un lien probit. Donner les estimations des paramètres et leur écart-type.

\item Refaire (b) en utilisant un lien log-log complémentaire. Donner les estimations des paramètres et leur écart-type.

\item Comparer les prévisions (et leurs mesures d'incertitude) sous les trois modèles ajustés en (b), (c) et (d) pour une turbine qui était en opération pour 2000 heures.

\item Tracer un graphique pour montrer si les modèles en (b), (c) et (d) ajustent bien (ou non) les données. Commenter.

\end{enumerate}
\begin{sol}
\begin{enumerate}
\item If $x_{i}$ is treated as a factor predictor with 11 levels, the linear predictor is written as $$\eta_{i}=\beta_{0}+\beta_{i} \mbox{,  } i=1,...,11$$ and $\beta_{1}=0$. The binomial density is the following: $$f_{Y}(y_{i})=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\pi_{i}^{y_{i}}(1-\pi_{i})^{m_{i}-y_{i}},$$ which can be rewritten in a exponential family representation as: $$f_{Y}(y_{i})=\exp\left[y_{i}\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)+m\ln(1-\pi_{i})+\ln\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\right].$$ Hence, the canonical parameter is $\theta_{i}=\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)$ and the canonical link is the logit link. Thus, 
\begin{align*}
\eta_{i}&=\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)=\beta_{0}+\beta_{i} \\
\pi_{i}&=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}= \frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\\
\end{align*}
The expression of the density in the reparametrization is then 
\begin{align*}
f_{Y}(y_{i})&=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\left(\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\beta_{0}+\beta_{i}}}\right)^{m_{i}-y_{i}}\\
&=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\frac{e^{y_{i}(\beta_{0}+\beta_{i})}}{(1+e^{\beta_{0}+\beta_{i}})^{m_{i}}}\\
\end{align*} The likelihood $L$ and the log-likelihood $l$ are shown below:
\begin{align*}
L(\beta_{0},...,\beta_{11};y_{1},...,y_{11})&=\prod_{i=1}^{11} \begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\frac{e^{y_{i}(\beta_{0}+\beta_{i})}}{(1+e^{\beta_{0}+\beta_{i}})^{m_{i}}}\\
\ell(\beta_{0},...,\beta_{11};y_{1},...,y_{11})&=\sum_{i=1}^{11} \left[\ln\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}+y_{i}(\beta_{0}+\beta_{i})-m_{i}\ln(1+e^{\beta_{0}+\beta_{i}})\right]\\
\end{align*} We have 
\begin{align*}
\frac{\partial \ell}{\partial \beta_{0}}&=\sum_{i=1}^{11} \left[y_{i}-m_{i}\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\right]\\
\frac{\partial \ell}{\partial \beta_{i}}&=y_{i}-m_{i}\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}} \mbox{,  } i=2,...,11,\\
\end{align*} and $\beta_{1}=0$ by constraint of the model. The maximum likelihood estimators for the parameters are derived by solving the system of equations $\frac{\partial \ell}{\partial \beta_{i}}=0,$ $i=0,...,11$:
\begin{align*}
\sum_{i=1}^{11} \left[y_{i}-m_{i}\frac{e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}{1+e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}\right]&=0\\
y_{i}-m_{i}\frac{e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}{1+e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}&=0 \mbox{,  } i=2,...,11,\\
\Rightarrow \hat{\beta}_{0}+\hat{\beta}_{i}=\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)& \mbox{,  } i=2,...,11,\\
\end{align*}  Using the first equation and replacing $\hat{\beta}_{0}+\hat{\beta}_{i}$ by $\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)$,
\begin{align*}
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}+\sum_{i=2}^{11} \left[y_{i}-m_{i}\frac{\left(\frac{y_{i}}{m_{i}-y_{i}}\right)}{1+\left(\frac{y_{i}}{m_{i}-y_{i}}\right)}\right]=0\\
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}+\sum_{i=2}^{11} \left[y_{i}-m_{i}\frac{y_{i}}{m_{i}}\right]=0\\
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}=0\\
&\hat{\beta}_{0}=\ln\left(\frac{y_{1}}{m_{1}-y_{1}}\right)\\
&\hat{\beta}_{i}=\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)-\hat{\beta}_{0}=\ln\left(\frac{y_{i}/(m_{i}-y_{i})}{y_{1}/(m_{1}-y_{1})}\right)  \mbox{,  } i=2,...,11.\\
\end{align*}
The estimates of the model parameters are easily found in \textsf{R} as follows:
<<>>=
(beta0 <- log(y[1]/(m[1]-y[1])))
(beta <- c(0,log(y[-1]/(m[-1]-y[-1]))-beta0))
@
Hence, here $$\hat{\beta}=(-2.9178, 0, 0.4122, 0.6152, 0.6740, 1.3083, 1.7138, 1.6185, 2.7636, 3.5239, 3.0179, 3.2542)^\top.$$

As a consistency check following from the invariance property of maximum likelihood estimation, we can verify that the estimates of $\pi_{i}$ using the expit function are equal to the MLE estimates $\hat{\pi}_{i}=\frac{y_{i}}{m_{i}}$:
<<>>=
(pi <- exp(beta0+beta)/(1+exp(beta0+beta)))
y/m
@

\item The Binomial GLM model with logit link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \textsf{R} and the command:
<<echo=TRUE, eval=FALSE>>=
glm(cbind(y,m-y)~x,family=binomial)
@

The estimates of the parameters are $\hat{\beta}_{0}=-3.6070615$ and $\hat{\beta}_{1}=0.0009121$, with standard error $SE(\hat{\beta}_{0})=0.3533875$ and $SE(\hat{\beta}_{1})=0.0001084$.

\item The Binomial GLM model with probit link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \textsf{R} and the command:
<<echo=TRUE, eval=FALSE>>=
glm(cbind(y,m-y)~x,family=binomial(link=probit))
@

The estimates of the parameters are $\hat{\beta}_{0}=-2.080$ and $\hat{\beta}_{1}=5.230\times10^{-4}$, with standard error $SE(\hat{\beta}_{0})=0.1852$ and $SE(\hat{\beta}_{1})=5.973\times10^{-5}$.

\item The Binomial GLM model with complementary log-log link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \textsf{R} and the command:
<<echo=TRUE, eval=FALSE>>=
glm(cbind(y,m-y)~x,family=binomial(link=cloglog))
@

The estimates of the parameters are $\hat{\beta}_{0}=-3.360$ and $\hat{\beta}_{1}=7.480\times10^{-4}$, with standard error $SE(\hat{\beta}_{0})=0.3061$ and $SE(\hat{\beta}_{1})=8.622\times10^{-5}$.

\item Predictions can be found using the inverse of the link function. For the model with canonical link (model from b), we find that $$\hat{y}_{2000}=\frac{e^{\hat{\beta}_{0}+2000\hat{\beta}_{1}}}{1+e^{\hat{\beta}_{0}+2000\hat{\beta}_{1}}}=0.1439472.$$ Alternatively, the command \texttt{predict(modelb,data.frame(x=2000),type="response",se.fit=TRUE)} can be used to calculate the predictions and associated standard errors. The resulting predictions and standard errors are presented in Table~\ref{tab:glm:pred}. The probability of developing fissures after 2000 hours of operations is 14.39\% according to model b, and the standard deviation is 2.08\%. This prediciton is quite comparable with the complementary log-log model (d), for which the estimated probability of developing fissures is 14.36\%, with a standard error of 2.03\%. The precision is slightly better in this model than the two others due to a smaller variance. The estimated probability with Model c, using the probit link, is higher at 15.06\%, with standard error of 2.06\%. 
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l||c|c|c|}
  \hline
   & Model b (logit link) & Model c (probit link) & Model d (compl. log-log link) \\
  \hline
  \hline
	 $\hat{y}_{2000}$ &0.1439472 & 0.150615 & 0.1436447\\ \hline
	 $SE(\hat{y}_{2000})$ &0.02080256 & 0.02062154 & 0.02030803 \\ \hline
\end{tabular}
\caption{Predictions and Standard Errors for Predictions for the 3 Models} \label{tab:glm:pred}
\end{center}
\end{table}

\item Figure~\ref{fig:glm:modfit} shows a plot of the data points along with the three fitted lines. It was obtained using the following code in \textsf{R}, where \texttt{ilogit, iprobit} and \texttt{icloglog} are the inverse of the corresponding link functions:
<<echo=TRUE, eval=FALSE>>=
plot(x,y/m,pch=19,xlab="Number of Hours of Operation (x)",
     ylab="Prob of Developing Fissures")
j <- seq(0,4800,1)
lines(j,ilogit(coef(modelb)[1]+coef(modelb)[2]*j),lwd=2)
lines(j,iprobit(coef(modelc)[1]+coef(modelc)[2]*j),lty=2,col=2,lwd=2)
lines(j,icloglog(coef(modeld)[1]+coef(modeld)[2]*j),lty=3,col=4,lwd=2)
legend("topleft",legend=c("Logit","Probit","Complementary log-log"),
       lty=c(1,2,3),col=c(1,2,4),lwd=rep(2,3))
@
<<echo=FALSE>>=
modelb <- glm(cbind(y,m-y)~x,family=binomial)
modelc <- glm(cbind(y,m-y)~x,family=binomial(link=probit))
modeld <- glm(cbind(y,m-y)~x,family=binomial(link=cloglog))

ilogit <- function(x) exp(x)/(1+exp(x))
iprobit <- function(x) pnorm(x)
icloglog <- function(x) 1 - exp(-exp(x))
@

\begin{figure}
<<glm-modfit,echo=FALSE>>=
plot(x,y/m,pch=19,xlab="Number of Hours of Operation (x)",
     ylab="Prob of Developing Fissures")
j <- seq(0,4800,1)
lines(j,ilogit(coef(modelb)[1]+coef(modelb)[2]*j),lwd=2)
lines(j,iprobit(coef(modelc)[1]+coef(modelc)[2]*j),lty=2,col=2,lwd=2)
lines(j,icloglog(coef(modeld)[1]+coef(modeld)[2]*j),lty=3,col=4,lwd=2)
legend("topleft",legend=c("Logit","Probit","Complementary log-log"),
       lty=c(1,2,3),col=c(1,2,4),lwd=rep(2,3))
@
\caption{Logistic, Probit and Complementary Log-Log Model Fit} \label{fig:glm:modfit}
\end{figure}

It is easy to observe that the fit is better when the number of hours of operations is lower, it seems that the variance of the observations is increasing with the predictor. This is expected in a generalized linear model framework. The three fitted lines are slightly different. The probit link produces lower estimates in the tails and higher estimates in the middle of the range of the predictors. It seems like this model is less representative of the data than the others. The complementary log-log model (d) predicts higher probabilities of failures in the extremes of the range of the predictors. This seems to fit the data well, and recall that the variance of the predictions where also smaller than other models in this case, which is a desirable property. The line for the model with canonical link is between the two others. It could also be a reasonable model for the data.

\end{enumerate}
\end{sol}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-glm}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
