\documentclass[letterpaper,10pt]{memoir}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{natbib,url}
  \usepackage[english,french]{babel}
  \usepackage[autolanguage]{numprint}
  
  \usepackage{lucidabr,pslatex}
  \usepackage[sc]{mathpazo}
  %\usepackage{vgmath,vgsets}
  \usepackage{vmcommands,icomma,amsmath,amsthm,upgreek} 
  \usepackage{graphicx,color}
  \usepackage[absolute]{textpos}
  \usepackage{answers,listings}
  \usepackage[alwaysadjust,defblank]{paralist}
  %\usepackage{verbatim}
  %\usepackage[noae]{Sweave}
  \usepackage{threeparttable}
  

  %%% Hyperliens
  \usepackage{hyperref}
  \definecolor{link}{rgb}{0,0,0.3}
  \hypersetup{
    pdftex,
    colorlinks,%
    citecolor=link,%
    filecolor=link,%
    linkcolor=link,%
    urlcolor=link}

  %%% Page titre
  \title{\HUGE
    \fontseries{ub}\selectfont Modèles \\
    \fontseries{ub}\selectfont linéaires\\
    \fontseries{ub}\selectfont en actuariat \\[0.5\baselineskip]
    \huge\fontseries{m}\selectfont Exercices et solutions}
  \author{\LARGE Marie-Pier Côté \\[3mm]
          \LARGE Vincent Mercier \\[3mm]
    \large École d'actuariat, Université Laval}
  \date{\large Seconde édition}
  %\newcommand{\ISBN}{978-2-9811416-0-6}
  
  %%% Marge plus large 
	\setlrmarginsandblock{3.5cm}{3cm}{*}
	\setulmarginsandblock{3.5cm}{3cm}{*}
	\checkandfixthelayout
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  %% Options de babel
  \frenchbsetup{ThinSpaceInFrenchNumbers=true}
  \addto\captionsfrench{\def\tablename{{\scshape Tab.}}}
  \addto\captionsfrench{\def\figurename{{\scshape Fig.}}}

  %%% Style des entêtes de chapitres
  \chapterstyle{hangnum}

  %%% Styles des entêtes et pieds de page
  \setlength{\marginparsep}{1mm}
  \setlength{\marginparwidth}{1mm}
  \setlength{\headwidth}{\textwidth}
  \addtolength{\headwidth}{\marginparsep}
  \addtolength{\headwidth}{\marginparwidth}
  
  %%% Style de la bibliographie
  %\bibliographystyle{francais}
  \bibliographystyle{plain}
  
    %%% Numéroter les sous-sections
  \maxsecnumdepth{subsection}

%  %%% Nouveaux environnements
%  \theoremstyle{plain}
%  \newtheorem{thm}{Théorème}[chapter]
%  \theoremstyle{definition}
%  \newtheorem{exemple}{Exemple}[chapter]
%  \theoremstyle{remark}
%  \newtheorem*{rem}{Remarque}
%  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  %%% Associations entre les environnements et les fichiers
  \Newassociation{sol}{solution}{solutions}
  \Newassociation{rep}{reponse}{reponses}

  %%% Environnement pour les exercices
  \newcounter{exercice}[chapter]
  \newenvironment{exercice}{%
     \begin{list}{\bfseries \arabic{chapter}.\arabic{exercice}}{%
         \refstepcounter{exercice}
         \settowidth{\labelwidth}{\bfseries \arabic{chapter}.\arabic{exercice}}
         \setlength{\leftmargin}{\labelwidth}
         \addtolength{\leftmargin}{\labelsep}
         \setdefaultenum{a)}{i)}{}{}}\item}
     {\end{list}}

  %%% Environnement pour les réponses
  \renewenvironment{reponse}[1]{%
    \begin{list}{\bfseries #1}{%
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{\labelwidth}
        \addtolength{\leftmargin}{\labelsep}
        \setdefaultenum{a)}{i)}{}{}}\item}
    {\end{list}}
  \renewcommand{\reponseparams}{{\thechapter.\theexercice}}

  %%% Environnement pour les listes de commandes
  \newenvironment{ttscript}[1]{%
    \begin{list}{}{%
        \setlength{\labelsep}{1.5ex}
        \settowidth{\labelwidth}{\code{#1}}
        \setlength{\leftmargin}{\labelwidth}
        \addtolength{\leftmargin}{\labelsep}
        \setlength{\parsep}{0.5ex plus0.2ex minus0.2ex}
        \setlength{\itemsep}{0.3ex}
        \renewcommand{\makelabel}[1]{##1\hfill}}}
    {\end{list}}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
  %%% Environnement pour les solutions
  \renewenvironment{solution}[1]{%
    \begin{list}{\bfseries #1}{%
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{\labelwidth}
        \addtolength{\leftmargin}{\labelsep}
        \setdefaultenum{a)}{i)}{}{}}\item}
    {\end{list}}
  \renewcommand{\solutionparams}{{\thechapter.\theexercice}}

  %%% Nouvelles commandes
  \newcommand{\bobeta}{\mbox{\bm$\beta$}}

  %%% Sous-figures
  \newsubfloat{figure}

  %%% Paramètres pour les sections de code source
  \lstloadlanguages{R}
  \lstdefinelanguage{Renhanced}[]{R}{%
    morekeywords={acf,ar,arima,arima.sim,colMeans,colSums,is.na,is.null,%
      mapply,ms,na.rm,nlmin,replicate,row.names,rowMeans,rowSums,seasonal,%
      sys.time,system.time,ts.plot,which.max,which.min},
    deletekeywords={c},
    alsoletter={.\%},%
    alsoother={:_\$}}
  \lstset{language=Renhanced,
    extendedchars=true,
    inputencoding=latin1,
    basicstyle=\small\ttfamily,
    commentstyle=\textsl,
    keywordstyle=\mdseries,
    showstringspaces=false,
    index=[1][keywords],
    indexstyle=\indexfonction}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
%  %%% Environnements pour le code S: police plus petite
%  \RecustomVerbatimEnvironment{Sinput}{Verbatim}{fontsize=\small}
%  \RecustomVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\small}
%  \RecustomVerbatimEnvironment{Scode}{Verbatim}{fontsize=\small}

  %%% Index
  \renewcommand{\preindexhook}{%
    Cet index contient des entrées pour les annexes
    \ref{chap:regression} seulement. Les numéros de
    page en caractères gras indiquent les pages où les concepts sont
    introduits, définis ou expliqués.\vskip\onelineskip}
  \newcommand{\Index}[1]{\index{#1|textbf}}
  \newcommand{\indexargument}[1]{\index{#1@\code{#1}}}
  \newcommand{\indexclasse}[1]{\index{#1@\code{#1} (classe)}}
  \newcommand{\indexfonction}[1]{\index{#1@\code{#1}}}
  \newcommand{\Indexfonction}[1]{\Index{#1@\code{#1}}}

  \newcommand{\argument}[1]{\code{#1}\indexargument{#1}}
  \newcommand{\classe}[1]{\code{#1}\indexclasse{#1}}
  \newcommand{\fonction}[1]{\code{#1}\indexfonction{#1}}
  \newcommand{\Fonction}[1]{\code{#1}\Indexfonction{#1}}
  \makeindex
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% Options pour les graphiques directement appelés en R


% \shortcites{R:intro} aucune idée c'est pour quoi

\frontmatter

\pagestyle{empty}
\include{pagetitre}

\pagestyle{companion}

\include{introduction}

\cleardoublepage
\tableofcontents*

\mainmatter

\part{Régression linéaire}

\stepcounter{chapter}


\chapter{Régression linéaire simple}
\label{chap:simple}

\Opensolutionfile{reponses}[reponses-simple]
\Opensolutionfile{solutions}[solutions-simple]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:simple}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:simple}}

\end{Filesave}


\begin{exercice}
  \label{ex:simple:base}
  Considérer les données suivantes et le modèle de régression
  linéaire $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$:
  \begin{center}
      \begin{tabular}{l*{10}{c}}
        \toprule
        $i$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
        \midrule
        $x_i$ & 65 & 43 & 44 & 59 & 60 & 50 & 52 & 38 & 42 & 40 \\
        $Y_i$ & 12 & 32 & 36 & 18 & 17 & 20 & 21 & 40 & 30 & 24 \\
        \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item Placer ces points ci-dessus sur un graphique.
  \item Calculer les équations normales. \label{ex:simple:base:eq_normales}
  \item Calculer les estimateurs $\hat{\beta}_0$ et $\hat{\beta}_1$ en
    résolvant le système d'équations obtenu en b).
  \item Calculer les prévisions $\hat{Y}_i$ correspondant à $x_i$ pour
    $i = 1, \dots, n$.  Ajouter la droite de régression au graphique
    fait en a).
  \item Vérifier empiriquement que $\sum_{i=1}^{10} \varepsilon_i = 0$.
  \end{enumerate}
  
  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
      \stepcounter{enumi}
    \item $\hat{\beta}_0=66.44882$ et $\hat{\beta}_1=-0.8407468$
    \item $\hat{Y}_1 = 11,80, \hat{Y}_2 = 30,30, \hat{Y}_3 = 29,46,
      \hat{Y}_4 = 16,84, \hat{Y}_5 = 16,00, \hat{Y}_6 = 24,41,
      \hat{Y}_7 = 22,73, \hat{Y}_8 = 34,50, \hat{Y}_9 = 31,14,
      \hat{Y}_{10} = 32,82$
    \end{inparaenum}
  \end{rep}
  
  \begin{sol}
    \begin{enumerate}
    \item Voir la figure \ref{fig:simple:base}. Remarquer que l'on
      peut, dans la fonction \texttt{plot}, utiliser une formule pour
      exprimer la relation entre les variables.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlnum{65}\hlstd{,} \hlnum{43}\hlstd{,} \hlnum{44}\hlstd{,} \hlnum{59}\hlstd{,} \hlnum{60}\hlstd{,} \hlnum{50}\hlstd{,} \hlnum{52}\hlstd{,} \hlnum{38}\hlstd{,} \hlnum{42}\hlstd{,} \hlnum{40}\hlstd{)}
\hlstd{y}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlnum{12}\hlstd{,} \hlnum{32}\hlstd{,} \hlnum{36}\hlstd{,} \hlnum{18}\hlstd{,} \hlnum{17}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{21}\hlstd{,} \hlnum{40}\hlstd{,} \hlnum{30}\hlstd{,} \hlnum{24}\hlstd{)}
\hlkwd{plot}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{pch} \hlstd{=} \hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-base-1} 

}



\end{knitrout}
        \caption{Relation entre les données de l'exercice
          \ref{chap:simple}.\ref{ex:simple:base}}
        \label{fig:simple:base}
      \end{figure}
    \item Les équations normales sont les équations à résoudre pour
      trouver les estimateurs de $\beta_0$ et $\beta_1$ minimisant la
      somme des carrés
      \begin{align*}
        S(\beta_0, \beta_1)
        &=\sum_{i = 1}^n \varepsilon^2_i \\
        &=\sum_{i = 1}^n \left(Y_i-\beta_0-\beta_1x_i\right)^2.
      \end{align*}
      Or,
      \begin{align*}
        \frac{\partial S}{\partial \beta_0}
        &= -2 \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 x_i) \\
        \frac{\partial S}{\partial \beta_1}
        &= -2 \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 x_i) x_i,
      \end{align*}
      d'où les équations normales sont
      \begin{align*}
        \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) &= 0 \\
        \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) x_i &= 0.
      \end{align*}
    \item Par la première des deux équations normales, on trouve
      \begin{displaymath}
        \sum_{i=1}^nY_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^nx_i = 0,
      \end{displaymath}
      soit, en isolant $\hat{\beta}_0$,
      \begin{displaymath}
        \hat{\beta}_0=\frac{\sum_{i=1}^nY_i-\hat{\beta}_1\sum_{i=1}^nx_i}{n}=\bar{Y}-\hat{\beta}_1\bar{x}.
      \end{displaymath}
      De la seconde équation normale, on obtient
      \begin{displaymath}
        \sum_{i=1}^n x_i Y_i -
        \hat{\beta}_0 \sum_{i=1}^n x_i -
        \hat{\beta}_1 \sum_{i=1}^n x_i^2 = 0
      \end{displaymath}
      puis, en remplaçant $\hat{\beta}_0$ par la valeur obtenue ci-dessus,
      \begin{displaymath}
        \hat{\beta}_1
        \left(
          \sum_{i=1}^n x_i^2 - n \bar{x}^2
        \right) =
        \sum_{i=1}^n x_i Y_i - n \bar{x} \bar{Y}.
      \end{displaymath}
      Par conséquent,
      \begin{align*}
        \hat{\beta}_1
        &= \frac{\sum_{i=1}^n x_i Y_i - n \bar{x}\bar{Y}}{\sum_{i=1}^n
          x_i^2 - n \bar{x}^2} \\
        &= \frac{\nombre{11654} - (10)(49,3)(25)}{\nombre{25103} -
          (10)(49,3)^2} \\
        &= -0,8407 \\
        \intertext{et}
        \hat{\beta}_0
        &=\bar{Y}-\hat{\beta}_1\bar{x}\\
        &=25 - (-0,8407)(49,3)\\
        &=66,4488.
      \end{align*}
    \item On peut calculer les prévisions correspondant à $x_1, \dots,
      x_{10}$ --- ou valeurs ajustées --- à partir de la relation
      $\hat{Y}_i = 66,4488 - 0,8407 x_i$, $i = 1, 2, \dots, 10$. Avec
      \textsf{R}, on crée un objet de type modèle de régression avec
      \texttt{lm} et on en extrait les valeurs ajustées avec
      \texttt{fitted}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x)}
\hlkwd{fitted}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
##        1        2        3        4        5        6 
## 11.80028 30.29670 29.45596 16.84476 16.00401 24.41148 
##        7        8        9       10 
## 22.72998 34.50044 31.13745 32.81894
\end{verbatim}
\end{kframe}
\end{knitrout}
      Pour ajouter la droite de régression au graphique de la figure
      \ref{fig:simple:base}, il suffit d'utiliser la fonction
      \texttt{abline} avec en argument l'objet créé avec
      \texttt{lm}. L'ordonnée à l'origine et la pente de la droite
      seront extraites automatiquement. Voir la figure \ref{fig:simple:base2}.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{abline}\hlstd{(fit)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-base2-1} 

}



\end{knitrout}
        \caption{Relation entre les données de l'exercice
          \ref{chap:simple}.\ref{ex:simple:base} et la droite de
          régression}
        \label{fig:simple:base2}
      \end{figure}
    \item Les résidus de la régression sont $\varepsilon_i = Y_i - \hat{Y}_i$,
      $i = 1, \dots, 10$. Dans \textsf{R}, la fonction
      \texttt{residuals} extrait les résidus du modèle:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{residuals}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
##          1          2          3          4          5 
##  0.1997243  1.7032953  6.5440421  1.1552437  0.9959905 
##          6          7          8          9         10 
## -4.4114773 -1.7299837  5.4995615 -1.1374514 -8.8189450
\end{verbatim}
\end{kframe}
\end{knitrout}
      On vérifie ensuite que la somme des résidus est
      (essentiellement) nulle:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{sum}\hlstd{(}\hlkwd{residuals}\hlstd{(fit))}
\end{alltt}
\begin{verbatim}
## [1] -4.440892e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  On vous donne les observations ci-dessous.
  \begin{center}
    \begin{minipage}[t]{0.3\textwidth}
      \mbox{} \\
      \begin{tabular}{ccc}
        \toprule
        $t$ & $x_i$ & $Y_i$ \\
        \midrule
        1 & 2 & 6 \\
        2 & 3 & 4 \\
        3 & 5 & 6 \\
        4 & 7 & 3 \\
        5 & 4 & 6 \\
        6 & 4 & 4 \\
        7 & 1 & 7 \\
        8 & 6 & 4 \\
        \bottomrule
      \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
      \mbox{} \\[-1cm]
      \begin{align*}
        \sum_{i=1}^8 x_i &= 32 &
        \sum_{i=1}^8 x_i^2 &= 156 \\
        \sum_{i=1}^8 Y_i &= 40 &
        \sum_{i=1}^8 Y_i^2 &= 214 \\
        \sum_{i=1}^8 x_i\, Y_i &= 146 \\
      \end{align*}
    \end{minipage}
  \end{center}
  \begin{enumerate}
  \item Calculer les coefficients de la régression $Y_i = \beta_0 +
    \beta_1 x_i + \varepsilon_i$, $\var{\varepsilon_i} = \sigma^2$.
  \item Construire le tableau d'analyse de variance de la régression
    en a) et calculer le coefficient de détermination $R^2$.
    Interpréter les résultats.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat{\beta}_0=7$ et $\hat{\beta}_1=-0,5$
    \item SST = 14, SSR = 7, SSE = 7, MSR = 7, MSE = 7/6, $F$ = 6, $R^2$ = 0,5
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons le modèle de régression usuel. Les coefficients
      de la régression sont
      \begin{align*}
        \hat{\beta}_1
        &=\frac{\sum_{i=1}^8 x_iY_i-n\bar{x}\bar{Y}}{\sum_{i=1}^8
          x_i^2-n\bar{x}^2} \\
        &=\frac{146-(8)(32/8)(40/8)}{156-(8)(32/8)^2}  \\
        &=-0,5 \\
        \intertext{et}
        \hat{\beta}_0
        &=\bar{Y}-\hat{\beta}_1\bar{x} \\
        &=(40/8)-(-0,5)(32/8) \\
        &=7.
      \end{align*}
    \item Les sommes de carrés sont
      \begin{align*}
        \SST
        &=\sum_{i=1}^8(Y_i-\bar{Y})^2 \\
        &=\sum_{i=1}^8Y_i^2-n\bar{Y}^2 \\
        &=214-(8)(40/8)^2 \\
        &=14, \\
        \SSR
        &=\sum_{i=1}^8(\hat{Y}_i-\bar{Y})^2 \\
        &=\sum_{i=1}^8\hat{\beta}_1^2(x_i-\bar{x})^2 \\
        &=\hat{\beta}_1^2(\sum_{i=1}^8x_i^2-n\bar{x}^2) \\
        &=(-1/2)^2(156-(8)(32/8)^2) \\
        &=7.
      \end{align*}
      et $\SSE = \SST - \SSR = 14 - 7 = 7$. Par conséquent, $R^2 =
      SSR/SST = 7/14 = 0,5$, donc la régression explique 50~\% de la
      variation des $Y_i$ par rapport à leur moyenne $\bar{Y}$. Le
      tableau ANOVA est le suivant:
      \begin{center}
        \begin{tabular}{lcccc}
          \toprule
          Source & SS & d.l. & MS & Ratio F \\
          \midrule
          Régression & 7 & 1 & 7   & 6 \\
          Erreur     & 7 & 6 & 7/6 &  \\
          \midrule
          Total & 14 & 7 & & \\
          \bottomrule
        \end{tabular}
      \end{center}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le jeu de données \texttt{women.dat}, disponible à l'URL mentionnée
  dans l'introduction et inclus dans \textsf{R}, contient les tailles
  et les poids moyens de femmes américaines âgées de 30 à 39 ans.
  Importer les données dans dans \textsf{R} ou rendre le jeu de
  données disponible avec \texttt{data(women)}, puis répondre aux
  questions suivantes.
  \begin{enumerate}
  \item Établir graphiquement une relation entre la taille
    (\emph{height}) et le poids (\emph{weight}) des femmes.
  \item À la lumière du graphique en a), proposer un modèle de
    régression approprié et en estimer les paramètres.
  \item Ajouter la droite de régression calculée en b) au
    graphique. Juger visuellement de l'ajustement du modèle.
  \item Obtenir, à l'aide de la fonction \texttt{summary} la valeur du
    coefficient de détermination $R^2$. La valeur est-elle conforme à
    la conclusion faite en c)?
  \item Calculer les statistiques $\SST$, $\SSR$ et
    $\SSE$, puis vérifier que $\SST = \SSR +
    \SSE$.  Calculer ensuite la valeur de $R^2$ et la comparer
    à celle obtenue en d).
  \end{enumerate}

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item
      $\hat{\beta}_0 = -87,5167$
      et
      $\hat{\beta}_1 = 3,45$
      \stepcounter{enumi}
    \item $R^2 = 0,991$
    \item
      $\SSR = 3332,7$
      $\SSE = 30,23$ et
      $\SST = 3362,93$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Voir la figure \ref{fig:simple:women}.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(women)}
\hlkwd{plot}\hlstd{(weight} \hlopt{~} \hlstd{height,} \hlkwc{data} \hlstd{= women,} \hlkwc{pch} \hlstd{=} \hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-women-1} 

}



\end{knitrout}
        \caption{Relation entre la taille et le poids moyen de femmes américaines âgées de 30 à 39 ans (données \texttt{women})}
        \label{fig:simple:women}
      \end{figure}
    \item Le graphique montre qu'un modèle linéaire serait
      excellent. On estime les paramètres de ce modèle avec \texttt{lm}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(weight} \hlopt{~} \hlstd{height,} \hlkwc{data} \hlstd{= women))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Coefficients:
## (Intercept)       height  
##      -87.52         3.45
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item Voir la figure \ref{fig:simple:women2}.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{abline}\hlstd{(fit)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-women2-1} 

}



\end{knitrout}
        \caption{Relation entre les données \texttt{women} et droite de régression linéaire simple}
        \label{fig:simple:women2}
      \end{figure}
      On constate que l'ajustement est excellent.
    \item Le résultat de la fonction \texttt{summary} appliquée au
      modèle \texttt{fit} est le suivant:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = weight ~ height, data = women)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7333 -1.1333 -0.3833  0.7417  3.1167 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
## height        3.45000    0.09114   37.85 1.09e-14 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.525 on 13 degrees of freedom
## Multiple R-squared:  0.991,	Adjusted R-squared:  0.9903 
## F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14
\end{verbatim}
\end{kframe}
\end{knitrout}
      Le coefficient de détermination est donc
      $R^2 = 0,991$, %$
      ce qui est près de 1 et confirme donc l'excellent
      ajustement du modèle évoqué en c).
    \item On a
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{attach}\hlstd{(women)}
\hlstd{SST} \hlkwb{<-} \hlkwd{sum}\hlstd{((weight} \hlopt{-} \hlkwd{mean}\hlstd{(weight))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{SSR} \hlkwb{<-} \hlkwd{sum}\hlstd{((}\hlkwd{fitted}\hlstd{(fit)} \hlopt{-} \hlkwd{mean}\hlstd{(weight))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{SSE} \hlkwb{<-} \hlkwd{sum}\hlstd{((weight} \hlopt{-} \hlkwd{fitted}\hlstd{(fit))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlkwd{all.equal}\hlstd{(SST, SSR} \hlopt{+} \hlstd{SSE)}
\end{alltt}
\begin{verbatim}
## [1] TRUE
\end{verbatim}
\begin{alltt}
\hlkwd{all.equal}\hlstd{(}\hlkwd{summary}\hlstd{(fit)}\hlopt{$}\hlstd{r.squared, SSR}\hlopt{/}\hlstd{SST)}
\end{alltt}
\begin{verbatim}
## [1] TRUE
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans le contexte de la régression linéaire simple, démontrer que
  \begin{displaymath}
    \sum_{i=1}^n (\hat{Y}_i - \bar{Y}) \varepsilon_i = 0.
  \end{displaymath}
  \begin{sol}
    Puisque $\hat{Y}_i = (\bar{Y} - \hat{\beta}_1 \bar{x}) +
    \hat{\beta}_1 x_i = \bar{Y} + \hat{\beta}_1 (x_i - \bar{x})$ et
    que $\varepsilon_i = Y_i - \hat{Y}_i = (Y_i - \bar{Y}) - \hat{\beta}_1 (x_i
    - \bar{x})$, alors
    \begin{align*}
      \sum_{i = 1}^n (\hat{Y}_i - \bar{Y}) \varepsilon_i
      &= \hat{\beta}_1
      \left(
        \sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y}) -
        \hat{\beta}_1 \sum_{i = 1}^n (x_i - \bar{x})^2
      \right) \\
      & = \hat{\beta}_1
      \left(
        S_{xy} - \frac{S_{xy}}{S_{xx}}\, S_{xx}
      \right) \\
      & = 0.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Considérer le modèle de régression linéaire par rapport au temps
  $Y_t = \beta_0 + \beta_1 t + \varepsilon_t$, $t = 1, \dots, n$. Écrire
  les équations normales et obtenir les estimateurs des moindres
  carrés des paramètres $\beta_0$ et $\beta_1$. \emph{Note}:
  $\sum_{i=1}^n i^2 = n(n + 1)(2n + 1)/6$.
  \begin{rep}
    $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 (n + 1)/2$,
    $\hat{\beta}_1 = (12 \sum_{t = 0}^n t Y_t - 6 n (n + 1)
    \bar{Y})/(n (n^2 - 1)$
  \end{rep}
  \begin{sol}
    On a un modèle de régression linéaire simple usuel avec $x_t =
    t$. Les estimateurs des moindres carrés des paramètres $\beta_0$ et
    $\beta_1$ sont donc
    \begin{align*}
      \hat{\beta}_0
      &= \bar{Y} - \hat{\beta}_1\, \frac{\sum_{t = 1}^n t}{n} \\
      \intertext{et}
      \hat{\beta}_1
      &= \frac{\sum_{t = 1}^n t Y_t - \bar{Y} \sum_{t = 1}^n t}{\sum_{t
          = 1}^n t^2 - n^{-1} (\sum_{t = 1}^n t)^2}.
    \end{align*}
    Or, puisque $\sum_{t = 1}^n t = n(n + 1)/2$ et $\sum_{t = 1}^n t^2
    = n(n + 1)(2n + 1)/6$, les expressions ci-dessus se simplifient en
    \begin{align*}
      \hat{\beta}_0
      & = \bar{Y} - \hat{\beta}_1\, \frac{n + 1}{2} \\
      \intertext{et}
      \hat{\beta}_1
      & = \frac{\sum_{t=1}^n t Y_t - n(n + 1) \bar{Y}/2}{n(n + 1)(2n +
        1)/6 - n(n + 1)^2/4} \\
      & = \frac{12 \sum_{t=1}^n t Y_t - 6 n (n + 1) \bar{Y}}{n (n^2 - 1)}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simple:origine}
  \begin{enumerate}
  \item Trouver l'estimateur des moindres carrés du paramètre $\beta$
    dans le modèle de régression linéaire passant par l'origine $Y_i =
    \beta x_i + \varepsilon_i$, $i = 1, \dots, n$,
    $\esp{\varepsilon_i} = 0$, $\cov{\varepsilon_i, \varepsilon_j} =
    \delta_{ij} \sigma^2$.
  \item Démontrer que l'estimateur en a) est sans biais.
  \item Calculer la variance de l'estimateur en a).
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat{\beta}= \sum_{i = 1}^n x_i Y_i/\sum_{i = 1}^n x_i^2$
      \stepcounter{enumi}
    \item $\var{\hat{\beta}} = \sigma^2/\sum_{i = 1}^n x_i^2$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item L'estimateur des moindres carrés du paramètre $\beta$ est la
      valeur $\hat{\beta}$ minimisant la somme de carrés
      \begin{align*}
        S(\beta)
        &=\sum_{i = 1}^n \varepsilon_i^2 \\
        &=\sum_{i = 1}^n (Y_i - \beta x_i)^2.
      \end{align*}
      Or,
      \begin{displaymath}
        \frac{d}{d \beta}\, S(\beta) = -2 \sum_{i = 1}^n (Y_i -
        \hat{\beta} x_i) x_i,
      \end{displaymath}
      d'où l'unique équation normale de ce modèle est
      \begin{displaymath}
        \sum_{i = 1}^n x_i Y_i - \hat{\beta} \sum_{i=1}^n x_i^2 = 0.
      \end{displaymath}
      L'estimateur des moindres carrés de $\beta$ est donc
      \begin{displaymath}
        \hat{\beta} = \frac{\sum_{i=1}^n x_i Y_i}{\sum_{i=1}^n x_i^2}.
      \end{displaymath}
    \item On doit démontrer que $\esp{\hat{\beta}} = \beta$. On a
      \begin{align*}
        \esp{\hat{\beta}}
        &= \Esp{\frac{\sum_{i=1}^n x_i Y_i}{\sum_{i=1}^n x_i^2}} \\
        &= \frac{1}{\sum_{i=1}^n x_i^2} \sum_{i=1}^n x_i \esp{Y_i} \\
        &= \frac{1}{\sum_{i=1}^nx_i^2} \sum_{i=1}^n x_i \beta x_i \\
        &= \beta\, \frac{\sum_{i=1}^n x_i^2}{\sum_{i=1}^n x_i^2} \\
        &= \beta.
      \end{align*}
    \item Des hypothèses du modèle, on a
      \begin{align*}
        \var{\hat{\beta}}
        &= \Var{\frac{\sum_{i=1}^n x_i Y_i}{\sum_{i=1}^n x_i^2}} \\
        &= \frac{1}{(\sum_{i=1}^n x_i^2)^2} \sum_{i=1}^n x_i^2 \var{Y_i} \\
        &= \frac{\sigma^2}{(\sum_{i=1}^n x_i^2)^2} \sum_{i=1}^n x_i^2 \\
        &= \frac{\sigma^2}{\sum_{i=1}^n x_i^2}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que l'estimateur des moindres carrés $\hat{\beta}$ trouvé à
  l'exercice \ref{chap:simple}.\ref{ex:simple:origine} est
  l'estimateur sans biais à variance (uniformément) minimale du
  paramètre $\beta$.  En termes mathématiques: soit
  \begin{displaymath}
    \beta^* = \sum_{i=1}^n c_i Y_i
  \end{displaymath}
  un estimateur linéaire du paramètre $\beta$. Démontrer qu'en
  déterminant les coefficients $c_1, \dots, c_n$ de façon à minimiser
  \begin{displaymath}
    \var{\beta^*} = \Var{\sum_{i=1}^n c_i Y_i}
  \end{displaymath}
  sous la contrainte que
  \begin{displaymath}
    \esp{\beta^*} = \Esp{\sum_{i=1}^n c_i Y_i} = \beta,
  \end{displaymath}
  on obtient $\beta^* = \hat{\beta}$.
  \begin{sol}
    On veut trouver les coefficients $c_1, \dots, c_n$ tels que
    $\esp{\beta^*} = \beta$ et $\var{\beta^*}$ est minimale. On
    cherche donc à minimiser la fonction
    \begin{align*}
      f(c_1, \dots, c_n)
      &= \var{\beta^*} \\
      &= \sum_{i=1}^n c_i^2 \var{Y_i} \\
      &= \sigma^2 \sum_{i=1}^n c_i^2
    \end{align*}
    sous la contrainte $\esp{\beta^*} = \sum_{i=1}^nc_i\esp{Y_i} =
    \sum_{i=1}^nc_i\beta x_i = \beta\sum_{i=1}^nc_ix_i = \beta$, soit
    $\sum_{i=1}^n c_i x_i = 1$ ou $g(c_1, \dots, c_n) = 0$ avec
    \begin{displaymath}
      g(c_1, \dots, c_n) = \sum_{i=1}^n c_i x_i - 1.
    \end{displaymath}
    Pour utiliser la méthode des multiplicateurs de Lagrange, on pose
    \begin{align*}
      \mathcal{L}(c_1, \dots, c_n,\lambda)
      &= f(c_1, \dots, c_n) - \lambda g(c_1, \dots, c_n), \\
      &= \sigma^2 \sum_{i=1}^n c_i^2 - \lambda
      \left(
        \sum_{i=1}^n c_i x_i - 1
      \right),
    \end{align*}
    puis on dérive la fonction $\mathcal{L}$ par rapport à chacune des
    variables $c_1, \dots, c_n$ et $\lambda$. On trouve alors
    \begin{align*}
      \frac{\partial \mathcal{L}}{\partial c_u}
      & = 2 \sigma^2 c_u - \lambda x_u, \quad u = 1, \dots, n \\
      \frac{\partial \mathcal{L}}{\partial \lambda}
      & = - \sum_{i=1}^n c_i x_i + 1.
    \end{align*}
    En posant les $n$ premières dérivées égales à zéro, on obtient
    \begin{displaymath}
      c_i = \frac{\lambda x_i}{2 \sigma^2}.
    \end{displaymath}
    Or, de la contrainte,
    \begin{displaymath}
      \sum_{i=1}^n c_i x_i =
      \frac{\lambda}{2\sigma^2} \sum_{i=1}^n x_i^2 = 1,
    \end{displaymath}
    d'où
    \begin{displaymath}
      \frac{\lambda}{2 \sigma^2} = \frac{1}{\sum_{i=1}^n x_i^2}
    \end{displaymath}
    et, donc,
    \begin{displaymath}
      c_i = \frac{x_i}{\sum_{i=1}^n x_i^2}.
    \end{displaymath}
    Finalement,
    \begin{align*}
      \beta^*
      & = \sum_{i=1}^n c_i Y_i \\
      & = \frac{\sum_{i=1}^n x_i Y_i}{\sum_{i=1}^n x_i^2} \\
      & = \hat{\beta}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans le contexte de la régression linéaire simple, démontrer que
  \begin{enumerate}
  \item $\esp{\MSE} = \sigma^2$
  \item $\esp{\MSR} = \sigma^2 + \beta_1^2 \sum_{i=1}^n (x_i -
    \bar{x})^2$
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Tout d'abord, puisque $\MSE = \SSE/(n - 2) = \sum_{i=1}^n
      (Y_i - \hat{Y}_i)^2/(n - 2)$ et que $\esp{Y_i} =
      \esp{\hat{Y}_i}$, alors
      \begin{align*}
        \esp{MSE}
        &= \frac{1}{n - 2} \Esp{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2} \\
        &= \frac{1}{n - 2} \sum_{i=1}^n \esp{(Y_i - \hat{Y}_i)^2} \\
        &= \frac{1}{n - 2} \sum_{i=1}^n \esp{((Y_i - \esp{Y_i}) -
          (\hat{Y}_i - \esp{\hat{Y}_i}))^2} \\
        &= \frac{1}{n - 2} \sum_{i=1}^n
        \left(
          \var{Y_i} + \var{\hat{Y}_i} - 2\, \cov{Y_i, \hat{Y}_i}
        \right).
      \end{align*}
      Or, on a par hypothèse du modèle que $\cov{Y_i, Y_j} =
      \cov{\varepsilon_i, \varepsilon_j} = \delta_{ij} \sigma^2$, d'où
      $\var{Y_i} = \sigma^2$ et $\var{\bar{Y}} = \sigma^2/n$. D'autre
      part,
      \begin{align*}
        \var{\hat{Y}_i}
        &= \var{\bar{Y} + \hat{\beta}_1 (x_i - \bar{x})} \\
        &= \var{\bar{Y}} + (x_i - \bar{x})^2 \var{\hat{\beta}_1} +
        2 (x_i - \bar{x}) \cov{\bar{Y}, \hat{\beta}_1}
      \end{align*}
      et l'on sait que
      \begin{align*}
        \var{\hat{\beta}_1}
        &= \frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2} \\
        \intertext{et que}
        \cov{\bar{Y}, \hat{\beta}_1}
        & = \Cov{
          \frac{\sum_{i = 1}^n Y_i}{n},
          \frac{\sum_{j = 1}^n (x_j - \bar{x}) Y_j}{\sum_{j = 1}^n
            (x_j - \bar{x})^2}} \\
        &= \frac{1}{n \sum_{j = 1}^n (x_j - \bar{x})^2}
        \sum_{i = 1}^n \sum_{j = 1}^n \cov{Y_i, (x_j - \bar{x}) Y_j} \\
        &= \frac{1}{n \sum_{j = 1}^n (x_j - \bar{x})^2}
        \sum_{i = 1}^n (x_j - \bar{x}) \var{Y_i} \\
        & = \frac{\sigma^2}{n \sum_{j = 1}^n (x_j - \bar{x})^2}
        \sum_{i = 1}^n (x_i - \bar{x}) \\
        & = 0,
      \end{align*}
      puisque $\sum_{i=1}^n(x_i - \bar{x}) = 0$. Ainsi,
      \begin{displaymath}
        \var{\hat{Y}_i} = \frac{\sigma^2}{n} +
        \frac{(x_i - \bar{x})^2 \sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}.
      \end{displaymath}
      De manière similaire, on détermine que
      \begin{align*}
        \cov{Y_i, \hat{Y}_i}
        &= \cov{Y_i, \bar{Y} + \hat{\beta}_1 (x_i - \bar{x})} \\
        &= \cov{Y_i, \bar{Y}} +
        (x_i - \bar{x}) \cov{Y_i, \hat{\beta}_1} \\
        &= \frac{\sigma^2}{n} + \frac{(x_i -
          \bar{x})^2 \sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}.
      \end{align*}
      Par conséquent,
      \begin{align*}
        \esp{(Y_i - \hat{Y}_i)^2}
        &= \frac{n - 1}{n}\, \sigma^2 -
        \frac{(x_i - \bar{x})^2 \sigma^2}{\sum_{i = 1}^n (x_i - \bar{x})^2} \\
        \intertext{et}
        \sum_{i=1}^n \esp{(Y_i - \hat{Y}_i)^2}
        & = (n - 2) \sigma^2,
      \end{align*}
      d'où $\esp{\MSE} = \sigma^2$.
    \item On a
      \begin{align*}
        \esp{\MSR}
        &= \esp{\SSR} \\
        &= \Esp{\sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2} \\
        &= \sum_{i=1}^n \esp{\hat{\beta}_1^2 (x_i - \bar{x})^2} \\
        &= \sum_{i=1}^n (x_i - \bar{x})^2 \esp{\hat{\beta}_1^2} \\
        &= \sum_{i=1}^n (x_i - \bar{x})^2 (\var{\hat{\beta}_1} +
        \esp{\hat{\beta}_1}^2) \\
        &= \sum_{i=1}^n (x_i - \bar{x})^2
        \left(
          \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} + \beta_1^2
        \right) \\
        &= \sigma^2 + \beta_1^2 \sum_{i=1}^n (x_i - \bar{x})^2.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Supposons que les observations $(x_1, Y_1), \dots, (x_n, Y_n)$
  sont soumises à une transformation linéaire, c'est-à-dire que $Y_i$
  devient $Y_i^\prime = a + b Y_i$ et que $x_i$ devient $x_i^\prime =
  c + d x_i$, $i = 1, \dots, n$.
  \begin{enumerate}
  \item Trouver quel sera l'impact sur les estimateurs des moindres
    carrés des paramètres $\beta_0$ et $\beta_1$ dans le modèle de
    régression linéaire $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$.
  \item Démontrer que le coefficient de détermination $R^2$ n'est pas
    affecté par la transformation linéaire.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\hat{\beta}_1^\prime = (b/d) \hat{\beta}_1$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il faut exprimer $\hat{\beta}_0^\prime$ et
      $\hat{\beta}_1^\prime$ en fonction de $\hat{\beta}_0$ et
      $\hat{\beta}_1$. Pour ce faire, on trouve d'abord une expression
      pour chacun des éléments qui entrent dans la définition de
      $\hat{\beta}_1^\prime$. Tout d'abord,
      \begin{align*}
        \bar{x}^\prime
        &= \frac{1}{n} \sum_{i=1}^n x_i^\prime \\
        &= \frac{1}{n} \sum_{i=1}^n (c + d x_i) \\
        &= c + d \bar{x},
      \end{align*}
      et, de manière similaire, $\bar{Y}^\prime = a + b \bar{Y}$. Ensuite,
      \begin{align*}
        S_{xx}^\prime
        &= \sum_{i=1}^n (x_i^\prime - \bar{x}^\prime)^2 \\
        &= \sum_{i=1}^n (c + d x_i - c - d \bar{x})^2 \\
        &= d^2 S_{xx}
      \end{align*}
      et $S_{yy}^\prime = b^2 S_{yy}$, $S_{xy}^\prime = bd S_{xy}$.
      Par conséquent,
      \begin{align*}
        \hat{\beta}_1^\prime
        &= \frac{S_{xy}^\prime}{S_{xx}^\prime} \\
        &= \frac{bd S_{xy}}{d^2 S_{xx}} \\
        &= \frac{b}{d}\, \hat{\beta}_1 \\
        \intertext{et}
        \hat{\beta}_0^\prime
        &= \bar{Y}^\prime - \hat{\beta}_1^\prime \bar{x}^\prime \\
        &= a + b \bar{Y} - \frac{b}{d}\, \hat{\beta}_1 (c + d \bar{x}) \\
        &= a - \frac{bc}{d}\, \hat{\beta}_1 + b (\bar{Y} -
        \hat{\beta}_1 \bar{x}) \\
        &= a - \frac{bc}{d}\, \hat{\beta}_1 + b \hat{\beta}_0.
      \end{align*}
    \item Tout d'abord, on établit que
      \begin{align*}
        R^2
        &= \frac{\SSR}{\SST} \\
        &= \frac{\sum_{i=1}^n (\hat{Y_i} - \bar{Y})^2}{\sum_{i=1}^n
          (Y_i - \bar{Y})^2} \\
        &= \hat{\beta}_1^2\, \frac{\sum_{i=1}^n (x_i -
          \bar{x})^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2} \\
        &= \hat{\beta}_1^2\, \frac{S_{xx}}{S_{yy}}.
      \end{align*}
      Maintenant, avec les résultats obtenus en a), on démontre
      directement que
      \begin{align*}
        (R^2)^\prime
        &= (\hat{\beta}_1^\prime)^2 \frac{S_{xx}^\prime}{S_{yy}^\prime} \\
        &=
        \left(
          \frac{b}{d}
        \right)^2\,
        \hat{\beta}_1^2\, \frac{d^2 S_{xx}}{b^2 S_{yy}} \\
        &= \hat{\beta}_1^2\, \frac{S_{xx}}{S_{yy}} \\
        &= R^2.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  On sait depuis l'exercice \ref{chap:simple}.\ref{ex:simple:origine}
  que pour le modèle de régression linéaire simple passant par
  l'origine $Y_i = \beta x_i + \varepsilon_i$, l'estimateur des
  moindres carrés de $\beta$ est
  \begin{displaymath}
    \hat{\beta} = \frac{\sum_{i = 1}^n x_i Y_i}{\sum_{i = 1}^n x_i^2}.
  \end{displaymath}
  Démontrer que l'on peut obtenir ce résultat en utilisant la formule
  pour $\hat{\beta}_1$ dans la régression linéaire simple usuelle
  ($Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$) en ayant d'abord
  soin d'ajouter aux données un $(n + 1)${\ieme} point $(m\bar{x},
  m\bar{Y})$, où
  \begin{displaymath}
    m = \frac{n}{\sqrt{n + 1} - 1} = \frac{n}{a}.
  \end{displaymath}
  \begin{sol}
    Considérons un modèle de régression usuel avec l'ensemble de
    données $(x_1, Y_1), \dots, (x_n, Y_n), (m \bar{x}, m \bar{Y})$,
    où $\bar{x} = n^{-1} \sum_{i = 1}^n x_i$, $\bar{Y} = n^{-1}
    \sum_{i = 1}^n Y_i$, $m = n/a$ et $a = \sqrt{n + 1} - 1$. On
    définit
    \begin{align*}
      \bar{x}^\prime
      &= \frac{1}{n + 1} \sum_{i = 1}^{n + 1} x_i \\
      &= \frac{1}{n + 1} \sum_{i = 1}^n x_i + \frac{m}{n + 1} \bar{x} \\
      &= k \bar{x} \\
      \intertext{et, de manière similaire,}
      \bar{Y}^\prime
      &= k \bar{Y},
      \intertext{où}
      k
      &= \frac{n + m}{n + 1} \\
      &= \frac{n (a + 1)}{a (n + 1)}.
    \end{align*}
    L'expression pour l'estimateur des moindres carrés de la pente de
    la droite de régression pour cet ensemble de données est
    \begin{align*}
      \hat{\beta}_1
      &= \frac{\sum_{i = 1}^{n + 1} x_i Y_i - (n + 1)
        \bar{x}^\prime \bar{Y}^\prime}{%
        \sum_{i = 1}^{n + 1} x_i^2 - (n + 1) (\bar{x}^\prime)^2} \\
      &= \frac{\sum_{i = 1}^n x_i Y_i + m^2 \bar{x} \bar{Y} - (n + 1)
        k^2 \bar{x} \bar{Y}}{%
        \sum_{i = 1}^n x_i^2 + m^2 \bar{x}^2 - (n + 1) k^2 \bar{x}^2}.
    \end{align*}
    Or,
    \begin{align*}
      m^2 - k^2 (n + 1)
      &= \frac{n^2}{a^2} - \frac{n^2 (a + 1)^2}{a^2 (n + 1)} \\
      &= \frac{n^2 (n + 1) - n^2 (n + 1)}{a^2 (n + 1)} \\
      &= 0.
    \end{align*}
    Par conséquent,
    \begin{align*}
      \hat{\beta}_1
      &= \frac{\sum_{i = 1}^n x_i Y_i}{\sum_{i = 1}^n x_i^2} \\
      &= \hat{\beta}.
    \end{align*}
    Interprétation: en ajoutant un point bien spécifique à n'importe
    quel ensemble de données, on peut s'assurer que la pente de la
    droite de régression sera la même que celle d'un modèle passant
    par l'origine. Voir la figure \ref{fig:simple:pointmagique} pour
    une illustration du phénomène.

    \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-pointmagique-1} 
\includegraphics[width=.45\linewidth]{figure/fig-simple-pointmagique-2} 

}



\end{knitrout}
      \caption{Illustration de l'effet de l'ajout d'un point spécial à
        un ensemble de données. À gauche, la droite de régression
        usuelle. À droite, le même ensemble de points avec le point
        spécial ajouté (cercle plein), la droite de régression avec ce
        nouveau point (ligne pleine) et la droite de régression
        passant par l'origine (ligne pointillée). Les deux droites
        sont parallèles.}
      \label{fig:simple:pointmagique}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit le modèle de régression linéaire simple
  \begin{displaymath}
    Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad
    \varepsilon_i \sim N(0, \sigma^2).
  \end{displaymath}
  Construire un intervalle de confiance de niveau $1 - \alpha$ pour le
  paramètre $\beta_1$ si la variance $\sigma^2$ est connue.
  \begin{rep}
    $\beta_1 \in \hat{\beta}_1 \pm z_{\alpha/2} \sigma
    \left( \sum_{i=1}^n (x_i - \bar{x})^2 \right)^{-1/2}$
  \end{rep}
  \begin{sol}
    Puisque, selon le modèle, $\varepsilon_i \sim N(0, \sigma^2)$ et
    que $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$, alors $Y_i \sim
    N(\beta_0 + \beta_1 x_i, \sigma^2)$. De plus, on sait que
    \begin{align*}
      \hat{\beta}_1
      &= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{%
        \sum_{i=1}^n (x_i - \bar{x})^2} \\
      &= \frac{\sum_{i=1}^n (x_i - \bar{x}) Y_i}{%
        \sum_{i=1}^n (x_i - \bar{x})^2},
    \end{align*}
    donc l'estimateur $\hat{\beta}_1$ est une combinaison linéaire des
    variables aléatoires $Y_1, \dots, Y_n$. Par conséquent,
    $\hat{\beta}_1 \sim N(\esp{\hat{\beta}_1}, \var{\hat{\beta}_1})$,
    où $\esp{\hat{\beta}_1} = \beta_1$ et $\var{\hat{\beta}_1} =
    \sigma^2/S_{xx}$ et, donc,
    \begin{displaymath}
      \Pr
      \left[
        -z_{\alpha/2} <
        \frac{\hat{\beta}_1 - \beta_1}{\sigma/\sqrt{S_{xx}}} <
        z_{\alpha/2}
      \right] = 1 - \alpha.
    \end{displaymath}
    Un intervalle de confiance de niveau $1 - \alpha$ pour le
    paramètre $\beta_1$ lorsque la variance $\sigma^2$ est connue est donc
    \begin{displaymath}
      \beta_1 \in \hat{\beta}_1 \pm z_{\alpha/2}
      \frac{\sigma}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2}}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Vous analysez la relation entre la consommation de gaz naturel
  \emph{per capita} et le prix du gaz naturel. Vous avez colligé les
  données de 20 grandes villes et proposé le modèle
  \begin{displaymath}
    Y = \beta_0 + \beta_1 x + \varepsilon,
  \end{displaymath}
  où $Y$ représente la consommation de gaz \emph{per capita}, $x$ le
  prix et $\varepsilon$ est le terme d'erreur aléatoire distribué
  selon une loi normale. Vous avez obtenu les résultats suivants:
  \begin{align*}
    \hat{\beta}_0 &= 138,581 &
      \sum_{i=1}^{20} (x_i - \bar{x})^2 &= \nombre{10668} \\
    \hat{\beta}_1 &= -1,104 &
      \sum_{i=1}^{20} (Y_i - \bar{Y})^2 &= \nombre{20838} \\
    \sum_{i=1}^{20} x_i^2 &= \nombre{90048} &
      \sum_{i=1}^{20} \varepsilon_i^2 &= \nombre{7832}. \\
    \sum_{i=1}^{20} Y_i^2 &= \nombre{116058}
  \end{align*}
  Trouver le plus petit intervalle de confiance à 95~\% pour le
  paramètre $\beta_1$.
  \begin{rep}
    $(-1,5, -0,7)$
  \end{rep}
  \begin{sol}
    L'intervalle de confiance pour $\beta_1$ est
    \begin{align*}
      \beta_1
      &\in \hat{\beta}_1 \pm t_{\alpha/2}(n - 2)
      \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}} \\
      &\in \hat{\beta}_1 \pm t_{0,025}(20 - 2) \sqrt{\frac{MSE}{S_{xx}}}.
     \end{align*}
     On nous donne $\SST = S_{yy} = \nombre{20838}$ et $S_{xx} =
     \nombre{10668}$. Par conséquent,
     \begin{align*}
       \SSR
       &= \hat{\beta}_1^2 \sum_{i=1}^{20} (x_i - \bar{x})^2 \\
       &= (-1,104)^2(\nombre{10668}) \\
       &= \nombre{13002,33} \\
       \SSE
       &= \SST - \SSR \\
       &= \nombre{7835,67} \\
       \intertext{et}
       \MSE
       &= \frac{\SSE}{18} \\
       &= 435,315.
     \end{align*}
     De plus, on trouve dans une table de quantiles de la loi de
     Student (ou à l'aide de la fonction \texttt{qt} dans \textsf{R})
     que $t_{0,025}(18) = 2,101$. L'intervalle de confiance recherché
     est donc
     \begin{align*}
       \beta_1
       &\in -1,104 \pm 2,101 \sqrt{\frac{435,315}{\nombre{10668}}} \\
       &\in (-1,528, -0,680).
     \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le tableau ci-dessous présente les résultats de l'effet de la
  température sur le rendement d'un procédé chimique.
  \begin{center}
    \begin{tabular}{rr}
      \toprule
      $x$ & $Y$ \\
      \midrule
      $-5$ &  1 \\
      $-4$ &  5 \\
      $-3$ &  4 \\
      $-2$ &  7 \\
      $-1$ & 10 \\
        0  &  8 \\
        1  &  9 \\
        2  & 13 \\
        3  & 14 \\
        4  & 13 \\
        5  & 18 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item On suppose une relation linéaire simple entre la température
    et le rendement. Calculer les estimateurs des moindres carrés de
    l'ordonnée à l'origine et de la pente de cette relation.
  \item Établir le tableau d'analyse de variance et tester si la pente
    est significativement différente de zéro avec un niveau de
    confiance de \nombre{0,95}.
  \item Quelles sont les limites de l'intervalle de confiance à 95~\%
    pour la pente?
  \item Y a-t-il quelque indication qu'un meilleur modèle devrait être
    employé?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat{\beta}_0 = 9,273$, $\hat{\beta}_1 = 1,436$
    \item $t = 9,809$
    \item $(1,105, 1,768)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On trouve aisément les estimateurs de la pente et de
      l'ordonnée à l'origine de la droite de régression:
      \begin{align*}
        \hat{\beta}_1
        &= \frac{\sum_{i=1}^n x_i Y_i - n \bar{x}\bar{Y}}{%
          \sum_{i=1}^n x_i^2 - n \bar{x}^2} \\
        &= 1,436 \\
        \hat{\beta}_0
        &= \bar{Y} - \hat{\beta}_1 \bar{x} \\
        &= 9,273.
      \end{align*}
    \item Les sommes de carrés sont
      \begin{align*}
        \SST
        &= \sum_{i=1}^n Y_i^2 - n \bar{Y}^2 \\
        &= 1194 - 11 (9,273)^2 \\
        &= 248,18 \\
        \SSR
        &= \hat{\beta}_1^2
        \left(
          \sum_{i=1}^n x_i^2 - n \bar{x}^2
        \right) \\
        &= (1,436)^2 (110 - 11 (0)) \\
        &= 226,95
      \end{align*}
      et $\SSE = \SST - \SSR = 21,23$. Le tableau d'analyse de
      variance est donc le suivant:

      \begin{center}
        \begin{tabular}{lrrrc}
          \toprule
          Source
          & \multicolumn{1}{c}{SS}
          & \multicolumn{1}{c}{d.l.}
          & \multicolumn{1}{c}{MS}
          & Ratio F \\
          \midrule
          Régression & 226,95 &   1  & 226,95 & 96,21 \\
          Erreur     &  21,23 &   9  &   2,36 &  \\
          \midrule
          Total      & 248,18 &  10  &        & \\
          \bottomrule
        \end{tabular}
      \end{center}

      Or, puisque $t = \sqrt{F} = 9,809 > t_{\alpha/2}(n-2) =
      t_{0,025}(9) = 2,26$, on rejette l'hypothèse $H_0: \beta_1 =
      0$ soit, autrement dit, la pente est significativement
      différente de zéro.
    \item Puisque la variance $\sigma^2$ est inconnue, on l'estime par
      $s^2 = \MSE = 2,36$. On a alors
      \begin{align*}
        \beta_1
        &\in \hat{\beta}_1 \pm t_{\alpha/2}(n-2)
        \sqrt{\widehat{\mathrm{Var}}[\hat{\beta}_1]} \\
        &\in 1,436 \pm 2,26 \sqrt{\frac{2,36}{110}} \\
        &\in (1,105, 1,768).
      \end{align*}
    \item Le coefficient de détermination de la régression est $R^2 =
      \SSR/\SST = 226,95/248,18 = 0,914$, ce qui indique que
      l'ajustement du modèle aux données est très bon. En outre, suite
      au test effectué à la partie b), on conclut que la régression
      est globalement significative.  Toutes ces informations portent
      à conclure qu'il n'y a pas lieu d'utiliser un autre modèle.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Y a-t-il une relation entre l'espérance de vie et la longueur de la
  «ligne de vie» dans la main? Dans un article de 1974 publié dans le
  \emph{Journal of the American Medical Association}, Mather et Wilson
  dévoilent les 50 observations contenues dans le fichier
  \texttt{lifeline.dat}. À la lumière de ces données, y a-t-il, selon
  vous, une relation entre la «ligne de vie» et l'espérance de vie?
  Vous pouvez utiliser l'information partielle suivante:
  \begin{align*}
    \sum_{i=1}^{50} x_i &= \nombre{3333} &
    \sum_{i=1}^{50} x_i^2 &= \nombre{231933} &
    \sum_{i=1}^{50} x_i Y_i &= \nombre{30549,75} \\
    \sum_{i=1}^{50} Y_i &= \nombre{459,9} &
    \sum_{i=1}^{50} Y_i^2 &= \nombre{4308,57}.
  \end{align*}
  \begin{rep}
    $F = 0,73$, valeur $p$: $0,397$
  \end{rep}
  \begin{sol}
    On doit déterminer si la régression est significative, ce qui peut
    se faire à l'aide de la statistique $F$. Or, à partir de
    l'information donnée dans l'énoncé, on peut calculer
    \begin{align*}
      \hat{\beta}_1
      &= \frac{\sum_{i=1}^{50} x_i Y_i - 50 \bar{x} \bar{Y}}{%
        \sum_{i=1}^{50} x_i^2 - 50 \bar{x})^2} \\
      &= -0,0110 \\
      \SST
      &= \sum_{i=1}^{50} Y_i^2 - 50 \bar{Y}^2 \\
      &= 78,4098 \\
      \SSR
      &= \hat{\beta}_1^2 \sum_{i=1}^{50} (x_i - \bar{x})^2 \\
      &= 1,1804 \\
      \SSE
      &= \SST - \SSR \\
      &= 77,2294 \\
      \intertext{d'où}
      \MSR
      &= 1,1804 \\
      \MSE
      &= \frac{\SSE}{50 - 2} \\
      &= 1,6089 \\
      \intertext{et, enfin,}
      F
      &= \frac{\MSR}{\MSE} \\
      &= 0,7337.
    \end{align*}
    Soit $F$ une variable aléatoire ayant une distribution de Fisher
    avec 1 et 48 degrés de liberté, soit la même distribution que la
    statistique $F$ sous l'hypothèse $H_0: \beta_1 = 0$. On a que
    $\Pr[F > 0,7337] = 0,3959$, donc la valeur $p$ du test $H_0:
    \beta_1 = 0$ est $0,3959$. Une telle valeur $p$ est généralement
    considérée trop élevée pour rejeter l'hypothèse $H_0$. On ne peut
    donc considérer la relation entre la ligne de vie et l'espérance
    de vie comme significative. (Ou on ne la considère significative
    qu'avec un niveau de confiance de $1 - p = 60,41$~\%.)
  \end{sol}
\end{exercice}

\begin{exercice}
  Considérer le modèle de régression linéaire passant par l'origine
  présenté à l'exercice \ref{chap:simple}.\ref{ex:simple:origine}.
  Soit $x_0$ une valeur de la variable indépendante, $Y_0$ la vraie
  valeur de la variable indépendante correspondant à $x_0$ et
  $\hat{Y}_0$ la prévision (ou estimation) de $Y_0$. En supposant que
  \begin{enumerate}[i)]
  \item $\varepsilon_i \sim N(0, \sigma^2)$;
  \item $\cov{\varepsilon_0, \varepsilon_i} = 0$ pour tout $i = 1,
    \dots, n$;
  \item $\var{\varepsilon_i} = \sigma^2$ est estimé par $s^2$,
  \end{enumerate}
  construire un intervalle de confiance de niveau $1 - \alpha$ pour
  $Y_0$. Faire tous les calculs intermédiaires.
  \begin{rep}
    $\hat{Y}_0 \pm t_{\alpha/2}(n - 1)\, s\,
    \sqrt{1 + x_0^2/\sum_{i=1}^n x_i^2}$
  \end{rep}
  \begin{sol}
    Premièrement, selon le modèle de régression passant par l'origine,
    $Y_0 = \beta x_0 + \varepsilon_0$ et $\hat{Y}_0 = \hat{\beta}
    x_0$. Considérons, pour la suite, la variable aléatoire $Y_0 -
    \hat{Y}_0$. On voit facilement que $\esp{\hat{\beta}} = \beta$,
    d'où $\esp{Y_0 - \hat{Y}_0} = \esp{\beta x_0 + \varepsilon_0 -
      \hat{\beta} x_0} = \beta x_0 - \beta x_0 = 0$ et
    \begin{displaymath}
      \var{Y_0 - \hat{Y}_0} = \var{Y_0} + \var{\hat{Y}_0} - 2\,
      \cov{Y_0, \hat{Y}_0}.
    \end{displaymath}
    Or, $\cov{Y_0, \hat{Y}_0} = 0$ par l'hypothèse ii) de l'énoncé,
    $\var{Y_0} = \sigma^2$ et $\var{\hat{Y}_0} = x_0^2\,
    \var{\hat{\beta}}$. De plus,
    \begin{align*}
      \var{\hat{\beta}}
      &= \frac{1}{(\sum_{i=1}^n x_i^2)^2} \sum_{i=1}^n x_i^2\,
      \var{Y_i} \\
      &= \frac{\sigma^2}{\sum_{i=1}^n x_i^2}
    \end{align*}
    d'où, finalement,
    \begin{displaymath}
      \var{Y_0 - \hat{Y}_0} =
      \sigma^2 \left( 1 + \frac{x_0^2}{\sum_{i=1}^n x_i^2} \right).
    \end{displaymath}
    Par l'hypothèse de normalité et puisque $\hat{\beta}$ est une
    combinaison linéaire de variables aléatoires normales,
    \begin{displaymath}
      Y_0 - \hat{Y}_0 \sim N
      \left(
        0, \sigma^2 \left( 1 + \frac{x_0^2}{\sum_{i=1}^n x_i^2} \right)
      \right)
    \end{displaymath}
    ou, de manière équivalente,
    \begin{displaymath}
      \frac{Y_0 - \hat{Y}_0}{\sigma \sqrt{1 + x_0^2/\sum_{i=1}^n x_i^2}}
      \sim N(0, 1).
    \end{displaymath}
    Lorsque la variance $\sigma^2$ est estimée par $s^2$, alors
    \begin{displaymath}
      \frac{Y_0 - \hat{Y}_0}{s \sqrt{1 + x_0^2/\sum_{i=1}^n x_i^2}}
      \sim t(n - 1).
    \end{displaymath}
    La loi de Student a $n - 1$ degrés de liberté puisque le modèle
    passant par l'origine ne compte qu'un seul paramètre. Les bornes
    de l'intervalle de confiance pour la vraie valeur de $Y_0$ sont
    donc
    \begin{displaymath}
      \hat{Y}_0 \pm t_{\alpha/2}(n - 1)\, s\, \sqrt{1 +
        \frac{x_0^2}{\sum_{i=1}^n x_i^2}}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  La masse monétaire et le produit national brut (en millions de
  \emph{snouks}) de la Fictinie (Asie postérieure) sont reproduits dans le
  tableau ci-dessous.
  \begin{center}
    \begin{tabular}{ccr}
      \toprule
      Année & Masse monétaire & \multicolumn{1}{c}{PNB} \\
      \midrule
      1987 & 2,0 & 5,0 \\
      1988 & 2,5 & 5,5 \\
      1989 & 3,2 & 6,0 \\
      1990 & 3,6 & 7,0 \\
      1991 & 3,3 & 7,2 \\
      1992 & 4,0 & 7,7 \\
      1993 & 4,2 & 8,4 \\
      1994 & 4,6 & 9,0 \\
      1995 & 4,8 & 9,7 \\
      1996 & 5,0 & 10,0 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item Établir une relation linéaire dans laquelle la masse monétaire
    explique le produit national brut (PNB).
  \item Construire des intervalles de confiance pour l'ordonnée à
    l'origine et la pente estimées en a). Peut-on rejeter l'hypothèse
    que la pente est nulle? Égale à 1?
  \item Si, en tant que ministre des Finances de la Fictinie, vous
    souhaitez que le PNB soit de 12,0 en 1997, à combien fixeriez-vous
    la masse monétaire?
  \item Pour une masse monétaire telle que fixée en c), déterminer
    les bornes inférieure et supérieure à l'intérieur desquelles
    devrait, avec une probabilité de 95~\%, se trouver le PNB moyen.
    Répéter pour la valeur du PNB de l'année 1997.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\text{PNB} = 1,168 + 1,716 \text{ MM}$
    \item $\beta_0 \in (0,060, 2,276)$, $\beta_1 \in (1,427, 2,005)$
    \item $6,31$
    \item $(11,20, 12,80)$ et $(10,83, 13,17)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Soit $x_1, \dots, x_{10}$ les valeurs de la masse monétaire
      et $Y_1, \dots, Y_{10}$ celles du PNB. On a $\bar{x} = 3,72$,
      $\bar{Y} = 7,55$, $\sum_{i = 1}^{10} x_i^2 = 147,18$, $\sum_{t =
        1}^{10} Y_i^2 = 597,03$ et $\sum_{i = 1}^{10} x_i Y_i =
      295,95$. Par conséquent,
      \begin{align*}
        \hat{\beta}_1
        &= \frac{\sum_{i=1}^{10} x_i Y_i - 10 \bar{x} \bar{Y}}{%
          \sum_{i=1}^{10} x_i^2 - 10 \bar{x}^2} \\
        &= 1,716 \\
        \intertext{et}
        \hat{\beta}_0
        &= \bar{Y} - \hat{\beta}_1 \bar{x} \\
        &= 1,168.
      \end{align*}
      On a donc la relation linéaire $\text{PNB} = 1,168 + 1,716
      \text{ MM}$.
    \item Tout d'abord, on doit calculer l'estimateur $s^2$ de la
      variance car cette quantité entre dans le calcul des intervalles
      de confiance demandés. Pour les calculs à la main, on peut
      éviter de calculer les valeurs de $\hat{Y}_1, \dots,
      \hat{Y}_{10}$ en procédant ainsi:
      \begin{align*}
        \SST
        &= \sum_{i=1}^{10} Y_i^2 - 10 \bar{Y}^2 \\
        &= 27,005 \\
        \SSR
        &= \hat{\beta}_1^2
        \left(
          \sum_{i=1}^{10} x_i^2 - 10 \bar{x}^2
        \right) \\
        &= 25,901,
      \end{align*}
      puis $\SSE = \SST - \SSR = 1,104$ et $s^2 = \MSE = \SSE/(10 - 2)
      = 0,1380$.  On peut maintenant construire les intervalles de
      confiance:
      \begin{align*}
        \beta_0
        &\in \hat{\beta}_0 \pm t_{\alpha/2}(n - 2)\, s\,
        \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}} \\
        &\in 1,168 \pm (2,306) (0,3715)
        \sqrt{\frac{1}{10} + \frac{3,72^2}{8,796}} \\
        &\in (0,060, 2,276) \\
        \beta_1
        &\in \hat{\beta}_1 \pm t_{\alpha/2}(n - 2)\, s\,
        \sqrt{\frac{1}{S_{xx}}} \\
        &\in 1,716 \pm (2,306) (0,3715) \sqrt{\frac{1}{8,796}} \\
        &\in (1,427, 2,005).
      \end{align*}
      Puisque l'intervalle de confiance pour la pente $\beta_1$ ne
      contient ni la valeur 0, ni la valeur 1, on peut rejeter, avec
      un niveau de confiance de 95~\%, les hypothèses $H_0: \beta_1 =
      0$ et $H_0: \beta_1 = 1$.
    \item Par l'équation obtenue en a) liant le PNB à la masse
      monétaire (MM), un PNB de 12,0 correspond à une masse monétaire
      de
      \begin{align*}
        \text{MM}
        &= \frac{12,0 - 1,168}{1,716} \\
        &= 6,31.
      \end{align*}
    \item On cherche un intervalle de confiance pour la droite de
      régression en $\text{MM}_{1997} = 6,31$ ainsi qu'un intervalle
      de confiance pour la prévision $\text{PNB} = 12,0$ associée à
      cette même valeur de la masse monétaire.  Avec une probabilité
      de $\alpha = 95~\%$, le PNB moyen se trouve dans l'intervalle
      \begin{displaymath}
        12,0 \pm t_{\alpha/2}(n - 2)\, s\,
        \sqrt{\frac{1}{n} + \frac{(6,31 - \bar{x})^2}{S_{xx}}} =
        (11,20, 12,80),
      \end{displaymath}
      alors que la vraie valeur du PNB se trouve dans l'intervalle
      \begin{displaymath}
        12,0 \pm t_{\alpha/2}(n - 2)\, s\,
        \sqrt{1 + \frac{1}{n} + \frac{(6,31 - \bar{x})^2}{S_{xx}}} =
        (10,83, 13,17).
      \end{displaymath}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le fichier \texttt{house.dat} contient diverses données relatives à
  la valeur des maisons dans la région métropolitaine de Boston. La
  signification des différentes variables se trouve dans le fichier.
  Comme l'ensemble de données est plutôt grand (506 observations pour
  chacune des 13 variables), répondre aux questions suivantes à l'aide
  de \textsf{R}.
  \begin{enumerate}
  \item Déterminer à l'aide de graphiques à laquelle des variables
    suivantes le prix médian des maisons (\texttt{medv}) est le plus
    susceptible d'être lié par une relation linéaire: le nombre moyen
    de pièces par immeuble (\texttt{rm}), la proportion d'immeubles
    construits avant 1940 (\texttt{age}), le taux de taxe foncière par
    \nombre{10000}~\$ d'évaluation (\texttt{tax}) ou le pourcentage de
    population sous le seuil de la pauvreté (\texttt{lstat}).

    \emph{Astuce}: en supposant que les données se trouvent dans
    le \emph{data frame} \texttt{house}, essayer les commandes suivantes:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(house)}
\hlkwd{attach}\hlstd{(house)}
\hlkwd{plot}\hlstd{(}\hlkwd{data.frame}\hlstd{(rm, age, lstat, tax, medv))}
\hlkwd{detach}\hlstd{(house)}
\hlkwd{plot}\hlstd{(medv} \hlopt{~} \hlstd{rm} \hlopt{+} \hlstd{age} \hlopt{+} \hlstd{lstat} \hlopt{+} \hlstd{tax,} \hlkwc{data} \hlstd{= house)}
\end{alltt}
\end{kframe}
\end{knitrout}
  \item Faire l'analyse complète de la régression entre le prix médian
    des maisons et la variable choisie en a), c'est-à-dire: calcul de
    la droite de régression, tests d'hypothèses sur les paramètres
    afin de savoir si la régression est significative, mesure de la
    qualité de l'ajustement et calcul de l'intervalle de confiance de la
    régression.
  \item Répéter l'exercice en b) en utilisant une variable ayant été
    rejetée en a). Observer les différences dans les résultats.
  \end{enumerate}
  \begin{sol}

    \begin{enumerate}
    \item Les données du fichier \texttt{house.dat} sont importées
      dans \textsf{R} avec la commande
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{house} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"house.dat"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
      La figure \ref{fig:simple:house} contient les graphiques de
      \texttt{medv} en fonction de chacune des variables \texttt{rm},
      \texttt{age}, \texttt{lstat} et \texttt{tax}. Le meilleur choix
      de variable explicative pour le prix médian semble être le
      nombre moyen de pièces par immeuble, \texttt{rm}.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(medv} \hlopt{~} \hlstd{rm} \hlopt{+} \hlstd{age} \hlopt{+} \hlstd{lstat} \hlopt{+} \hlstd{tax,} \hlkwc{data} \hlstd{= house,} \hlkwc{ask} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-house-1} 
\includegraphics[width=.45\linewidth]{figure/fig-simple-house-2} 
\includegraphics[width=.45\linewidth]{figure/fig-simple-house-3} 
\includegraphics[width=.45\linewidth]{figure/fig-simple-house-4} 

}



\end{knitrout}
        \caption{Relation entre la variable \texttt{medv} et les
          variables \texttt{rm}, \texttt{age}, \texttt{lstat} et
          \texttt{tax} des données \texttt{house.dat}}
        \label{fig:simple:house}
      \end{figure}
    \item Les résultats ci-dessous ont été obtenus avec \textsf{R}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit1} \hlkwb{<-} \hlkwd{lm}\hlstd{(medv} \hlopt{~} \hlstd{rm,} \hlkwc{data} \hlstd{= house)}
\hlkwd{summary}\hlstd{(fit1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ rm, data = house)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -23.346  -2.547   0.090   2.986  39.433 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -34.671      2.650  -13.08   <2e-16 ***
## rm             9.102      0.419   21.72   <2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.616 on 504 degrees of freedom
## Multiple R-squared:  0.4835,	Adjusted R-squared:  0.4825 
## F-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}

      On peut voir que tant l'ordonnée à l'origine que la pente sont
      très significativement différentes de zéro. La régression est
      donc elle-même significative. Cependant, le coefficient de
      détermination n'est que de $R^2 =
      0,4835$, %$
      ce qui indique que d'autres facteurs pourraient expliquer la
      variation dans \texttt{medv}.

      On calcule les bornes de l'intervalle de confiance de la
      régression avec la fonction \texttt{predict}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pred.ci} \hlkwb{<-} \hlkwd{predict}\hlstd{(fit1,} \hlkwc{interval} \hlstd{=} \hlstr{"confidence"}\hlstd{,} \hlkwc{level} \hlstd{=} \hlnum{0.95}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

      La droite de régression et ses bornes d'intervalle de confiance
      inférieure et supérieure sont illustrée à la figure
      \ref{fig:simple:house2}.
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ord} \hlkwb{<-} \hlkwd{order}\hlstd{(house}\hlopt{$}\hlstd{rm)}
\hlkwd{plot}\hlstd{(medv} \hlopt{~} \hlstd{rm,} \hlkwc{data} \hlstd{= house,} \hlkwc{ylim} \hlstd{=} \hlkwd{range}\hlstd{(pred.ci))}
\hlkwd{matplot}\hlstd{(house}\hlopt{$}\hlstd{rm[ord], pred.ci[ord,],}
        \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{2}\hlstd{),} \hlkwc{lwd}\hlstd{=} \hlnum{2}\hlstd{,}
        \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{add} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-house2-1} 

}



\end{knitrout}
        \caption{Résultat de la régression de la variable \texttt{rm} sur la variable \texttt{medv} des données \texttt{house.dat}}
        \label{fig:simple:house2}
      \end{figure}
    \item On reprend la même démarche, mais cette fois avec la
      variable \texttt{age}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit2} \hlkwb{<-} \hlkwd{lm}\hlstd{(medv} \hlopt{~} \hlstd{age,} \hlkwc{data} \hlstd{= house)}
\hlkwd{summary}\hlstd{(fit2)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = medv ~ age, data = house)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.097  -5.138  -1.958   2.397  31.338 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 30.97868    0.99911  31.006   <2e-16 ***
## age         -0.12316    0.01348  -9.137   <2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8.527 on 504 degrees of freedom
## Multiple R-squared:  0.1421,	Adjusted R-squared:  0.1404 
## F-statistic: 83.48 on 1 and 504 DF,  p-value: < 2.2e-16
\end{verbatim}
\begin{alltt}
\hlstd{pred.ci} \hlkwb{<-} \hlkwd{predict}\hlstd{(fit2,} \hlkwc{interval} \hlstd{=} \hlstr{"confidence"}\hlstd{,} \hlkwc{level} \hlstd{=} \hlnum{0.95}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
      La régression est encore une fois très significative. Cependant,
      le $R^2$ est encore plus faible qu'avec la variable
      \texttt{rm}. Les variables \texttt{rm} et \texttt{age}
      contribuent donc chacune à expliquer les variations de la
      variable \texttt{medv} (et \texttt{rm} mieux que \texttt{age}),
      mais aucune ne sait le faire seule de manière satisfaisante. La
      droite de régression et l'intervalle de confiance de celle-ci
      sont reproduits à la figure \ref{fig:simple:house3}. On constate
      que l'intervalle de confiance est plus large qu'en b).
      \begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ord} \hlkwb{<-} \hlkwd{order}\hlstd{(house}\hlopt{$}\hlstd{age)}
\hlkwd{plot}\hlstd{(medv} \hlopt{~} \hlstd{age,} \hlkwc{data} \hlstd{= house,} \hlkwc{ylim} \hlstd{=} \hlkwd{range}\hlstd{(pred.ci))}
\hlkwd{matplot}\hlstd{(house}\hlopt{$}\hlstd{age[ord], pred.ci[ord,],}
        \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{2}\hlstd{),} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,}
        \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{add} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-house3-1} 

}



\end{knitrout}
        \caption{Résultat de la régression de la variable \texttt{age} sur la variable \texttt{medv} des données \texttt{house.dat}}
        \label{fig:simple:house3}
      \end{figure}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simple:carburant}
  On veut prévoir la consommation de carburant d'une automobile à
  partir de ses différentes caractéristiques physiques, notamment le
  type du moteur. Le fichier \texttt{carburant.dat} contient des
  données tirées de \emph{Consumer Reports} pour 38 automobiles des
  années modèle 1978 et 1979. Les caractéristiques fournies sont
  \begin{itemize}
  \item \texttt{mpg}: consommation de carburant en milles au gallon;
  \item \texttt{nbcyl}: nombre de cylindres (remarquer la forte
    représentation des 8 cylindres!);
  \item \texttt{cylindree}: cylindrée du moteur, en pouces cubes;
  \item \texttt{cv}: puissance en chevaux vapeurs;
  \item \texttt{poids}: poids de la voiture en milliers de livres.
  \end{itemize}
  Utiliser \textsf{R} pour faire l'analyse ci-dessous.
  \begin{enumerate}
  \item Convertir les données du fichier en unités métriques, le cas
    échéant. Par exemple, la consommation de carburant s'exprime en
    $\ell$/100~km.  Or, un gallon américain correspond à 3,785 litres
    et 1~mille à 1,6093 kilomètre. La consommation en litres aux
    100~km s'obtient donc en divisant 235,1954 par la consommation en
    milles au gallon.  De plus, 1~livre correspond à 0,45455
    kilogramme.
  \item Établir une relation entre la consommation de carburant d'une
    voiture et son poids. Vérifier la qualité de l'ajustement du
    modèle et si le modèle est significatif.
  \item Trouver un intervalle de confiance à 95~\% pour la
    consommation en carburant d'une voiture de \nombre{1350}~kg.
  \end{enumerate}

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $R^2 = 0,858$ et
      $F = 217,5$
    \item $10,57 \pm
      2,13$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On importe les données dans \textsf{R}, puis on effectue les
      conversions demandées. La variable \texttt{consommation}
      contient la consommation des voitures en $\ell$/100~km et la
      variable \texttt{poids} le poids en kilogrammes.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{carburant} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"carburant.dat"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{consommation} \hlkwb{<-} \hlnum{235.1954}\hlopt{/}\hlstd{carburant}\hlopt{$}\hlstd{mpg}
\hlstd{poids} \hlkwb{<-} \hlstd{carburant}\hlopt{$}\hlstd{poids} \hlopt{*} \hlnum{0.45455} \hlopt{*} \hlnum{1000}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item La fonction \texttt{summary} fournit l'information
      essentielle pour juger de la validité et de la qualité du
      modèle:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(consommation} \hlopt{~} \hlstd{poids)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = consommation ~ poids)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.07123 -0.68380  0.01488  0.44802  2.66234 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -0.0146530  0.7118445  -0.021    0.984    
## poids        0.0078382  0.0005315  14.748   <2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.039 on 36 degrees of freedom
## Multiple R-squared:  0.858,	Adjusted R-squared:  0.854 
## F-statistic: 217.5 on 1 and 36 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
      Le modèle est donc le suivant: $Y_i =
      -0,01465 +
      0,007838 x_i +
      \varepsilon_i$, $\varepsilon_i \sim N(0,
      1,039^2)$, où $Y_i$ est la
      consommation en litres aux 100 kilomètres et $x_i$ le poids en
      kilogrammes. La faible valeur $p$ du test $F$ indique une
      régression très significative. De plus, le $R^2$ de
      0,858 %$
      confirme que l'ajustement du modèle est assez bon.
    \item On veut calculer un intervalle de confiance pour la
      consommation en carburant prévue d'une voiture de
      \nombre{1350}~kg. On obtient, avec la fonction \texttt{predict}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(fit,} \hlkwc{newdata} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{poids} \hlstd{=} \hlnum{1350}\hlstd{),} \hlkwc{interval} \hlstd{=} \hlstr{"prediction"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##       fit      lwr     upr
## 1 10.5669 8.432089 12.7017
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
On s'intéresse à l'impact du sexe sur l'espérance de vie. On connaît les durées de vie de $n_{F}=300$ femmes et $n_H=200$ hommes. On choisit d'utiliser la variable indicatrice 
$$x_{i}=\left\{ \begin{array}{ll}
0 & \mbox{, si \texttt{SEXE}}_i=\texttt{H}\\
1 & \mbox{, si \texttt{SEXE}}_i=\texttt{F}\\
\end{array}\right. .$$
On note $\bar{Y}_F$ la moyenne des durées de vie des femmes et $\bar{Y}_H$ la moyenne des durées de vie des hommes.

\begin{enumerate}
\item Montrer que l'estimateur des moindres carrés $\hat{\beta}_1$ (lié à la variable explicative $x$) est égal à $\bar{Y}_F-\bar{Y}_H$. \emph{Indice: On peut exprimer $\bar{Y}$ en termes de $\bar{Y}_F$ et $\bar{Y}_H$.} 

\item Ce résultat permet-il d'interpréter le coefficient relié à une variable catégorique binaire? Expliquer. 

\item Que représente $\hat{\beta}_0$ dans ce cas? 

\end{enumerate}
\begin{sol}
\begin{enumerate}
\item On a $$\bar{Y}=\frac{\sum_{i=1}^{500}Y_i}{500}=\frac{300\bar{Y}_F+200\bar{Y}_H}{500}.$$ Aussi,
$$\hat{\beta}_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^{500}x_iY_i-500\bar{x}\bar{Y}}{\sum_{i=1}^{500}x_i^2-500\bar{x}^2}.$$  Or, 
\begin{align*}
\bar{x}&=\frac{\sum_{i=1}^{500}x_i}{500}=\frac{300}{500},\\
\sum_{i=1}^{500}x_i^2&=300,\\
\sum_{i=1}^{500}x_i Y_i&=300\bar{Y}_F
\end{align*}
Donc, 
\begin{align*}
\hat{\beta}_1&=\frac{300\bar{Y}_F-500\times\frac{300}{500}\times\frac{300\bar{Y}_F+200\bar{Y}_H}{500}}{300-500\left(\frac{300}{500}\right)^2}\\
&=\frac{500\bar{Y}_F-300\bar{Y}_F-200\bar{Y}_H}{500-300}\\
&=\bar{Y}_F-\bar{Y}_H.
\end{align*}

\item Oui, le coefficient relié à la variable indicatrice qui vaut 1 si le sexe est F représente la différence etre la moyenne de l'espérance de vie pour les femmes et la moyenne de l'espérace de vie pour les hommes.

\item
\begin{align*}
\hat{\beta}_0&=\bar{Y}-\hat{\beta}_1\bar{x}=\bar{Y}-(\bar{Y}_F-\bar{Y}_H)\frac{300}{500}=\bar{Y}_H.
\end{align*}
$\Rightarrow \hat{\beta}_0$ est la moyenne de l'espérance de vie pour les hommes.

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
On s'intéresse à la covariance entre deux résidus.

\begin{enumerate}
\item D'abord, trouver $\cov{Y_i,\hat{Y}_j}$. 

\item Puis, calculer $\cov{\hat{Y}_i,\hat{Y}_j}$. 

\item Déduire de a) et b) que $$\cov{\hat{\varepsilon}_i,\hat{\varepsilon}_j}=-\sigma^2\left(\frac{1}{n}+\frac{(x_i-\bar{x})(x_j-\bar{x})}{S_{xx}}\right).$$ 
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item
\begin{align*}
\cov{Y_i,\hat{Y}_j}&=\cov{Y_i,\hat{\beta}_0+\hat{\beta}_1x_j}\\
&=\cov{Y_i,\bar{Y}-\hat{\beta}_1\bar{x}+\hat{\beta}_1x_j}\\
&=\cov{Y_i,\bar{Y}}+(x_j-\bar{x})\cov{Y_i,\hat{\beta}_1} \mbox{ par indépendance des observations}\\
&=\frac{\sigma^2}{n}+\frac{(x_j-\bar{x})}{S_{xx}}\sum_{l=1}^n(x_l-\bar{x})\cov{Y_i,Y_l}\\
&=\frac{\sigma^2}{n}+\frac{(x_j-\bar{x})(x_i-\bar{x})}{S_{xx}}\sigma^2\mbox{ par indépendance des observations}.\\
\end{align*}

\item
\begin{align*}
\cov{\hat{Y}_i,\hat{Y}_j}&=\cov{\hat{\beta}_0+\hat{\beta}_1x_i,\hat{\beta}_0+\hat{\beta}_1x_j}\\
&=\var{\hat{\beta}_0}+(x_i+x_j)\cov{\hat{\beta}_0,\hat{\beta}_1}+x_ix_j\var{\hat{\beta}_1}\\
&=\sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right)-(x_i+x_j)\frac{\bar{x}\sigma^2}{S_{xx}}+x_ix_j\frac{\sigma^2}{S_{xx}}\\
&=...\\
&=\sigma^2\left(\frac{1}{n}+\frac{(x_j-\bar{x})(x_i-\bar{x})}{S_{xx}}\right).
\end{align*}

\item
\begin{align*}
\cov{\hat{\varepsilon}_i,\hat{\varepsilon}_j}&=\cov{Y_i-\hat{Y}_i,Y_j-\hat{Y}_j}\\
&=\cov{Y_i,Y_j}-\cov{Y_i,\hat{Y}_j}-\cov{\hat{Y}_i,Y_j}+\cov{\hat{Y}_i,\hat{Y}_j}\\
&=0-2\sigma^2\left(\frac{1}{n}+\frac{(x_j-\bar{x})(x_i-\bar{x})}{S_{xx}}\right)+\left(\frac{1}{n}+\frac{(x_j-\bar{x})(x_i-\bar{x})}{S_{xx}}\right)\\
&=-\sigma^2\left(\frac{1}{n}+\frac{(x_i-\bar{x})(x_j-\bar{x})}{S_{xx}}\right).
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Dans un graphiques des résidus en fonction des valeurs prédites, on observe de l'hétéroscédasticité. Après une analyse plus poussée, on note que la variance de $\hat{\varepsilon}_i$ est approximativement proportionnelle à $E[Y_i]^4$ . Proposer une transformation $g$ de la variable réponse qui permettra de stabiliser la variance.
\begin{sol}
Utiliser l'approximation de Taylor de premier ordre pour montrer que la variance de $g(Y)=1/Y$ est approximativement constante.
\end{sol}
\end{exercice}

\begin{exercice}
\label{ex:simple:bact}
Les données suivantes présentent le nombre moyen de bactéries vivantes dans une boîte de conserve de nourriture et le temps (en minutes) d'exposition à une chaleur de $300^{o}$F. \footnote{Source: D. Montgomery, E.A. Peck et G.G. Vining (2012). Introduction to Linear Regression Analysis. Fifth Edition. Wiley.}

\begin{center}
\begin{tabular}{cc}
\hline
Nombre de bactéries & Temps d'exposition (min) \\ \hline
175&1\\
108&2\\
95&3\\
82&4\\
71&5\\
50&6\\
49&7\\
31&8\\
28&9\\
17&10\\
16&11\\
11&12\\ \hline
\end{tabular}
\end{center}

\begin{enumerate}
\item Tracer un nuage de points des données. Est-ce qu'un modèle de régression linéaire semble adéquat?

\item Ajuster aux données un modèle de régression linéaire. Calculer les statistiques sommaires et produire les graphiques de résidus. Interpréter les résultats. Quelles sont vos conclusions par rapport à la validité du modèle de régression?

\item Identifier une transformation pour ces données afin d'utiliser adéquatement les méthodes de régression. Ajuster ce nouveau modèle et tester la validité de la régression.
\end{enumerate}



\begin{sol}
\begin{enumerate}
\item Figure~\ref{fig:simple:bact1} shows a scatter plot of the number of bacteria versus the minutes of exposure. The plot shows a straight line would be a reasonable model, but an even better model would capture the curvature. In fact, the plot shows that when the canned food is exposed to $300^{o}$ F for a long time, there is ultimately no bacteria left. This suggests a model that would capture the asymptotic behavior of the number of bacteria when the number of minutes of exposure increases. A linear model would continue to drive down the number of bacteria, eventually leading to negative values, which is nonsensical in this context.
  
\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact1-1} 

}



\end{knitrout}
\caption{Scatter Plot of the Number of Bacteria versus the Minutes of Exposure to $300^{o}$ F}
\label{fig:simple:bact1}
\end{figure}
      
\item A simple linear model is fitted to the data using \textsf{R}. Here is a summary of the model:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit1} \hlkwb{<-} \hlkwd{lm}\hlstd{(bact}\hlopt{~}\hlstd{min)}
\hlkwd{summary}\hlstd{(fit1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = bact ~ min)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -17.323  -9.890  -7.323   2.463  45.282 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   142.20      11.26  12.627 1.81e-07 ***
## min           -12.48       1.53  -8.155 9.94e-06 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 18.3 on 10 degrees of freedom
## Multiple R-squared:  0.8693,	Adjusted R-squared:  0.8562 
## F-statistic: 66.51 on 1 and 10 DF,  p-value: 9.944e-06
\end{verbatim}
\end{kframe}
\end{knitrout}
The fitted model is $$\hat{y}=142.20-12.48x,$$ where the parameters of the model are estimated by the best linear unbiased estimators. The ANOVA table is obtained using \textsf{R}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{anova}\hlstd{(fit1)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Response: bact
##           Df  Sum Sq Mean Sq F value    Pr(>F)    
## min        1 22268.8 22268.8  66.512 9.944e-06 ***
## Residuals 10  3348.1   334.8                      
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

In order to test for the significance of regression, we use the F-statistic. The F-statistic is 66.512, and it has 1 and 10 degrees of freedom, so the $p$-value is $$P[F_{(1,10)}>66.512]=9.944\times10^{-6} .$$ Since the $p$-value is much smaller than 1\%, there is enough evidence to reject the null hypothesis that $\beta_{1}=0$ at the 1\% level. The simple linear model is significant.

The value of $R^{2}$ is $86.93\%$. This is a high coefficient of correlation, it means that about 87\% of the variation in the number of bacteria in the canned food is explained by the minutes of exposure to $300^{o}$F. The model seems to perform well.

The Q-Q Plot of the studentized residuals is shown in Figure~\ref{fig:simple:bact2}. The line represents when the empirical quantiles are exactly equal to the standard normal quantiles. The normality assumption is seriously violated as the dots are clearly not on a straight line. This means there are serious flaws in the model, including the fact that the hypothesis tests are not reliable.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact2-1} 

}



\end{knitrout}
\caption{Q-Q Plot for Simple Linear Model in Problem \ref{chap:simple}.\ref{ex:simple:bact}} 
\label{fig:simple:bact2}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact3-1} 

}



\end{knitrout}
\caption{Residuals versus the Fitted Values for Simple Linear Model in Problem \ref{chap:simple}.\ref{ex:simple:bact}} \label{fig:simple:bact3}
\end{figure}

Figure~\ref{fig:simple:bact3} shows a plot of the studentized residuals versus the fitted values. The plot suggests a clear curve, which is usually an indicator of non-linearity. This is in line with the previous comments.

Finally, this model is inadequate and transformations on the response variables are required.

\item The Box-Cox method is used to determine which transformation is optimal. Figure~\ref{fig:simple:bact4} shows the plot of the log-likelihood function in terms of $\lambda$, for two different ranges of $\lambda$. It was obtained with the \textsf{R} commands:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{boxcox}\hlstd{(bact}\hlopt{~}\hlstd{min,} \hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlopt{-}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{,} \hlkwc{len} \hlstd{=} \hlnum{20}\hlstd{),} \hlkwc{plotit} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlkwd{boxcox}\hlstd{(bact}\hlopt{~}\hlstd{min,} \hlkwc{lambda} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlopt{-}\hlnum{0.2}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlkwc{len} \hlstd{=} \hlnum{20}\hlstd{),} \hlkwc{plotit} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact4-1} 
\includegraphics[width=.45\linewidth]{figure/fig-simple-bact4-2} 

}



\end{knitrout}
\caption{Log-likelihood versus $\lambda$ in the Box-Cox method for Problem \ref{chap:simple}.\ref{ex:simple:bact}} \label{fig:simple:bact4}
\end{figure}

Note that the maximum is around 0.1 and 0 is included in the 95\% confidence interval for $\lambda$. Therefore, it is preferable to use 0 as this is a common transformation, it represents the logarithm transformation. Let $y^{*}=\ln(y)$. A simple linear model is fitted to the transformed data. The output is the following:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{logbact} \hlkwb{<-} \hlkwd{log}\hlstd{(bact)}
\hlstd{fit2} \hlkwb{<-} \hlkwd{lm}\hlstd{(logbact}\hlopt{~}\hlstd{min)}
\hlkwd{summary}\hlstd{(fit2)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = logbact ~ min)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.184303 -0.083994  0.001453  0.072825  0.206246 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  5.33878    0.07409   72.05 6.47e-15 ***
## min         -0.23617    0.01007  -23.46 4.49e-10 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.1204 on 10 degrees of freedom
## Multiple R-squared:  0.9822,	Adjusted R-squared:  0.9804 
## F-statistic: 550.3 on 1 and 10 DF,  p-value: 4.489e-10
\end{verbatim}
\end{kframe}
\end{knitrout}

The fitted model is $$\hat{y}^{*}=5.33878-0.23617x,$$ where the parameters of the model are estimated by the best linear unbiased estimators. Figure~\ref{fig:simple:bact5} is a scatter plot of the transformed response variable versus the covariate, along with the fitted line. The scatter plot looks much more linear now than in (a).

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact5-1} 

}



\end{knitrout}
\caption{Scatter Plot of the Logarithm of the Number of Bacteria versus the Minutes of Exposure to $300^{o}$ F}
\label{fig:simple:bact5}
\end{figure}

The ANOVA table is obtained using \textsf{R}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{anova}\hlstd{(fit2)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Response: logbact
##           Df Sum Sq Mean Sq F value    Pr(>F)    
## min        1 7.9761  7.9761  550.33 4.489e-10 ***
## Residuals 10 0.1449  0.0145                      
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}

The F-statistic for the test of significance of regression is 550.33, and it has 1 and 10 degrees of freedom, so the $p$-value is $$P[F_{(1,10)}>550.33]= 4.489\times10^{-10}.$$ Since the $p$-value is much smaller than 1\%, there is enough evidence to reject the null hypothesis that $\beta_{1}=0$ at the 1\% level. This model is significant.

The value of $R^{2}$ is very high at $98.22\%$. This means that about 98\% of the variation in the log of the number of bacteria in the canned food is explained by the minutes of exposure to $300^{o}$F. The model seems to perform very well, better than the model proposed in (b).

The Q-Q Plot of the studentized residuals is shown in Figure~\ref{fig:simple:bact6}. The dots are beautifully aligned with the standard normal quantiles. The normality assumption is appropriate. Figure~\ref{fig:simple:bact7} shows a plot of the studentized residuals versus the fitted values. The dots can be contained in horizontal bands and looks randomly scattered.

Finally, this model is adequate and the transformation used on the response variables fixed the problems in the model.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact6-1} 

}



\end{knitrout}
\caption{Q-Q Plot of Model for the Logarithm of the Number of Bacteria in Problem \ref{chap:simple}.\ref{ex:simple:bact}} \label{fig:simple:bact6}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-simple-bact7-1} 

}



\end{knitrout}
\caption{Residuals versus the Fitted Values for Model for the Logarithm of the Number of Bacteria in Problem \ref{chap:simple}.\ref{ex:simple:bact}} 
\label{fig:simple:bact7}
\end{figure}

\end{enumerate}
\end{sol}

\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-simple}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:


\chapter{Régression linéaire multiple}
\label{chap:multiple}

\Opensolutionfile{reponses}[reponses-multiple]
\Opensolutionfile{solutions}[solutions-multiple]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:multiple}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:multiple}}

\end{Filesave}


\begin{exercice}
  \label{ex:multiple:preuve}
  Considérer le modèle de régression linéaire $\mat{y} = \mat{X}
  \betavec + \epsvec$, où $\mat{X}$ est une matrice $n \times (p+1)$.
  Démontrer, en dérivant
  \begin{align*}
    S(\betavec)
    &= \sum_{t=1}^n (Y_t - \mat{x}_t^\prime \betavec)^2 \\
    &= (\mat{y} - \mat{X} \betavec)^\prime (\mat{y} - \mat{X} \betavec)
  \end{align*}
  par rapport à $\betavec$, que les équations normales à résoudre pour
  obtenir l'estimateur des moindres carrés de $\betavec$ sont, sous
  forme matricielle,
  \begin{displaymath}
    (\mat{X}^\prime \mat{X}) \hat\betavec = \mat{X}^\prime \mat{y},
  \end{displaymath}
  Déduire l'estimateur des moindres carrés de ces équations.
  \emph{Astuce}: utiliser le théorème
  \ref{thm:elements:derivee_fonction} de l'annexe \ref{chap:elements}.
  \begin{sol}
    Tout d'abord, selon le théorème
    \ref{thm:elements:derivee_fonction} de l'annexe
    \ref{chap:elements},
    \begin{displaymath}
      \frac{d}{d \mat{x}}\, f(\mat{x})^\prime \mat{A} f(\mat{x}) =
      2 \left( \frac{d}{d \mat{x}} f(\mat{x}) \right)^\prime \mat{A} f(\mat{x}).
    \end{displaymath}
    Il suffit, pour faire la démonstration, d'appliquer directement ce
    résultat à la forme quadratique
    \begin{displaymath}
      S(\betavec) = (\mat{y} - \mat{X} \betavec)^\prime (\mat{y} - \mat{X} \betavec)
    \end{displaymath}
    avec $f(\betavec) = \mat{y} - \mat{X} \betavec$ et $\mat{A} =
    \mat{I}$, la matrice identité. On a alors
    \begin{align*}
      \frac{d}{d\betavec} S(\betavec)
      &= 2
      \left(
        \frac{d}{d\betavec} (\mat{y} - \mat{X} \betavec)
      \right)^\prime
      \mat{y} - \mat{X} \betavec \\
      &= 2 (-\mat{X})^\prime (\mat{y} - \mat{X} \betavec)\\
      &= -2 \mat{X}^\prime (\mat{y} - \mat{X} \betavec).
    \end{align*}
    En posant ces dérivées exprimées sous forme matricielle
    simultanément égales à zéro, on obtient les équations normales à
    résoudre pour calculer l'estimateur des moindres carrés du vecteur
    $\betavec$, soit
    \begin{displaymath}
      \mat{X}^\prime \mat{X} \hat\betavec = \mat{X}^\prime \mat{y}.
    \end{displaymath}
    En isolant $\hat\betavec$ dans l'équation ci-dessus, on obtient,
    finalement, l'estimateur des moindres carrés:
    \begin{displaymath}
      \hat\betavec = (\mat{X}^\prime \mat{X})^{-1} \mat{X}^\prime \mat{y}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Pour chacun des modèles de régression ci-dessous, spécifier la
  matrice de schéma $\mat{X}$ dans la représentation $\mat{y} =
  \mat{X} \betavec + \epsvec$ du modèle, puis obtenir, si possible, les
  formules explicites des estimateurs des moindres carrés des
  paramètres.
  \begin{enumerate}
  \item $Y_t = \beta_0 + \varepsilon_t$
  \item $Y_t = \beta_1 X_t + \varepsilon_t$
  \item $Y_t = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} +
    \varepsilon_t$
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
      \item $\hat{\beta}_0 = \bar{Y}$
      \item $\hat{\beta}_1 = (\sum_{t=1}^n X_t Y_t)/(\sum_{t=1}^n X_t^2)$
      \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a un modèle sans variable explicative. Intuitivement, la
      meilleure prévision de $Y_t$ sera alors $\bar{Y}$. En effet,
      pour ce modèle,
      \begin{displaymath}
        \mat{X} =
        \begin{bmatrix}
          1 \\ \vdots \\ 1
        \end{bmatrix}_{n \times 1}
      \end{displaymath}
      et
      \begin{align*}
        \hat\betavec
        &=\left(\mat{X}^\prime \mat{X} \right)^{-1} \mat{X}^\prime
        \mat{y} \\
        &=
        \left(
          \begin{bmatrix}
            1 & \cdots & 1
          \end{bmatrix}
          \begin{bmatrix}
            1 \\ \vdots \\ 1
          \end{bmatrix}
        \right)^{-1}
        \begin{bmatrix}
          1 & \cdots & 1
        \end{bmatrix}
        \begin{bmatrix}
          Y_1 \\ \vdots \\ Y_n
        \end{bmatrix}\\
        &= n^{-1} \sum_{t=1}^n Y_t \\
        &= \bar{Y}.
      \end{align*}
    \item Il s'agit du modèle de régression linéaire simple passant
      par l'origine, pour lequel la matrice de schéma est
      \begin{displaymath}
        \mat{X} =
        \begin{bmatrix}
          X_1 \\ \vdots \\ X_n
        \end{bmatrix}_{n \times 1}.
      \end{displaymath}
      Par conséquent,
      \begin{align*}
        \hat\betavec
        &=
        \left(
          \begin{bmatrix}
            X_1 & \cdots & X_n
          \end{bmatrix}
          \begin{bmatrix}
            X_1 \\ \vdots \\ X_n
          \end{bmatrix}
        \right)^{-1}
        \begin{bmatrix}
          X_1 & \cdots & X_n
        \end{bmatrix}
        \begin{bmatrix}
          Y_1 \\ \vdots \\ Y_n
        \end{bmatrix} \\
        &=
        \left(
          \sum_{t=1}^n X_t^2
        \right)^{-1} \sum_{t=1}^n X_tY_t \\
        &= \frac{\sum_{t=1}^n X_t Y_t}{\sum_{t=1}^n X_t^2},
      \end{align*}
      tel qu'obtenu à l'exercice
      \ref{chap:simple}.\ref{ex:simple:origine}.
    \item On est ici en présence d'un modèle de régression multiple ne
      passant pas par l'origine et ayant deux variables explicatives.
      La matrice de schéma est alors
      \begin{displaymath}
        \mat{X} =
        \begin{bmatrix}
          1      & X_{11}  & X_{12} \\
          \vdots & \vdots & \vdots \\
          1      & X_{n1}  & X_{n2}
        \end{bmatrix}_{n \times 3}.
      \end{displaymath}
      Par conséquent,
      \begin{align*}
        \hat\betavec
        &=
        \left(
          \begin{bmatrix}
            1     & \cdots & 1      \\
            X_{11} & \cdots & X_{n1} \\
            X_{12} & \cdots & X_{n2}
          \end{bmatrix}
          \begin{bmatrix}
            1      & X_{11}  & X_{12} \\
            \vdots & \vdots & \vdots \\
            1      & X_{n1}  & X_{n2}
          \end{bmatrix}
        \right)^{-1}
        \begin{bmatrix}
          1     & \cdots & 1      \\
          X_{11} & \cdots & X_{n1} \\
          X_{12} & \cdots & X_{n2}
        \end{bmatrix}
        \begin{bmatrix}
          Y_1 \\ \vdots \\ Y_n
        \end{bmatrix} \\
        & =
        \begin{bmatrix}
          n           & n \bar{X}_1             & n \bar{X}_2 \\
          n \bar{X}_1 & \sum_{t=1}^n X_{t1}^2    & \sum_{t=1}^n X_{t1}X_{t2} \\
          n \bar{X}_2 & \sum_{t=1}^n X_{t1}X_{t2} & \sum_{t=1}^n X_{t2}^2
        \end{bmatrix}^{-1}
        \begin{bmatrix}
          \sum_{t=1}^n Y_t \\ \sum_{t=1}^n X_{t1}Y_t \\ \sum_{t=1}^n
          X_{t2}Y_t
        \end{bmatrix}.
      \end{align*}
      L'inversion de la première matrice et le produit par la seconde
      sont laissés aux bons soins du lecteur plus patient que les
      rédacteurs de ces solutions.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Vérifier, pour le modèle de régression linéaire simple, que les
  valeurs trouvées dans la matrice de variance-covariance
  $\var{\hat\betavec} = \sigma^2 (\mat{X}^\prime \mat{X})^{-1}$
  correspondent à celles calculées au chapitre \ref{chap:simple}.
  \begin{sol}
    Dans le modèle de régression linéaire simple, la matrice schéma est
    \begin{displaymath}
      \mat{X} =
      \begin{bmatrix}
        1      & X_1    \\
        \vdots & \vdots \\
        1      & X_n
      \end{bmatrix}.
    \end{displaymath}
    Par conséquent,
    \begin{align*}
      \var{\hat\betavec}
      &= \sigma^2 (\mat{X}^\prime \mat{X})^{-1} \\
      &= \sigma^2
      \left(
        \begin{bmatrix}
          1   & \cdots & 1  \\
          X_1 & \cdots & X_n
        \end{bmatrix}
        \begin{bmatrix}
          1      & X_1    \\
          \vdots & \vdots \\
          1      & X_n
        \end{bmatrix}
      \right)^{-1} \\
      &= \sigma^2
      \begin{bmatrix}
        n         & n \bar{X}        \\
        n \bar{X} & \sum_{t=1}^n X_t^2
      \end{bmatrix}^{-1}\\
      &= \frac{\sigma^2}{n \sum_{t=1}^n X_t^2 - n^2 \bar{X}^2}
      \begin{bmatrix}
        \sum_{t=1}^n X_t^2 & -n \bar{X} \\
        -n \bar{X}        & n
      \end{bmatrix} \\
      &= \frac{\sigma^2}{\sum_{t=1}^n (X_t - \bar{X}^2)}
      \begin{bmatrix}
        n^{-1} \sum_{t=1}^n X_t^2 & -\bar{X} \\
        -\bar{X}                 & 1
      \end{bmatrix},
    \end{align*}
    d'où
    \begin{align*}
      \var{\hat{\beta}_0}
      &= \sigma^2\, \frac{\sum_{t=1}^n X_t^2}{%
        n \sum_{t=1}^n (X_t - \bar{X})^2} \\
      &= \sigma^2\, \frac{\sum_{t=1}^n (X_t - \bar{X}^2) + n \bar{X}^2}{%
        n \sum_{t=1}^n (X_t - \bar{X})^2} \\
      \intertext{et}
      \var{\hat{\beta}_1}
      &= \frac{\sigma^2}{\sum_{t=1}^n (X_t - \bar{X})^2}.
    \end{align*}
    Ceci correspond aux résultats antérieurs.
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer les relations ci-dessous dans le contexte de la régression
  linéaire multiple et trouver leur équivalent en régression linéaire
  simple. Utiliser $\mat{e} = \mat{y - \hat{y}}$.
  \begin{enumerate}
  \item $\mat{X}^\prime \mat{e} = \mat{0}$
  \item $\mat{\hat{y}}^\prime \mat{e} = 0$
  \item $\mat{\hat{y}}^\prime \mat{\hat{y}} = \hat\betavec^\prime
    \mat{X}^\prime \mat{y}$
  \end{enumerate}
  \begin{sol}
    Dans les démonstrations qui suivent, trois relations de base
    seront utilisées:
    $\mat{e} = \mat{y - \hat{y}}$,
    $\mat{\hat{y}} = \mat{X} \hat\betavec$ et
    $\hat\betavec = (\mat{X}^\prime \mat{X})^{-1} \mat{X}^\prime \mat{y}$.
    \begin{enumerate}
    \item On a
      \begin{align*}
        \mat{X}^\prime \mat{e}
        &= \mat{X}^\prime (\mat{y - \hat{y}}) \\
        &= \mat{X}^\prime (\mat{y} - \mat{X} \hat\betavec) \\
        &= \mat{X}^\prime \mat{y} - (\mat{X}^\prime \mat{X}) \hat\betavec \\
        &= \mat{X}^\prime \mat{y} - (\mat{X}^\prime \mat{X})
        (\mat{X}^\prime \mat{X})^{-1} \mat{X}^\prime \mat{y} \\
        &= \mat{X}^\prime \mat{y} - \mat{X}^\prime \mat{y} \\
        &= \mat{0}.
      \end{align*}
      En régression linéaire simple, cela donne
      \begin{align*}
        \mat{X}^\prime \mat{e} &=
        \begin{bmatrix}
          1   & \cdots & 1  \\
          X_1 & \cdots & X_n
        \end{bmatrix}
        \begin{bmatrix}
          e_1 \\ \vdots \\ e_n
        \end{bmatrix} \\
        &=
        \begin{bmatrix}
          \sum_{t=1}^n e_t \\
          \sum_{t=1}^n X_t e_t
        \end{bmatrix}.
      \end{align*}
      Par conséquent, $\mat{X}^\prime \mat{e} = \mat{0}$ se simplifie
      en $\sum_{t=1}^n e_t = 0$ et $\sum_{t=1}^n X_t e_t = 0$ soit,
      respectivement, la condition pour que l'estimateur des moindres
      carrés soit sans biais et la seconde équation normale obtenue à
      la partie \ref{ex:simple:base:eq_normales}) de l'exercice
      \ref{chap:simple}.\ref{ex:simple:base}.
     \item On a
       \begin{align*}
         \hat{\mat{y}}^\prime \mat{e}
         &= (\mat{X}\hat\betavec)^\prime (\mat{y - \hat{y}}) \\
         &= \hat\betavec^\prime \mat{X}^\prime (\mat{y} - \mat{X}\hat\betavec) \\
         &= \hat\betavec^\prime \mat{X}^\prime \mat{y} - \hat\betavec^\prime
         (\mat{X}^\prime \mat{X}) \hat\betavec \\
         &= \hat\betavec^\prime \mat{X}^\prime \mat{y} - \hat\betavec^\prime
         (\mat{X}^\prime \mat{X}) (\mat{X}^\prime \mat{X})^{-1}
         \mat{X}^\prime \mat{y} \\
         &= \hat\betavec^\prime \mat{X}^\prime \mat{y} - \hat\betavec^\prime
         \mat{X}^\prime \mat{y} \\
         &= 0.
       \end{align*}
       Pour tout modèle de régression cette équation peut aussi
       s'écrire sous la forme plus conventionnelle $\sum_{t=1}^n
       \hat{Y}_t e_t = 0$. Cela signifie que le produit scalaire entre
       le vecteur des prévisions et celui des erreurs doit être nul
       ou, autrement dit, que les vecteurs doivent être orthogonaux.
       C'est là une condition essentielle pour que l'erreur
       quadratique moyenne entre les vecteurs $\mat{y}$ et
       $\mat{\hat{y}}$ soit minimale. (Pour de plus amples détails sur
       l'interprétation géométrique du modèle de régression, consulter
       {\shorthandoff{:} \citet[chapitres 20 et
         21]{Draper:regression:1998}}.) 
         D'ailleurs, on constate que
       $\hat{\mat{y}}^\prime \mat{e} = \hat\betavec^\prime \mat{X}^\prime
       \mat{e}$ et donc, en supposant sans perte de généralité que
       $\hat\betavec \ne \mat{0}$, 
       que $\hat{\mat{y}}^\prime \mat{e} = 0$
       et $\mat{X}^\prime \mat{e} = \mat{0}$ sont des conditions en
       tous points équivalentes.
     \item On a
       \begin{align*}
         \mat{\hat{y}}^\prime \mat{\hat{y}}
         &= (\mat{X} \hat\betavec)^\prime \mat{X} \hat\betavec \\
         &= \hat\betavec^\prime (\mat{X}^\prime \mat{X}) \hat\betavec \\
         &= \hat\betavec^\prime (\mat{X}^\prime \mat{X}) (\mat{X}^\prime
         \mat{X})^{-1} \mat{X}^\prime \mat{y} \\
         &= \hat\betavec^\prime \mat{X}^\prime \mat{y}.
       \end{align*}
       Cette équation est l'équivalent matriciel de l'identité
       \begin{align*}
         \SSR
         &= \hat{\beta}_1^2 \sum_{t = 1}^n (X_t - \bar{X})^2 \\
         &= \frac{S_{XY}^2}{S_{XX}}
       \end{align*}
       utilisée à plusieurs reprises dans les solutions du chapitre
       \ref{chap:simple}.  En effet, en régression linéaire simple,
       $\mat{\hat{y}}^\prime \mat{\hat{y}} = \sum_{t = 1}^n
       \hat{Y}_t^2 = \sum_{t = 1}^n (\hat{Y} - \bar{Y})^2 + n
       \bar{Y}^2 = \SSR + n \bar{Y}^2$ et
       \begin{align*}
         \hat\betavec^\prime \mat{X}^\prime \mat{y}
         &= \hat{\beta}_0 n \bar{Y} + \hat{\beta}_1 \sum_{t = 1}^n X_t Y_t \\
         &= (\bar{Y} - \hat{\beta}_1 \bar{X}) n \bar{Y} +
         \hat{\beta}_1 \sum_{t = 1}^n X_t Y_t \\
         &= \hat{\beta}_1 \sum_{t = 1}^n (X_t - \bar{X})(Y_t -
         \bar{Y}) + n \bar{Y}^2 \\
         &= \frac{S_{XY}^2}{S_{XX}} + n \bar{Y}^2,
       \end{align*}
       d'où $\SSR = S_{XY}^2/S_{XX}$.
     \end{enumerate}
   \end{sol}
\end{exercice}

\begin{exercice}
  Considérer le modèle de régression linéaire multiple présenté à
  l'exercice \ref{chap:multiple}.\ref{ex:multiple:preuve}. Soit
  $\hat{Y}_0$ la prévision de la variable dépendante correspondant aux
  valeurs du vecteur ligne $\mat{x}_0 = (1, X_{01}, \dots, X_{0p})$
  des $p$ variables indépendantes. On a donc
  \begin{displaymath}
    \hat{Y}_0 = \mat{x}_0 \hat\betavec.
  \end{displaymath}
  \begin{enumerate}
  \item Démontrer que $\esp{\hat{Y}_0} = \esp{Y_0}$.
  \item Démontrer que l'erreur dans la prévision de la valeur moyenne
    de $Y_0$ est
    \begin{displaymath}
      \esp{(\hat{Y}_0 - \esp{Y_0})^2} =
      \sigma^2\, \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime.
    \end{displaymath}
    Construire un intervalle de confiance de niveau $1 - \alpha$ pour
    $\esp{Y_0}$.
  \item Démontrer que l'erreur dans la prévision de $Y_0$ est
    \begin{displaymath}
      \esp{(Y_0 - \hat{Y}_0)^2} =
      \sigma^2\, (1 + \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime).
    \end{displaymath}
    Construire un intervalle de confiance de niveau $1 - \alpha$ pour
    $Y_0$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Premièrement, $Y_0 = \mat{x}_0 \betavec + \varepsilon_0$ avec
      $\esp{\varepsilon_0} = 0$. Par conséquent, $\esp{Y_0} = \esp{x_0
        \betavec + \varepsilon_0} = \mat{x}_0 \betavec$. Deuxièmement,
      $\esp{\hat{Y}_0} = \esp{\mat{x}_0 \hat\betavec} = \mat{x}_0
      \esp{\hat\betavec} = x_0 \betavec$ puisque l'estimateur des moindres
      carrés de $\betavec$ est sans biais. Ceci complète la preuve.
    \item Tout d'abord, $\esp{(\hat{Y}_0 - \esp{Y_0})^2} =
      \varmat{\hat{Y}_0} = \var{\hat{Y}_0}$ puisque la matrice de
      variance-covariance du vecteur aléatoire $\hat{Y}_0$ ne
      contient, ici, qu'une seule valeur. Or, par le théorème
      \ref{thm:elements:esp_var},
       \begin{align*}
         \var{\hat{Y}_0}
         &= \varmat{\mat{x}_0 \hat\betavec} \\
         &= \mat{x}_0 \varmat{\hat\betavec} \mat{x}_0^\prime \\
         &= \sigma^2 \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1}
         \mat{x}_0^\prime.
       \end{align*}
       Afin de construire un intervalle de confiance pour $\esp{Y_0}$,
       on ajoute au modèle l'hypothèse $\epsvec \sim N(\mat{0}, \sigma^2
       \mat{I})$. Par linéarité de l'estimateur des moindres carrés,
       on a alors $\hat{Y}_0 \sim N(\esp{Y_0}, \var{\hat{Y}_0})$. Par
       conséquent,
       \begin{displaymath}
         \Pr
         \left[
           -z_{\alpha/2}
           \leq
           \frac{\hat{Y} - \esp{\hat{Y}_0}}{\sqrt{\var{\hat{Y}_0}}}
           \leq
           z_{\alpha/2}
         \right] = 1 - \alpha
       \end{displaymath}
       d'où un intervalle de confiance de niveau $1 - \alpha$ pour
       $\esp{Y_0}$ est
       \begin{displaymath}
         \esp{Y_0}
         \in \hat{Y}_0 \pm z_{\alpha/2}\, \sigma\,
         \sqrt{\mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime}.
       \end{displaymath}
       Si la variance $\sigma^2$ est inconnue et estimée par $s^2$,
       alors la distribution normale est remplacée par une
       distribution de Student avec $n - p - 1$ degrés de
       liberté. L'intervalle de confiance devient alors
       \begin{displaymath}
         \esp{Y_0}
         \in \hat{Y}_0 \pm t_{\alpha/2}(n - p - 1)\, s\,
         \sqrt{\mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime}.
       \end{displaymath}
     \item Par le résultat obtenu en a) et en supposant que
       $\Cov(\varepsilon_0, \varepsilon_t) = 0$ pour tout $t = 1,
       \dots, n$, on a
       \begin{align*}
         \esp{(Y_0 - \hat{Y}_0)^2}
         &= \var{Y_0 - \hat{Y}_0} \\
         &= \var{Y_0} + \var{\hat{Y}_0} \\
         &= \sigma^2 (1 + \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1}
         \mat{x}_0^\prime).
       \end{align*}
       Ainsi, avec l'hypothèse sur le terme d'erreur énoncée en b),
       $Y_0 - \hat{Y}_0 \sim N(0, \var{Y_0 - \hat{Y}_0})$. En suivant
       le même cheminement qu'en b), on détermine qu'un intervalle de
       confiance de niveau $1 - \alpha$ pour $Y_0$ est
       \begin{displaymath}
         Y_0
         \in \hat{Y}_0 \pm z_{\alpha/2}\, \sigma\,
         \sqrt{1 + \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime}.
       \end{displaymath}
       ou, si la variance $\sigma^2$ est inconnue et estimée par
       $s^2$,
       \begin{displaymath}
         Y_0
         \in \hat{Y}_0 \pm t_{\alpha/2}(n - p - 1)\, s\,
         \sqrt{1 + \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime}.
       \end{displaymath}
     \end{enumerate}
   \end{sol}
\end{exercice}

\begin{exercice}
  En ajustant le modèle
  \begin{displaymath}
    Y_t = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} + \beta_3 X_{t3} +
    \varepsilon_t
  \end{displaymath}
  à un ensemble de données, on a obtenu les statistiques suivantes:
  \begin{align*}
    R^2 &= 0,521 \\
    F   &= 5,438.
  \end{align*}
  Déterminer la valeur $p$ approximative du test global de validité du
  modèle.
  \begin{rep}
    $p \approx 0,01$
  \end{rep}
  \begin{sol}
    On a la relation suivante liant la statistique $F$ et le
    coefficient de détermination $R^2$:
    \begin{displaymath}
      F = \frac{R^2}{1 - R^2}\, \frac{n - p - 1}{p}
    \end{displaymath}
    La principale inconnue dans le problème est $n$, le nombre de
    données. Or,
    \begin{align*}
      n
      &= p F \left( \frac{1 - R^2}{R^2} \right) + p + 1 \\
      &= 3 (5,438) \left( \frac{1 - 0,521}{0,521} \right) + 3 + 1 \\
      &= 19.
    \end{align*}
    Soit $F$ une variable aléatoire dont la distribution est une loi
    de Fisher avec $3$ et $19 - 3 - 1 = 15$ degrés de liberté, soit la
    même distribution que la statistique $F$ du modèle. On obtient la
    valeur $p$ du test global de validité du modèle dans un tableau de
    quantiles de la distribution $F$ ou avec la fonction \texttt{pf}
    dans R:
    \begin{align*}
      \Pr[F > 5,438] = 0,0099
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On vous donne les observations suivantes:
  \begin{center}
    \begin{tabular}{rrr}
      \toprule
      $Y$ & $X_1$ & $X_2$ \\
      \midrule
      17 & 4 &  9 \\
      12 & 3 & 10 \\
      14 & 3 & 11 \\
      13 & 3 & 11 \\
      \bottomrule
    \end{tabular}
  \end{center}
  De plus, si $\mat{X}$ est la matrice de schéma du modèle
  \begin{displaymath}
    Y_t = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} + \varepsilon_t,
    \quad t = 1, 2, 3, 4,
  \end{displaymath}
  où $\varepsilon_t \sim N(0, \sigma^2)$, alors
  \begin{align*}
    (\mat{X}^\prime \mat{X})^{-1}
    &= \frac{1}{2}
    \left[
      \begin{array}{rrr}
        765 & -87 &  -47 \\
        -87 &  11 &    5 \\
        -47 &   5 &    3
      \end{array}
    \right]
    \intertext{et}
    (\mat{X}^\prime \mat{X})^{-1} \mat{X}^\prime
    &= \frac{1}{2}
    \left[
      \begin{array}{rrrr}
        -6 & 34 & -13 & -13 \\
        2 & -4 &   1 &   1 \\
        0 & -2 &   1 &   1
      \end{array}
    \right]
  \end{align*}
  \begin{enumerate}
  \item Trouver, par la méthode des moindres carrés, les estimateurs
    des paramètres du modèle mentionné ci-dessus.
  \item Construire le tableau d'analyse de variance du modèle obtenu
    en a) et calculer le coefficient de détermination.
  \item Vérifier si les variables $X_1$ et $X_2$ sont significatives
    dans le modèle.
  \item Trouver un intervalle de confiance à 95~\% pour la valeur de
    $Y$ lorsque $X_1 = 3,5$ et $X_2 = 9$.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat\betavec = (-22,5,\; 6,5,\; 1,5)$
    \item $F = 13,5$, $R^2 = 0,9643$
    \item $t_1 = 3,920$, $t_2 = 1,732$
    \item $13,75 \pm 13,846$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a
      \begin{align*}
        \hat\betavec
        &= (\mat{X}^\prime \mat{X})^{-1} \mat{X}^\prime \mat{y} \\
        &= \frac{1}{2}
        \left[
          \begin{array}{rrrr}
            -6 & 34 & -13 & -13 \\
            2 & -4 &   1 &   1 \\
            0 & -2 & 1 & 1
          \end{array}
        \right]
        \begin{bmatrix}
          17 \\ 12 \\ 14 \\ 13
        \end{bmatrix} \\
        &= \frac{1}{2}
        \left[
          \begin{array}{r}
            -45 \\ 13 \\ 3
          \end{array}
        \right] =
        \left[
          \begin{array}{r}
            -22,5 \\ 6,5 \\ 1,5
          \end{array}
        \right]
      \end{align*}
    \item Avec les résultats de la partie a), on a
      \begin{align*}
        \mat{\hat{y}} &= \mat{X} \hat\betavec =
        \begin{bmatrix}
          17 \\ 12 \\ 13,5 \\ 13,5
        \end{bmatrix}, \\
        \mat{e} &= \mat{y} - \mat{\hat{y}} =
        \left[
          \begin{array}{r}
            0 \\ 0 \\ 0,5 \\ -0,5
          \end{array}
        \right]
      \end{align*}
      et $\bar{Y} = 14$. Par conséquent,
      \begin{align*}
        \SST
        &= \mat{y}^\prime \mat{y} - n \bar{Y}^2 = 14 \\
        \SSE
        &= \mat{e}^\prime \mat{e} = 0,5 \\
        \SSR &= \SST - \SSR = 13,5,
      \end{align*}
      d'où le tableau d'analyse de variance est le suivant:
      \begin{center}
        \begin{tabular}{lrrrr}
          \toprule
          Source & SS & d.l. & MS & $F$ \\
          \midrule
          Régression & $13,5$ & 2 & $6,75$ &  $13,5$ \\
          Erreur     &  $0,5$ & 1 &  $0,5$ \\
          \midrule
          Total      &   $14$ & \\
          \bottomrule
        \end{tabular}
      \end{center}
      Le coefficient de détermination est
      \begin{displaymath}
        R^2 = 1 - \frac{\SSE}{\SST} = 0,9643.
      \end{displaymath}
    \item On sait que $\var{\hat{\beta}_i} = \sigma^2 c_{ii}$, où
      $c_{ii}$ est l'élément en position $(i+1, i+1)$ de la matrice
      $(\mat{X}^\prime \mat{X})^{-1}$. Or, $\hat{\sigma}^2 = s^2 =
      \text{MSE} = 0,5$, tel que calculé en b). Par conséquent, la
      statistique $t$ du test $H_0: \beta_1 = 0$ est
      \begin{displaymath}
        t
        = \frac{\hat{\beta}_1}{s \sqrt{c_{11}}}
        = \frac{6,5}{\sqrt{0,5 (\frac{11}{2})}}
        = 3,920,
      \end{displaymath}
      alors que celle du test $H_0: \beta_2 = 0$ est
      \begin{displaymath}
        t
        = \frac{\hat{\beta}_2}{s \sqrt{c_{22}}}
        = \frac{1,5}{\sqrt{0,5 (\frac{3}{2})}}
        = 1,732.
      \end{displaymath}
      À un niveau de signification de 5~\%, la valeur critique de ces
      tests est $t_{0,025}(1) = 12,706$. Dans les deux cas, on ne
      rejette donc pas $H_0$, les variables $X_1$ et $X_2$ ne sont pas
      significatives dans le modèle.
    \item Soit $\mat{x}_0 = \begin{bmatrix} 1 & 3,5 & 9 \end{bmatrix}$
      et $Y_0$ la valeur de la variable dépendante correspondant à
      $\mat{x}_0$. La prévision de $Y_0$ donnée par le modèle trouvé
      en a) est
      \begin{align*}
        \hat{Y}_0
        &= \mat{x}_0 \hat\betavec \\
        &= -22,5 + 6,5(3,5) + 1,5(9) \\
        &= 13,75.
      \end{align*}
      D'autre part,
      \begin{align*}
        \widehat{\text{Var}}[Y_0 - \hat{Y}_0]
        &= s^2 (1 + \mat{x}_0 (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime) \\
        &= 1,1875.
      \end{align*}
      Par conséquent, un intervalle de confiance à 95~\% pour $Y_0$
      est
      \begin{align*}
        \esp{Y_0} &\in \hat{Y}_0 \pm t_{0,025}(1) s \sqrt{1 + \mat{x}_0
          (\mat{X}^\prime \mat{X})^{-1} \mat{x}_0^\prime} \\
        &\in 13,75 \pm 12,706 \sqrt{1,1875} \\
        &\in (-0,096, 27,596).
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Répéter l'exercice \ref{chap:simple}.\ref{ex:simple:carburant} en
  ajoutant la cylindrée du véhicule en litres dans le modèle. La
  cylindrée est exprimée en pouces cubes dans les données.  Or, 1
  pouce correspond à 2,54~cm et un litre est définit comme étant
  1~$\text{dm}^3$, soit \nombre{1000}~$\text{cm}^3$. Trouver un
  intervalle de confiance pour la consommation en carburant d'une
  voiture de \nombre{1350}~kg ayant un moteur de 1,8 litre.

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $R^2 = 0,8927$ et
      $F = 145,6$
    \item $12,04 \pm
      2,08$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On importe les données dans \textsf{R}, puis on effectue les
      conversions nécessaires. Comme précédemment, la variable
      \texttt{consommation} contient la consommation des voitures en
      $\ell$/100~km et la variable \texttt{poids} le poids en
      kilogrammes. On ajoute la variable \texttt{cylindree}, qui
      contient la cylindrée des voitures en litres.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{carburant} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"carburant.dat"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{consommation} \hlkwb{<-} \hlnum{235.1954}\hlopt{/}\hlstd{carburant}\hlopt{$}\hlstd{mpg}
\hlstd{poids} \hlkwb{<-} \hlstd{carburant}\hlopt{$}\hlstd{poids} \hlopt{*} \hlnum{0.45455} \hlopt{*} \hlnum{1000}
\hlstd{cylindree} \hlkwb{<-} \hlstd{carburant}\hlopt{$}\hlstd{cylindree} \hlopt{*} \hlnum{2.54}\hlopt{^}\hlnum{3}\hlopt{/}\hlnum{1000}
\end{alltt}
\end{kframe}
\end{knitrout}
    \item La fonction \texttt{summary} fournit l'information
      essentielle pour juger de la validité et de la qualité du
      modèle:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(consommation} \hlopt{~} \hlstd{poids} \hlopt{+} \hlstd{cylindree)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = consommation ~ poids + cylindree)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8799 -0.5595  0.1577  0.6051  1.7900 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -3.049304   1.098281  -2.776  0.00877 ** 
## poids        0.012677   0.001512   8.386 6.85e-10 ***
## cylindree   -1.122696   0.333479  -3.367  0.00186 ** 
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9156 on 35 degrees of freedom
## Multiple R-squared:  0.8927,	Adjusted R-squared:  0.8866 
## F-statistic: 145.6 on 2 and 35 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}
      Le modèle est donc le suivant:
      \begin{displaymath}
        Y_t =
        -3,049 +
        0,01268 X_{t1} +
        -1,123 X_{t2} +
        \varepsilon_t,
        \quad
        \epsvec_t \sim N(0,
        0,9156^2 \mat{I})
      \end{displaymath}
      où $Y_t$ est la consommation en litres aux 100 kilomètres,
      $X_{t1}$ le poids en kilogrammes et $X_{t2}$ la cylindrée en
      litres. La faible valeur $p$ du test $F$ indique une régression
      globalement très significative. Les tests $t$ des paramètres
      individuels indiquent également que les deux variables du modèle
      sont significatives. Enfin, le $R^2$ de %
      0,8927 %$
      confirme que l'ajustement du modèle est toujours bon.
    \item On veut calculer un intervalle de confiance pour la
      consommation prévue d'une voiture de \nombre{1350}~kg ayant un
      moteur d'une cylindrée de 1,8 litres. On obtient, avec la
      fonction \texttt{predict}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(fit,} \hlkwc{newdata} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{poids} \hlstd{=} \hlnum{1350}\hlstd{,} \hlkwc{cylindree} \hlstd{=} \hlnum{1.8}\hlstd{),}
        \hlkwc{interval} \hlstd{=} \hlstr{"prediction"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##        fit      lwr      upr
## 1 12.04325 9.959855 14.12665
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans un exemple du chapitre \ref{chap:simple} des notes de cours,
  nous avons tâché d'expliquer les sinistres annuels moyens par
  véhicule pour différents types de véhicules uniquement par la
  puissance du moteur (en chevaux-vapeur).  Notre conclusion était à
  l'effet que la régression était significative --- rejet de $H_0$
  dans les tests \emph{t} et \emph{F} --- mais l'ajustement mauvais
  --- $R^2$ petit.

  Examiner les autres variables fournies dans le fichier
  \texttt{auto-price.dat} et choisir deux autres caractéristiques
  susceptibles d'expliquer les niveaux de sinistres. Par exemple,
  peut-on distinguer une voiture sport d'une minifourgonnette?

  Une fois les variables additionnelles choisies, calculer les
  différentes statistiques propres à une régression en ajoutant
  d'abord une, puis deux variables au modèle de base.  Quelles sont
  vos conclusions?
  \begin{sol}
    Il y a plusieurs réponses possibles pour cet exercice. Si l'on
    cherche, tel que suggéré dans l'énoncé, à distinguer les voitures
    sport des minifourgonnettes (en supposant que ces dernières ont
    moins d'accidents que les premières), alors on pourrait
    s'intéresser, en premier lieu, à la variable \texttt{peak.rpm}. Il
    s'agit du régime moteur maximal, qui est en général beaucoup plus
    élevé sur les voitures sport. Puisque l'on souhaite expliquer le
    montant total des sinistres de différents types de voitures, il
    devient assez naturel de sélectionner également la variable
    \texttt{price}, soit le prix du véhicule. Un véhicule plus luxueux
    coûte en général plus cher à faire réparer à dommages égaux.
    Voyons l'effet de l'ajout, pas à pas, de ces deux variables au
    modèle précédent ne comportant que la variable
    \texttt{horsepower}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{autoprice} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"data/auto-price.dat"}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{fit1} \hlkwb{<-} \hlkwd{lm}\hlstd{(losses} \hlopt{~} \hlstd{horsepower} \hlopt{+} \hlstd{peak.rpm,} \hlkwc{data} \hlstd{= autoprice)}
\hlkwd{summary}\hlstd{(fit1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = losses ~ horsepower + peak.rpm, data = autoprice)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -67.973 -24.074  -6.373  18.049 130.301 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  5.521414  29.967570   0.184 0.854060    
## horsepower   0.318477   0.086840   3.667 0.000336 ***
## peak.rpm     0.016639   0.005727   2.905 0.004205 ** 
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 33.44 on 156 degrees of freedom
## Multiple R-squared:  0.1314,	Adjusted R-squared:  0.1203 
## F-statistic:  11.8 on 2 and 156 DF,  p-value: 1.692e-05
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(fit1)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Response: losses
##             Df Sum Sq Mean Sq F value    Pr(>F)    
## horsepower   1  16949 16948.5 15.1573 0.0001463 ***
## peak.rpm     1   9437  9437.0  8.4397 0.0042049 ** 
## Residuals  156 174435  1118.2                      
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}
    La variable \texttt{peak.rpm} est significative, mais le $R^2$
    demeure faible. Ajoutons maintenant la variable \texttt{price} au modèle:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit2} \hlkwb{<-} \hlkwd{lm}\hlstd{(losses} \hlopt{~} \hlstd{horsepower} \hlopt{+} \hlstd{peak.rpm} \hlopt{+} \hlstd{price,} \hlkwc{data} \hlstd{= autoprice)}
\hlkwd{summary}\hlstd{(fit2)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = losses ~ horsepower + peak.rpm + price, data = autoprice)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -66.745 -25.214  -5.867  18.407 130.032 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)   
## (Intercept) -0.6972172 31.3221462  -0.022  0.98227   
## horsepower   0.2414922  0.1408272   1.715  0.08838 . 
## peak.rpm     0.0181386  0.0061292   2.959  0.00357 **
## price        0.0005179  0.0007451   0.695  0.48803   
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 33.49 on 155 degrees of freedom
## Multiple R-squared:  0.1341,	Adjusted R-squared:  0.1173 
## F-statistic: 8.001 on 3 and 155 DF,  p-value: 5.42e-05
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(fit2)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Response: losses
##             Df Sum Sq Mean Sq F value    Pr(>F)    
## horsepower   1  16949 16948.5 15.1071 0.0001502 ***
## peak.rpm     1   9437  9437.0  8.4118 0.0042702 ** 
## price        1    542   542.1  0.4832 0.4880298    
## Residuals  155 173893  1121.9                      
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}
    Du moins avec les variables \texttt{horsepower} et
    \texttt{peak.rpm}, la variable \texttt{price} n'est pas
    significative. D'ailleurs, l'augmentation du $R^2$ suite à l'ajout
    de cette variable est minime. À ce stade de l'analyse, il vaudrait
    sans doute mieux reprendre tout depuis le début avec d'autres
    variables. Des méthodes de sélection des variables seront étudiées
    plus avant dans le chapitre.
  \end{sol}
\end{exercice}

\begin{exercice}
  %%% Exercice 6.10 de Miller & Wichern. On n'a pas les données. Les
  %%% statistiques des résidus sont exactes, cependant.
  En bon étudiant(e), vous vous intéressez à la relation liant la
  demande pour la bière, $Y$, aux variables indépendantes $X_1$ (le
  prix de celle-ci), $X_2$ (le revenu disponible) et $X_3$ (la demande
  de l'année précédente). Un total de 20 observations sont
  disponibles.  Vous postulez le modèle
  \begin{displaymath}
    Y_t = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} + \beta_3 X_{t3} +
    \varepsilon_t,
  \end{displaymath}
  où $\esp{\varepsilon_t} = 0$ et $\Cov(\varepsilon_t, \varepsilon_s)
  = \delta_{ts} \sigma^2$. Les résultats de cette régression, tels que
  calculés dans \textsf{R}, sont fournis ci-dessous.

\begin{verbatim}
> fit <- lm(Y ~ X1 + X2 + X3, data = biere)
> summary(fit)
\end{verbatim}
\begin{verbatim}
Call: lm(formula = Y ~ X1 + X2 + X3, data = biere)
Residuals:
      Min.    1st Qu.     Median    3rd Qu.       Max.
-1.014e+04 -5.193e-03 -2.595e-03  4.367e-03  2.311e-02

Coefficients:
              Value Std. Error t value  Pr(>|t|)
(Intercept)  1.5943  1.0138     1.5726    0.1354
         X1 -0.0480  0.1479    -0.3243    0.7499
         X2  0.0549  0.0306     1.7950    0.0916
         X3  0.8130  0.1160     7.0121 2.933e-06

Residual standard error: 0.0098 on 16 degrees of freedom
Multiple R-Squared: 0.9810      Adjusted R-squared: 0.9774
F-statistic: 275.49 on 3 and 16 degrees of freedom,
the p-value is 7.160e-14
\end{verbatim}

  \begin{enumerate}
  \item Indiquer les dimensions des matrices et vecteurs dans la
    représentation matricielle $\mat{y} = \mat{X} \betavec + \epsvec$ du
    modèle.
  \item La régression est-elle significative? Expliquer.
  \item On porte une attention plus particulière au paramètre
    $\beta_2$. Est-il significativement différent de zéro? Quelle est
    l'interprétation du test $H_0: \beta_2 = 0$ versus $H_1: \beta_2
    \ne 0$?
  \item Quelle est la valeur et l'interprétation de $R^2$, le
    coefficient de détermination? De manière générale, est-il
    envisageable d'obtenir un $R^2$ élevé et, simultanément, toutes
    les statistiques $t$ pour les tests $H_0: \beta_1 = 0$, $H_0:
    \beta_2 = 0$ et $H_0: \beta_3 = 0$ non significatives?  Expliquer
    brièvement.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\mat{y}_{20 \times 1}$, $\mat{X}_{20 \times 4}$, $\betavec_{4
        \times 1}$ et $\epsvec_{20 \times 1}$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $p = 3$ variables explicatives et, du nombre de degrés
      de liberté de la statistique $F$, on apprend que $n - p - 1 =
      16$. Par conséquent, $n = 16 + 3 + 1 = 20$. Les dimensions des
      vecteurs et de la matrice de schéma dans la représentation
      $\mat{y} = \mat{X} \betavec + \epsvec$ sont donc: $n \times 1 = 20
      \times 1$ pour les vecteurs $\mat{y}$ et $\epsvec$, $n \times (p
      + 1) = 20 \times 4$ pour la matrice $\mat{X}$, $(p + 1) \times
      1$ pour le vecteur $\betavec$.
    \item La valeur $p$ associée à la statistique $F$ est, à toute fin
      pratique, nulle. Cela permet de rejeter facilement l'hypothèse
      nulle selon laquelle la régression n'est pas significative.
    \item On doit se fier ici au résultat du test $t$ associé à la
      variable $X_2$. Dans les résultats obtenus avec \textsf{R}, on
      voit que la valeur $p$ de la statistique $t$ du paramètre
      $\beta_2$ est $0,0916$. Cela signifie que jusqu'à un seuil de
      signification de 9,16~\% (ou un niveau de confiance supérieur à
      90,84~\%), on ne peut rejeter l'hypothèse $H_0: \beta_2 = 0$ en
      faveur de $H_1: \beta_2 \ne 0$. Il s'agit néanmoins d'un cas
      limite et il est alors du ressort de l'analyste de décider
      d'inclure ou non le revenu disponible dans le modèle.
    \item Le coefficient de détermination est de $R^2 = 0,981$. Cela
      signifie que le prix de la bière, le revenu disponible et la
      demande de l'année précédente expliquent plus de 98~\% de la
      variation de la demande en bière. L'ajustement du modèle aux
      données est donc particulièrement bon. Il est tout à fait
      possible d'obtenir un $R^2$ élevé et, simultanément, toutes les
      statistiques $t$ non significatives: comme chaque test $t$
      mesure l'impact d'une variable sur la régression étant donné la
      présence des autres variables, il suffit d'avoir une bonne
      variable dans un modèle pour obtenir un $R^2$ élevé et une ou
      plusieurs autres variables redondantes avec la première pour
      rendre les tests $t$ non significatifs.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  %%% Je ne me souviens plus comment j'ai obtenu ces résultats. Fort
  %%% probablement simulés.
  Au cours d'une analyse de régression, on a colligé les valeurs
  de trois variables explicatives $X_1$, $X_2$ et $X_3$ ainsi que
  celles d'une variable dépendante $Y$. Les résultats suivants ont par
  la suite été obtenus avec \textsf{R}.
  
%  \begin{verbatim}
%> anova(lm(Y ~ X2 + X3, data = foo))
%  \end{verbatim}
%  \begin{verbatim}
%Analysis of Variance Table
%
%Response: Y
%
%          Df Sum of Sq  Mean Sq  F Value       Pr(>F)
%       X2  1  45.59085 45.59085 106.0095 0.0000000007 ***
%       X3  1   8.76355  8.76355  20.3773 0.0001718416 ***
%Residuals 22   9.46140  0.43006
%---
%Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
%  \end{verbatim}
%  \begin{verbatim}
%> anova(lm(Y ~ X1 + X2 + X3, data = foo))
%  \end{verbatim}
%  \begin{verbatim}
%Analysis of Variance Table
%
%Response: Y
%
%          Df Sum of Sq  Mean Sq  F Value    Pr(>F)
%       X1  1  45.59240 45.59240 101.6681 0.0000000 ***
%       X2  1   0.01842  0.01842   0.0411 0.8413279
%       X3  1   8.78766  8.78766  19.5959 0.0002342 ***
%Residuals 21   9.41731  0.44844
%---
%Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
%  \end{verbatim}
  
  \begin{enumerate}
  \item On considère le modèle complet $Y = \beta_0 + \beta_1 X_1 +
    \beta_2 X_2 + \beta_3 X_3 + \varepsilon$. À partir de
    l'information ci-dessus, calculer la statistique appropriée pour
    compléter chacun des tests suivants. Indiquer également le nombre
    de degrés de liberté de cette statistique. Dans tous les cas,
    l'hypothèse alternative $H_1$ est la négation de l'hypothèse
    $H_0$.
    \begin{enumerate}
      %\renewcommand{\labelenumiii}{\roman{enumiii})}
    \item $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
    \item $H_0: \beta_1 = 0$
    \item $H_0: \beta_2 = \beta_3 = 0$
    \end{enumerate}
  \item À la lumière des résultats en a), quelle(s) variable(s)
    devrait-on inclure dans la régression? Justifier votre réponse.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item
      \begin{inparaenum}[i)]
      \item 40,44, 3 et 21 degrés de liberté
      \item 0,098, 1 et 21 degrés de liberté
      \item 9,82, 2 et 21 degrés de liberté
      \end{inparaenum}
    \item $X_1$ et $X_3$, ou $X_2$ et $X_3$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item L'information demandée doit évidemment être extraite des
      deux tableaux d'analyse de variance fournis dans l'énoncé. Il
      importe, ici, de savoir que le résultat de la fonction
      \texttt{anova} de \textsf{R} est un tableau d'analyse de
      variance séquentiel, où chaque ligne identifiée par le nom d'une
      variable correspond au test $F$ partiel résultant de l'ajout de
      cette variable au modèle. Ainsi, du premier tableau on obtient
      les sommes de carrés
      \begin{align*}
        \SSR(X_2)          &= 45,59085 \\
        \SSR(X_3|X_2)      &= 8,76355 \\
        \intertext{alors que du second tableau on a}
        \SSR(X_1)          &= 45,59240 \\
        \SSR(X_2|X_1)      &= 0,01842 \\
        \SSR(X_3|X_1, X_2) &= 8,78766,
      \end{align*}
      ainsi que
      \begin{align*}
        \MSE
        &= \frac{SSE(X_1, X_2, X_3)}{n - p - 1} \\
        &= 0,44844.
      \end{align*}
      \begin{enumerate}[i)]
      \item Le test d'hypothèse $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
        est le test global de validité du modèle. La statistique $F$
        pour ce test est
        \begin{align*}
          F
          &= \frac{\SSR(X_1, X_2, X_3)/3}{\MSE} \\
          &= \frac{(\SSR(X_1) + \SSR(X_2|X_1) + \SSR(X_3|X_1,X_2))/3}{\MSE} \\
          &= \frac{(45,5924 + 0,01842 + 8,78766)/3}{0,44844} \\
          &= 40,44.
        \end{align*}
        Puisque la statistique $\MSE$ a 21 degrés de liberté, la
        statistique $F$ en a 3 et 21.
      \item Pour tester cette hypothèse, il faut utiliser un test $F$
        partiel. On teste si la variable $X_1$ est significative dans
        la régression globale. La statistique du test est alors
        \begin{align*}
          F^*
          &= \frac{\SSR(X_1|X_2,X_3)/1}{\MSE} \\
          &= \frac{\SSR(X_1, X_2, X_3) - \SSR(X_2, X_3)}{\MSE} \\
          &= \frac{\SSR(X_1, X_2, X_3) - \SSR(X_2) - \SSR(X_3|X_2)}{\MSE} \\
          &= \frac{54,39848 - 45,59085 - 8,76355}{0,44844} \\
          &= 0,098,
        \end{align*}
        avec 1 et 21 degrés de liberté.
      \item Cette fois, on teste si les variables $X_2$ et $X_3$ (les
        deux ensemble) sont significatives dans la régression globale.
        On effectue donc encore un test $F$ partiel avec la
        statistique
        \begin{align*}
          F^*
          &= \frac{\SSR(X_2, X_3|X_1)/2}{\MSE} \\
          &= \frac{(\SSR(X_1, X_2, X_3) - \SSR(X_1))/2}{\MSE} \\
          &= \frac{(54,39848 - 45,5924)/2}{0,44844} \\
          &= 9,819,
        \end{align*}
        avec 2 et 21 degrés de liberté.
      \end{enumerate}
    \item À la lecture du premier tableau d'analyse de variance que
      tant les variables $X_2$ que $X_3$ sont significatives dans le
      modèle. Par contre, comme on le voit dans le second tableau, la
      variable $X_2$ devient non significative dès lors que la
      variable $X_1$ est ajoutée au modèle. (L'impact de la variable
      $X_3$ demeure, lui, inchangé.) Cela signifie que les variables
      $X_1$ et $X_2$ sont redondantes et qu'il faut choisir l'une ou
      l'autre, mais pas les deux. Par conséquent, les choix de modèle
      possibles sont $X_1$ et $X_3$, ou $X_2$ et $X_3$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans une régression multiple avec quatre variables explicatives et
  506 données, on a obtenu:
  \begin{align*}
    \mathrm{SSR}(X_1|X_4) &= \nombre{21348} \\
    \mathrm{SSR}(X_4) &= \nombre{2668} \\
    R^2 &= 0,6903 \\
    s^2 &= 26,41.
  \end{align*}
  Calculer la statistique appropriée pour le test
  \begin{align*}
    H_0 &: \beta_2 = \beta_3 = 0 \\
    H_1 &: \beta_2 \ne 0 \text{ ou } \beta_3 \ne 0.
  \end{align*}
  \begin{rep}
    $103,67$
  \end{rep}
  \begin{sol}
    La statistique à utiliser pour faire ce test $F$ partiel est
    \begin{align*}
      F^*
      &= \frac{\SSR(X_2, X_3|X_1, X_4)/2}{\MSE} \\
      &= \frac{\SSR(X_1, X_2, X_3, X_4) - \SSR(X_1, X_4)}{2\, \MSE} \\
      &= \frac{\SSR - \SSR(X_4) - \SSR(X_1|X_4)}{2 s^2}
    \end{align*}
    où $\SSR = \SSR(X_1,X_2,X_3,X_4)$. Or,
    \begin{align*}
      R^2
      &= \frac{\SSR}{\SST} \\
      &= \frac{\SSR}{\SSR + \SSE}, \\
      \intertext{d'où}
      \SSR
      &= \frac{R^2}{1 - R^2}\, \SSE \\
      &= \frac{R^2}{1 - R^2}\, \MSE (n - p - 1) \\
      &= \frac{0,6903}{1 - 0,6903}\, (26,41) (506 - 4 - 1) \\
      &= \nombre{29492}.
    \end{align*}
    Par conséquent,
    \begin{align*}
      F^*
      &= \frac{\nombre{29492} - \nombre{2668} - \nombre{21348}}{%
        (2) (26,41)} \\
      &= 103,67.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  En régression linéaire multiple, on a $\hat{\betavec} \sim
  N(\betavec, \sigma^2 (\mat{X}^\prime \mat{X})^{-1})$ et
  $\text{SSE}/\sigma^2 \sim \chi^2(n - p - 1)$.
  \begin{enumerate}
  \item Vérifier que
    \begin{displaymath}
      \frac{\hat{\beta_i} - \beta_i}{s \sqrt{c_{ii}}} \sim
      t(n - p -  1), \quad i = 0, 1, \dots, p,
    \end{displaymath}
    où $c_{ii}$ est le $(i + 1)$\ieme{} élément de la diagonale de la
    matrice $(\mat{X}^\prime \mat{X})^{-1}$ et $s^2 = \text{MSE}$.
  \item Que vaut $c_{11}$ en régression linéaire simple? Adapter le
    résultat ci-dessus à ce modèle.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Tout d'abord, si $Z \sim N(0,1)$ et $V \sim \upchi^2(r)$
      alors, par définition,
      \begin{displaymath}
        \frac{Z}{\sqrt{V/r}} \sim t(r).
      \end{displaymath}
      Tel que mentionné dans l'énoncé, $\hat{\beta}_i \sim N(\beta_i,
      \sigma^2 c_{ii})$ ou, de manière équivalente,
      \begin{displaymath}
        \frac{\hat{\beta}_i - \beta_i}{\sigma \sqrt{c_{ii}}} \sim
        N(0, 1).
      \end{displaymath}
      Par conséquent,
      \begin{displaymath}
        \frac{\frac{\hat{\beta}_i - \beta_i}{\sigma \sqrt{c_{ii}}}}{%
          \sqrt{\frac{\SSE}{\sigma^2(n - p - 1)}}}
        = \frac{\hat{\beta}_i - \beta_i}{s \sqrt{c_{ii}}}
        \sim t(n - p - 1).
      \end{displaymath}
    \item En régression linéaire simple, $c_{11} = 1/\sum_{t = 1}^n
      (X_t - \bar{X})^2 = 1/S_{XX}$ et $\sigma^2 c_{11} =
      \var{\hat{\beta}_1}$. Le résultat général en a) se réduit donc,
      en régression linéaire simple, au résultat bien connu du test $t$
      sur le paramètre $\beta_1$
      \begin{displaymath}
        \frac{\hat{\beta}_1 - \beta_1}{s \sqrt{1/S_{XX}}}
        \sim t(n - 1 - 1).
      \end{displaymath}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Considérer le modèle de régression linéaire $\mat{y} = \mat{X}
  \betavec + \epsvec$, où $\mat{X}$ est une matrice $n \times (p+1)$,
  $\var{\epsvec} = \sigma^2\, \mat{W}^{-1}$ et $\mat{W} = \diag(w_1,
  \dots, w_n)$.  Démontrer, en dérivant
  \begin{align*}
    S(\betavec)
    &= \sum_{t=1}^n w_t (\mat{y}_t - \mat{x}_t^\prime \betavec)^2 \\
    &= (\mat{y} - \mat{X} \betavec)^\prime\, \mat{W}\, (\mat{y} -
       \mat{X} \betavec)
  \end{align*}
  par rapport à $\betavec$, que les équations normales à résoudre pour
  obtenir l'estimateur des moindres carrés pondérés de $\betavec$ sont,
  sous forme matricielle,
  \begin{displaymath}
    (\mat{X}^\prime\, \mat{W}\, \mat{X}) \hat\betavec^* = \mat{X}^\prime\,
    \mat{W}\,  \mat{y},
  \end{displaymath}
  puis en déduire cet estimateur.  \emph{Astuce}: cette preuve est
  simple si l'on utilise le théorème
  \ref{thm:elements:derivee_fonction} de l'annexe \ref{chap:elements}
  avec $\mat{A} = \mat{W}$ et $f(\betavec) = \mat{y} - \mat{X} \betavec$.
  \begin{sol}
    En suivant les indications donnée dans l'énoncé, on obtient aisément
    \begin{align*}
      \frac{d}{d\betavec} S(\betavec)
      &= 2 \left(\frac{d}{d\betavec}(\mat{y} - \mat{X}
        \betavec)\right)^\prime \mat{W} (\mat{y} - \mat{X}
        \betavec) \\
      &= - 2 \mat{X}^\prime \mat{W} (\mat{y} - \mat{X}
        \betavec) \\
      &= -2 (\mat{X}^\prime \mat{W} \mat{y} - \mat{X}^\prime \mat{W}
      \mat{X} \betavec).
    \end{align*}
    Par conséquent, les équations normales à résoudre pour trouver
    l'estimateur $\hat\betavec^*$ minimisant la somme de carrés pondérés
    $S(\betavec)$ sont $(\mat{X}^\prime\, \mat{W}\, \mat{X}) \hat\betavec^* =
    \mat{X}^\prime\, \mat{W}\, \mat{y}$ et l'estimateur des moindres
    carrés pondérés est
    \begin{displaymath}
      \hat\betavec^* = (\mat{X}^\prime \mat{W} \mat{X})^{-1} \mat{X}^\prime
      \mat{W} \mat{y}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Considérer le modèle de régression linéaire simple passant par
  l'origine $Y_t = \beta X_t + \varepsilon_t$. Trouver l'estimateur
  linéaire sans biais à variance minimale du paramètre $\beta$, ainsi
  que sa variance, sous chacune des hypothèses suivantes.
  \begin{enumerate}
  \item $\var{\varepsilon_t} = \sigma^2$
  \item $\var{\varepsilon_t} = \sigma^2/w_t$
  \item $\var{\varepsilon_t} = \sigma^2 X_t$
  \item $\var{\varepsilon_t} = \sigma^2 X_t^2$
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\hat{\beta}^* = \sum_{t=1}^n X_t Y_t/\sum_{t=1}^n X_t^2$,
      $\var{\hat{\beta}^*} = \sigma^2/\sum_{t=1}^n X_t^2$
    \item $\hat{\beta}^* = \sum_{t=1}^n w_t X_t Y_t/\sum_{t=1}^n
      w_t X_t^2$,
      $\var{\hat{\beta}^*} = \sigma^2/\sum_{t=1}^n w_t X_t^2$
    \item $\hat{\beta}^* = \bar{Y}/\bar{X}$,
      $\var{\hat{\beta}^*} = \sigma^2/(n \bar{X})$
    \item $\hat{\beta}^* = \sum_{t=1}^n Y_t/X_t$,
      $\var{\hat{\beta}^*} = \sigma^2/n$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    De manière tout à fait générale, l'estimateur linéaire sans biais
    à variance minimale dans le modèle de régression linéaire $\mat{y}
    = \mat{X} \betavec + \epsvec$, $\var{\epsvec} = \sigma^2\,
    \mat{W}^{-1}$ est
    \begin{displaymath}
      \hat\betavec^* = (\mat{X}^\prime \mat{W} \mat{X})^{-1} \mat{X}^\prime
      \mat{W} \mat{y}
    \end{displaymath}
    et sa variance est, par le théorème \ref{thm:elements:esp_var},
    \begin{align*}
      \varmat{\hat\betavec^*}
      &= (\mat{X}^\prime \mat{W} \mat{X})^{-1} \mat{X}^\prime \mat{W}
      \varmat{\mat{y}}
      \mat{W}^\prime \mat{X} (\mat{X}^\prime \mat{W} \mat{X})^{-1} \\
      &= \sigma^2
      (\mat{X}^\prime \mat{W} \mat{X})^{-1} \mat{X}^\prime \mat{W}
      \mat{W}^{-1}
      \mat{W} \mat{X} (\mat{X}^\prime \mat{W} \mat{X})^{-1} \\
      &= \sigma^2 (\mat{X}^\prime \mat{W} \mat{X})^{-1}
    \end{align*}
    puisque les matrices $\mat{W}$ et $\mat{X}^\prime \mat{W} \mat{X}$
    sont symétriques. Dans le cas de la régression linéaire simple
    passant par l'origine et en supposant que $\mat{W} = \diag(w_1,
    \dots, w_n)$, ces formules se réduisent en
    \begin{align*}
      \hat{\beta}^*
      &= \frac{\sum_{t=1}^n w_t X_t Y_t}{\sum_{t=1}^n w_t X_t^2} \\
      \intertext{et}
      \var{\hat{\beta}^*}
      &= \frac{\sigma^2}{\sum_{t=1}^n w_t X_t^2}.
    \end{align*}
    \begin{enumerate}
    \item Cas déjà traité à l'exercice
      \ref{chap:simple}.\ref{ex:simple:origine} où $\mat{W} = \mat{I}$
      et, donc,
      \begin{align*}
        \hat{\beta}^*
        &= \frac{\sum_{t=1}^n X_t Y_t}{\sum_{t=1}^n X_t^2} \\
        \intertext{et}
        \var{\hat{\beta}^*}
        &= \frac{\sigma^2}{\sum_{t=1}^n w_t X_t^2}.
      \end{align*}
    \item Cas général traité ci-dessus.
    \item Si $\var{\varepsilon_t} = \sigma^2 X_t$, alors $w_t =
      X_t^{-1}$. Le cas général se simplifie donc en
      \begin{align*}
        \hat{\beta}^*
        &= \frac{\sum_{t=1}^n Y_t}{\sum_{t=1}^n X_t} \\
        &= \frac{\bar{Y}}{\bar{X}}, \\
        \var{\hat{\beta}^*}
        &= \frac{\sigma^2}{\sum_{t=1}^n X_t} \\
        &= \frac{\sigma^2}{n \bar{X}}.
      \end{align*}
    \item Si $\var{\varepsilon_t} = \sigma^2 X_t^2$, alors $w_t =
      X_t^{-2}$. On a donc
      \begin{align*}
        \hat{\beta}^*
        &= \frac{1}{n} \sum_{t=1}^n \frac{Y_t}{X_t} \\
        \var{\hat{\beta}^*}
        &= \frac{\sigma^2}{n}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:multiple:quadratique}
  Proposer, à partir des données ci-dessous, un modèle de
  régression complet (incluant la distribution du terme d'erreur)
  pouvant expliquer le comportement de la variable $Y$ en fonction de
  celui de $X$.
  \begin{center}
    \begin{tabular}{rr}
      \toprule
      \multicolumn{1}{c}{$Y$} & $X$ \\
      \midrule
      32,83 & 25 \\
       9,70 &  3 \\
      29,25 & 24 \\
      15,35 & 11 \\
      13,25 & 10 \\
      24,19 & 20 \\
       8,59 &  6 \\
      25,79 & 21 \\
      24,78 & 19 \\
      10,23 &  9 \\
       8,34 &  4 \\
      22,10 & 18 \\
      10,00 &  7 \\
      18,64 & 16 \\
      18,82 & 15 \\
      \bottomrule
    \end{tabular}
  \end{center}

  \begin{rep}
    $Y_t = 18,12 +
    29,68 X_t +
    4,09 X_t^2 + \varepsilon_t$,
    $\varepsilon_t \sim N(0, 1,373)$
  \end{rep}
  \begin{sol}
    Le graphique des valeurs de $Y$ en fonction de celles de $X$, à la
    figure \ref{fig:multiple:quadratique}, montre clairement une
    relation quadratique. On postule donc le modèle
    \begin{displaymath}
      Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_t^2 + \varepsilon_t, \quad
      \varepsilon_t \sim N(0, \sigma^2).
    \end{displaymath}
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(Y} \hlopt{~} \hlstd{X,} \hlkwc{data} \hlstd{= donnees)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-unnamed-chunk-36-1} 

}



\end{knitrout}
      \caption{Graphique des données de l'exercice
        \ref{chap:multiple}.\ref{ex:multiple:quadratique}}
      \label{fig:multiple:quadratique}
    \end{figure}
    Par la suite, on peut estimer les paramètres de ce modèle avec la
    fonction \texttt{lm} de \textsf{R}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlkwd{poly}\hlstd{(X,} \hlnum{2}\hlstd{),} \hlkwc{data} \hlstd{= donnees)}
\hlkwd{summary}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ poly(X, 2), data = donnees)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9123 -0.6150 -0.1905  0.6367  1.6921 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  18.1240     0.3025   59.91 3.10e-16 ***
## poly(X, 2)1  29.6754     1.1717   25.33 8.72e-12 ***
## poly(X, 2)2   4.0899     1.1717    3.49  0.00446 ** 
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.172 on 12 degrees of freedom
## Multiple R-squared:  0.982,	Adjusted R-squared:  0.979 
## F-statistic: 326.8 on 2 and 12 DF,  p-value: 3.434e-11
\end{verbatim}
\begin{alltt}
\hlkwd{anova}\hlstd{(fit)}
\end{alltt}
\begin{verbatim}
## Analysis of Variance Table
## 
## Response: Y
##            Df Sum Sq Mean Sq F value    Pr(>F)    
## poly(X, 2)  2 897.36  448.68  326.79 3.434e-11 ***
## Residuals  12  16.48    1.37                      
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}
\end{kframe}
\end{knitrout}
    Tant le test $F$ global que les tests $t$ individuels sont
    concluants, le coefficient de détermination est élevé et l'on peut
    constater à la figure \ref{fig:multiple:quadratique2} que
    l'ajustement du modèle est bon. On conclut donc qu'un modèle
    adéquat pour cet ensemble de données est
    \begin{displaymath}
      Y_t = 18,12 +
      29,68 X_t +
      4,09 X_t^2 + \varepsilon_t, \quad
      \varepsilon_t \sim N(0, 1,373).
    \end{displaymath}
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(Y} \hlopt{~} \hlstd{X,} \hlkwc{data} \hlstd{= donnees)}
\hlstd{x} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwd{min}\hlstd{(donnees}\hlopt{$}\hlstd{X),} \hlkwd{max}\hlstd{(donnees}\hlopt{$}\hlstd{X),} \hlkwc{length} \hlstd{=} \hlnum{200}\hlstd{)}
\hlkwd{lines}\hlstd{(x,} \hlkwd{predict}\hlstd{(fit,} \hlkwd{data.frame}\hlstd{(}\hlkwc{X} \hlstd{= x),} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{))}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-unnamed-chunk-38-1} 

}



\end{knitrout}
      \caption{Graphique des données de l'exercice
        \ref{chap:multiple}.\ref{ex:multiple:quadratique} et courbe
        obtenue par régression}
      \label{fig:multiple:quadratique2}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:multiple:pondere}
  On vous donne les 23 données dans le tableau ci-dessous.
  \begin{center}
    \begin{tabular}{rcc@{\qquad}rcc@{\qquad}rcc}
      \toprule
      $t$ & $Y_t$ & $X_t$ & $t$ & $Y_t$ & $X_t$ & $t$ & $Y_t$ & $X_t$ \\
      \midrule
      12 & 2,3 & 1,3 & 19 & 1,7 & 3,7 &  6 & 2,8 & 5,3 \\
      23 & 1,8 & 1,3 & 20 & 2,8 & 4,0 & 10 & 2,1 & 5,3 \\
      7 & 2,8 & 2,0 &  5 & 2,8 & 4,0 &  4 & 3,4 & 5,7 \\
      8 & 1,5 & 2,0 &  2 & 2,2 & 4,0 &  9 & 3,2 & 6,0 \\
      17 & 2,2 & 2,7 & 21 & 3,2 & 4,7 & 13 & 3,0 & 6,0 \\
      22 & 3,8 & 3,3 & 15 & 1,9 & 4,7 & 14 & 3,0 & 6,3 \\
      1 & 1,8 & 3,3 & 18 & 1,8 & 5,0 & 16 & 5,9 & 6,7 \\
      11 & 3,7 & 3,7 &  3 & 3,5 & 5,3 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item Calculer l'estimateur des moindres carrés ordinaires
    $\hat\betavec$.
  \item Supposons que la variance de $Y_{16}$ est $4 \sigma^2$ plutôt
    que $\sigma^2$. Recalculer la régression en a) en utilisant cette
    fois les moindres carrés pondérés.
  \item Refaire la partie b) en supposant maintenant que la variance
    de l'observation $Y_{16}$ est $16 \sigma^2$. Quelles différences
    note-t-on?
  \end{enumerate}

  \begin{rep}
    \begin{inparaenum}
    \item $\hat\betavec = (%
      1,4256,
      0,3158)$
    \item $\hat\betavec^* = (%
      1,7213,
      0,2243)$
    \item $\hat\betavec^* = (%
      1,808,
      0,1975)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Comme on peut le constater à la figure \ref{fig:multiple:pondere},
    le point $(X_{16}, Y_{16})$ est plus éloigné des autres. En b) et
    c), on diminue son poids dans la régression.
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(Y} \hlopt{~} \hlstd{X,} \hlkwc{data} \hlstd{= donnees)}
\hlkwd{points}\hlstd{(donnees}\hlopt{$}\hlstd{X[}\hlnum{16}\hlstd{], donnees}\hlopt{$}\hlstd{Y[}\hlnum{16}\hlstd{],} \hlkwc{pch} \hlstd{=} \hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-unnamed-chunk-40-1} 

}



\end{knitrout}
      \caption{Graphique des données de l'exercice
        \ref{chap:multiple}.\ref{ex:multiple:pondere}. Le cercle plein
        représente la donnée $(X_{16}, Y_{16})$.}
      \label{fig:multiple:pondere}
    \end{figure}
    \begin{enumerate}
    \item On calcule d'abord l'estimateur des moindres carrés ordinaires:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(fit1} \hlkwb{<-} \hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{X,} \hlkwc{data} \hlstd{= donnees))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X, data = donnees)
## 
## Coefficients:
## (Intercept)            X  
##      1.4256       0.3158
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item Si l'on suppose que la variance de la données $(X_{16},
      Y_{16})$ est quatre fois plus élevée que la variance des autres
      données, alors il convient d'accorder un point quatre fois moins
      grand à cette donnée dans la régression. Cela requiert les
      moindres carrés pondérés. Pour calculer les estimateurs avec
      \texttt{lm} dans \textsf{R}, on utilise l'argument
      \texttt{weights}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{w} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{nrow}\hlstd{(donnees))}
\hlstd{w[}\hlnum{16}\hlstd{]} \hlkwb{<-} \hlnum{0.25}
\hlstd{(fit2} \hlkwb{<-} \hlkwd{update}\hlstd{(fit1,} \hlkwc{weights} \hlstd{= w))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X, data = donnees, weights = w)
## 
## Coefficients:
## (Intercept)            X  
##      1.7213       0.2243
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item On répète la procédure en b) avec un poids de encore plus
      petit pour la donnée $(X_{16}, Y_{16})$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{w[}\hlnum{16}\hlstd{]} \hlkwb{<-} \hlnum{0.0625}
\hlstd{(fit3} \hlkwb{<-} \hlkwd{update}\hlstd{(fit1,} \hlkwc{weights} \hlstd{= w))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X, data = donnees, weights = w)
## 
## Coefficients:
## (Intercept)            X  
##      1.8080       0.1975
\end{verbatim}
\end{kframe}
\end{knitrout}
      Plus le poids accordé à la donnée $(X_{16}, Y_{16})$ est faible,
      moins la droite de régression est attirée vers ce point (voir la
      figure \ref{fig:multiple:pondere2}).
      \begin{figure}[t]
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(Y} \hlopt{~} \hlstd{X,} \hlkwc{data} \hlstd{= donnees)}
\hlkwd{points}\hlstd{(donnees}\hlopt{$}\hlstd{X[}\hlnum{16}\hlstd{], donnees}\hlopt{$}\hlstd{Y[}\hlnum{16}\hlstd{],} \hlkwc{pch} \hlstd{=} \hlnum{16}\hlstd{)}
\hlkwd{abline}\hlstd{(fit1,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlnum{1}\hlstd{)}
\hlkwd{abline}\hlstd{(fit2,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{abline}\hlstd{(fit3,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlnum{3}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlnum{1.2}\hlstd{,} \hlnum{6}\hlstd{,} \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"Modèle a)"}\hlstd{,} \hlstr{"Modèle b)"}\hlstd{,} \hlstr{"Modèle c)"}\hlstd{),}
       \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{3}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-unnamed-chunk-44-1} 

}



\end{knitrout}
        \caption{Graphique des données de l'exercice
          \ref{chap:multiple}.\ref{ex:multiple:pondere} avec les
          droites de régression obtenues à l'aide des moindres carrés
          pondérés.}
        \label{fig:multiple:pondere2}
      \end{figure}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:multiple:taxi}
  Une coopérative de taxi new-yorkaise s'intéresse à la consommation
  de carburant des douze véhicules de sa flotte en fonction de leur
  âge.  Hormis leur âge, les véhicules sont identiques et utilisent
  tous le même type d'essence. La seule chose autre différence notable
  d'un véhicule à l'autre est le sexe du conducteur: la coopérative
  emploie en effet des hommes et des femmes. La coopérative a
  recueilli les données suivantes afin d'établir un modèle de
  régression pour la consommation de carburant:
  \begin{center}
    \begin{tabular}{ccc}
      \toprule
      Consommation (mpg) & Âge du véhicule & Sexe du conducteur \\
      \midrule
      12,3 & 3 & M \\
      12,0 & 4 & F \\
      13,7 & 3 & F \\
      14,2 & 2 & M \\
      15,5 & 1 & F \\
      11,1 & 5 & M \\
      10,6 & 4 & M \\
      14,0 & 1 & M \\
      16,0 & 1 & F \\
      13,1 & 2 & M \\
      14,8 & 2 & F \\
      10,2 & 5 & M \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item En plaçant les points sur un graphique de la consommation de
    carburant en fonction de l'âge du véhicule, identifier s'il existe
    ou non une différence entre la consommation de carburant des
    femmes et celle des hommes. \emph{Astuce}: utiliser un symbole
    (\texttt{pch}) différent pour chaque groupe.
  \item Établir un modèle de régression pour la consommation de
    carburant. Afin de pouvoir intégrer la variable qualitative «sexe
    du conducteur» dans le modèle, utiliser une variable indicatrice
    du type
    \begin{displaymath}
      X_{t2} =
      \begin{cases}
        1, & \text{si le conducteur est un homme} \\
        0, & \text{si le conducteur est une femme}.
      \end{cases}
    \end{displaymath}
  \item Quelle est, selon le modèle établi en b), la consommation
    moyenne d'une voiture taxi de quatre ans conduite par une femme?
    Fournir un intervalle de confiance à 90~\% pour cette prévision.
  \end{enumerate}

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $\text{mpg} = %
      16,687
      -1,04 \text{ age}
      -1,206 \text{ sexe}$
    \item $12,53 \pm
      0,58 \text{ mpg}$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Voir la figure \ref{fig:multiple:taxi} pour le graphique. Il
      y a effectivement une différence entre la consommation de
      carburant des hommes et des femmes: ces dernières font plus de
      milles avec un gallon d'essence.
      \begin{figure}
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{hommes} \hlkwb{<-} \hlkwd{subset}\hlstd{(donnees, sexe} \hlopt{==} \hlstr{"M"}\hlstd{)}
\hlstd{femmes} \hlkwb{<-} \hlkwd{subset}\hlstd{(donnees, sexe} \hlopt{==} \hlstr{"F"}\hlstd{)}
\hlkwd{plot}\hlstd{(mpg} \hlopt{~} \hlstd{age,} \hlkwc{data} \hlstd{= hommes,}
     \hlkwc{xlim} \hlstd{=} \hlkwd{range}\hlstd{(donnees}\hlopt{$}\hlstd{age),} \hlkwc{ylim} \hlstd{=} \hlkwd{range}\hlstd{(donnees}\hlopt{$}\hlstd{mpg))}
\hlkwd{points}\hlstd{(mpg} \hlopt{~} \hlstd{age,} \hlkwc{data} \hlstd{= femmes,} \hlkwc{pch} \hlstd{=} \hlnum{16}\hlstd{)}
\hlkwd{legend}\hlstd{(}\hlnum{4}\hlstd{,} \hlnum{16}\hlstd{,} \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"Hommes"}\hlstd{,} \hlstr{"Femmes"}\hlstd{),} \hlkwc{pch} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{16}\hlstd{))}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=.45\linewidth]{figure/fig-unnamed-chunk-46-1} 

}



\end{knitrout}
        \caption{Graphique des données de l'exercice
          \ref{chap:multiple}.\ref{ex:multiple:taxi}}
        \label{fig:multiple:taxi}
      \end{figure}
    \item Remarquer que la variable \texttt{sexe} est un facteur et peut
      être utilisée telle quelle dans \texttt{lm}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(fit} \hlkwb{<-} \hlkwd{lm}\hlstd{(mpg} \hlopt{~} \hlstd{age} \hlopt{+} \hlstd{sexe,} \hlkwc{data} \hlstd{= donnees))}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = mpg ~ age + sexe, data = donnees)
## 
## Coefficients:
## (Intercept)          age        sexeM  
##      16.687       -1.040       -1.206
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item Calcul d'une prévision pour la valeur moyenne de la variable
      \texttt{mpg}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{predict}\hlstd{(fit,} \hlkwc{newdata} \hlstd{=} \hlkwd{data.frame}\hlstd{(}\hlkwc{age} \hlstd{=} \hlnum{4}\hlstd{,} \hlkwc{sexe} \hlstd{=} \hlstr{"F"}\hlstd{),}
        \hlkwc{interval} \hlstd{=} \hlstr{"confidence"}\hlstd{,} \hlkwc{level} \hlstd{=} \hlnum{0.90}\hlstd{)}
\end{alltt}
\begin{verbatim}
##        fit      lwr      upr
## 1 12.52876 11.94584 13.11168
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
Le modèle de régression linéaire multiple $$Y_i=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\beta_3 x_{i3}+\varepsilon_i, \mbox{ pour }i=1,...,n$$ a été ajusté à des données avec la méthode des moindres carrés.

\begin{enumerate}
\item La figure~\ref{fig:multiple:postulat1} montre le QQ-plot des résidus studentisés. À la lumière de ce graphique, y a-t-il un postulat du modèle qui n'est pas vérifié? Si oui, lequel et pourquoi? S'il y a lieu, expliquer l'impact de la violation de ce postulat. 

\bigskip
\item La figure~\ref{fig:multiple:postulat2} montre les résidus studentisés en fonction de chacune des variables exogènes et en fonction des valeurs prédites. Utiliser ces graphiques pour commenter sur la validité des postulats du modèle. Y en a-t-il qui ne sont pas respectés? S'il y a lieu, expliquer l'impact de la violation de ce ou ces postulats. 

\end{enumerate}

\begin{figure}
\centering \caption{QQ-Plot des résidus studentisés}\label{fig:multiple:postulat1}
\centerline{\includegraphics[width=0.6\textwidth]{figure/QQPlotLifeExp-c.pdf}}
\end{figure}

\begin{figure}
\centering \caption{Nuage de points des résidus studentisés en fonction de chacune des variables exogènes et en fonction de la variable prédite}\label{fig:multiple:postulat2}
\centerline{\includegraphics[width=14cm]{figure/ResPlots.pdf}}
\end{figure}

\begin{sol}
\begin{enumerate}
\item Le postulat de normalité semble violé.

La distribution des résidus a une queue inférieure plus épaisse que la loi normale, ce que l'on voit à gauche du Q-Q plot, puisque les poits ne sont pas alignés.

Le postulat de normalité n'est pas critique, parce que les estimateurs des moindres carrés ont un sens quand même. Toutefois, les tests d'hypothèses et les intervalles de confiance ne sont pas valides.

\item Le graphique des résidus en fonction de $x_2$ montre que le postulat de linéarité semble violé. Cela implique que le modèle n'est pas valide.

On observe de l'hétéroscédasticité (par exemple, dans les graphiques 1, 3 ou 4) puisque les résidus ne semblent pas avoir une variance constante. 

Cela signifie que les variances des paramètres ne sont pas calculées de façon appropriée OU il faudrait effectuer une transformation sur les variables pour régler ces problèmes.


\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
La base de données \texttt{OutlierExample.csv} disponible sur le site du cours contient 19 observations de base, et trois observations supplémentaires, notées par les \texttt{CODES} 1, 2 et 3, qui sont aberrantes ou influentes.

\begin{enumerate}
\item Importez la base de données et tracez un nuage de points de \texttt{Y} en fonction de \texttt{X}.
\item Roulez les lignes de code suivantes pour observer le graphique avec les 3 points ajoutés
\begin{verbatim}
library(ggplot2)
ggplot(dat, aes(x= X, y= Y, label=CODES))+
  geom_point() +
  geom_text(aes(label=ifelse(CODES>0,CODES,'')),hjust=0,vjust=0)
\end{verbatim}

\item Ajustez un modèle linéaire en incluant seulement les 19 points dont le code est 0. Regardez l'ajustement et commentez.
\item Ajustez un modèle linéaire en incluant les 19 points dont le code est 0 et le point de code 1. Quel est l'impact de l'inclusion de ce point sur le $R^2$ et sur les estimations des paramètres? Étudiez le résultat de la fonction \texttt{influence.measures()}.
\item Ajustez un modèle linéaire en incluant les 19 points dont le code est 0 et le point de code 2. Quel est l'impact de l'inclusion de ce point sur le $R^2$ et sur les estimations des paramètres? Étudiez le résultat de la fonction \texttt{influence.measures()}.
\item Ajustez un modèle linéaire en incluant les 19 points dont le code est 0 et le point de code 3. Quel est l'impact de l'inclusion de ce point sur le $R^2$ et sur les estimations des paramètres? Étudiez le résultat de la fonction \texttt{influence.measures()}.
\end{enumerate}

\begin{sol}
On pourrait croire qu'un point sur 20, ça ne change rien, mais ce n'est pas le cas! Le point 1 a un impact sur la pente et la qualité de l'ajustement. Le point 2 a un grand levier mais n'affecte pas beaucoup les estimations, le point 3 a un grand levier et un gros impact.

\begin{verbatim}
dat <- read.csv("OutlierExample.csv")

dim(dat)

summary(dat)

library(ggplot2)

ggplot(dat, aes(x= X, y= Y, label=CODES))+
  geom_point() +
  geom_text(aes(label=ifelse(CODES>0,CODES,'')),hjust=0,vjust=0)

fit0 <- lm(Y~X,dat,subset=(CODES==0))
summary(fit0)
plot(dat[,1:2],pch=16)
points(dat[match(1:3,dat$CODES),1:2],col=2:4,pch=16:18,cex=1.2)
abline(fit0)

fit1 <- lm(Y~X,dat,subset=(CODES<=1))
summary(fit1)
abline(fit1,col=2,lty=2)

fit2 <- lm(Y~X,dat,subset=(CODES%in%c(0,2)))
summary(fit2)
abline(fit2,col=3,lty=3)

fit3 <- lm(Y~X,dat,subset=(CODES%in%c(0,3)))
summary(fit3)
abline(fit3,col=4,lty=4)

influence.measures(fit0)
influence.measures(fit1)
influence.measures(fit2)
influence.measures(fit3)

\end{verbatim}
\end{sol}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-multiple}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:

%<<child='selection.Rnw'>>=
%@

\part{Modèles linéaires généralisés}

%<<child='glm.Rnw'>>=
%@
%
%<<child='comptage.Rnw'>>=
%@

\part{Annexes}
\appendix
%<<child='regression.Rnw'>>=
%@
%\include{elements}
\include{solutions}

%\nocite{Miller:stat:1977}

%\bibliography{stat,vg,r} %%% à arranger

\cleardoublepage
%\printindex %%% à vérifier

\cleardoublepage
\cleartoverso

\pagestyle{empty}
\renewcommand{\ttdefault}{hlst}

\bandeverso
\begin{textblock*}{71mm}(135mm, -50mm)
  \textblockcolor{}
%  \includegraphics{codebarre}
\end{textblock*}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
