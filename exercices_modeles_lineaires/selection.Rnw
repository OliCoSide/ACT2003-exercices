\chapter{Sélection de modèle et régression régularisée}
\label{chap:selection}

\Opensolutionfile{reponses}[reponses-selection]
\Opensolutionfile{solutions}[solutions-selection]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:selection}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:selection}}

\end{Filesave}


\begin{exercice}
Est-ce que les compagnies d'assurance utilisent l'ethnicité comme un facteur déterminant dans leur décision de rendre de l'assurance disponible? Fienberg (1985) a rassemblé des données d'un rapport de la \emph{U.S. Commission on Civil Rights} sur le nombre de polices d'assurance habitation émises à Chicago entre Décembre 1977 et Février 1978. Les polices d'assurance étaient placées dans 2 catégories:
\begin{itemize}
\item[$\bullet$] polices émises dans le marché standard, volontaire
\item[$\bullet$] polices émises dans le marché sous-standard, involontaire
\end{itemize}
Les polices du marché sous-standard sont émises selon un programme gouvernemental d'accès à l'assurance. Les personnes qui contractent ce type d'assurance se sont vues refuser une police d'assurance sur le marché volontaire. On s'intéresse à l'accessibilité à l'assurance selon l'ethnicité et on utilise le nombre de polices émises (ou renouvellées) sur le marché sous-standard comme mesure de ``non-accessibilité''.

\medskip \noindent
La ville de Chicago a été divisée en 45 régions (selon le code postal). Pour chaque région, on a les informations suivantes:
\begin{center}
\begin{tabular}{|l|l|}
\hline
Variable & Description\\
\hline
\texttt{race} & pourcentage de la population de la région provenant d'une minorité raciale\\
\texttt{fire}& Nombre d'incendies par millier de maisons\\
\texttt{theft}& Nombre de vols par millier de maisons\\
\texttt{age} & Pourcentage des maisons construites avant 1940\\
\texttt{involact} & Nouvelles polices et renouvellements dans le marché sous-standard,\\;
& par centaine de maisons\\
\texttt{income}& Revenu familial moyen\\ \hline
\end{tabular}
\end{center}
On s'intéresse majoritairement à l'effet de la variable explicative \texttt{race}, mais on veut aussi tenir compte des autres facteurs qui pourraient être en cause, et des interactions entre ces facteurs. Les modèles considérés sont:
\begin{description}
\item[Modèle A:] \texttt{involact}$\sim$\texttt{race}
\item[Modèle B:] \texttt{involact}$\sim$\texttt{race+I(log(income))}
\item[Modèle C:] \texttt{involact}$\sim$\texttt{race+fire+age}
\item[Modèle D:] \texttt{involact}$\sim$\texttt{race+fire+theft+age}
\item[Modèle E:] \texttt{involact}$\sim$\texttt{race+I(log(income))+fire+theft+age}
\item[Modèle F:] \texttt{involact}$\sim$\texttt{race+I(log(income))*age+fire+theft}
\item[Modèle G:] \texttt{involact}$\sim$\texttt{I(log(income))*(age+race)+fire+theft}
\item[Modèle H:] \texttt{involact}$\sim$\texttt{I(log(income))*age+race*(fire+theft+I(log(income)))}
\end{description}
Note: \texttt{A*B} représente \texttt{A+B+A:B}, c'est-à-dire les effets principaux et les interactions entre les variables explicatives \texttt{A} et \texttt{B}.

\noindent On a les informations suivantes sur les modèles A à H: 

\begin{center}
\begin{tabular}{|c|ccccccc|}
\hline
Modèle &     $p'$ &   PRESS    &   $R^2_p$ &    $C_p$ de Mallows &        AIC   &    BIC   &    $R^2_a$ \\ \hline
A&     2 &9.6344 &0.4735 &63.24 &-69.86 &-66.25 &0.5126\\
B      &3 &8.8248& 0.5177 &49.55&  -75.20& -69.78& 0.5761\\
C   &   4& 5.2083 &0.7154 & 8.58 &-103.09&-95.87 &0.7765\\
D    &  5 &4.5727&0.7501&  7.97& -103.75& -94.71& 0.7840\\
E     & 6& 4.8985 &0.7323  &9.88 &-101.84 &-91.00 &0.7790\\
F      &7& 4.8999& 0.7322& 9.64& -102.25& -89.61& 0.7850\\
G     & 8 &4.7528 &0.7403& 8.46&-103.92 &-89.47 &0.7964\\
H     &10& 5.4817& 0.7004& 10.00 &-102.98& -84.91&0.7989\\ \hline
\end{tabular}
\end{center}

\noindent Les facteurs d'inflation de la variance pour ces modèles sont présentés dans le tableau suivant:

\begin{center}
\begin{tabular}{|l|rrrrrr|}
\hline
 &  C& D& E & F&G&H\\\hline
\texttt{race}& 1.73&1.81&3.81&3.83&2191&5449\\
\texttt{fire}& 2.03&2.03&2.16 &2.48&2.50&19\\
\texttt{age}& 1.25 &  1.39 &2.08 &4070 &5247&6316\\
\texttt{theft}&& 1.23&1.63&1.64 &1.68&4.05\\
\texttt{I(log(income))}& & & 4.66&21 &21&22\\
\texttt{I(log(income)):age}& & & &3793 &4932 &5919 \\
\texttt{I(log(income)):race}&&&&&2064&5155 \\
\texttt{race:theft}&&&&&&24\\
\texttt{race:fire} &&&&&&40\\\hline
\end{tabular}
\end{center}

\noindent On sait également que les postulats de la régression linéaire multiple sont vérifiés.
        
\begin{enumerate}
\item Quel est le meilleur modèle selon 
\begin{enumerate}
\item le critère PRESS?
\item le critère du coefficient de détermination de prévision $R^2_p$?
\item le $C_p$ de Mallows?
\item le critère d'information d'Akaike?
\item le critère d'information de Bayes?
\item le coefficient de détermination ajusté $R^2_a$?
\end{enumerate}

\item Que peut-on remarquer en regardant les facteurs d'inflation de la variance pour les modèles C à H?

\item Selon vous, quel serait le meilleur modèle à utiliser pour ces données? Pourquoi?
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item 
\begin{enumerate}
\item modèle D
\item modèle D
\item modèle G
\item modèle G
\item modèle C
\item modèle H
\end{enumerate}

\item Il y a un très gros problème de multicolinéarité pour les modèle F, G et H, car certains VIFs sont beaucoup plus grands que 10. Ce problème augmente inutilement la variance des paramètres estimés.

\item On évite les modèles F G et H pour ne pas avoir de problème de multicolinéarité. Le modèle D est préférable selon les critères PRESS et $R^2_p$. De plus, ses critères AIC et BIC sont les deuxièmes plus petits. Le $C_p$ est 8, donc 8-5=3. Ce n'est pas parfait, mais ce n'est pas si mal, etc. 

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Cet exercice est inspiré de James et al. (2013). Considérons le cas simplifié où $n=p$ et la matrice d'incidence $\Xmat$ est diagonale, avec des 1 sur la diagonale et des 0 pour tous les éléments hors-diagonale.

On ajuste une régression linéaire multiple passant par l'origine avec de telles données, c'est-à-dire que $\beta_0=0$ est connu et on ne l'estime pas.


Sous ces hypothèses,
\begin{enumerate}
\item Trouver les estimateurs des moindres carrés $\hat\beta_1,\ldots,\hat\beta_p$.

\item Écrire l'expression à minimiser pour trouver les estimateurs sous la régression ridge.

\item Trouver l'expression de l'estimateur ridge.

\item Écrire l'expression à minimiser pour trouver les estimateurs sous la régression lasso.

\item Démontrer que l'estimateur lasso a la forme
$$
\hat\beta_j^{\mathrm{lasso}}=\left\{\begin{array}{ll}
y_j-\lambda/2, & \mbox{ si } Y_j> \lambda/2\\
y_j+\lambda/2, & \mbox{ si } Y_j < -\lambda/2\\
0, & \mbox{ si } |Y_j| < \lambda/2.
\end{array}\right.
$$

\item Interpréter les effets des pénalités ridge et lasso à la lumière de vos réponses aux sous-questions précédentes.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Puisque $n=p$, $\beta_0=0$ et que la matrice d'incidence est diagonale, on a $\hat{Y}_i = \hat\beta_i$ pour $i=1,\ldots,n$. On minimise $S(\beta)=\sum_{i=1}^n (Y_i-\beta_i)^2$ et on trouve pour $i\in \{1,\ldots,n\}$,
$$
\left.\frac{\partial}{\partial\beta_i}S(\beta)\right|_{\hat\beta_i}= -2 (Y_i-\hat\beta_i) =0 \quad \Rightarrow \quad \hat\beta_i =  Y_i.
$$

\item On minimise, pour une valeur $\lambda>0$,
$$
S^{\mathrm{ridge}}(\betavec)=\sum_{i=1}^n (Y_i-\beta_i)^2 +\lambda \sum_{i=1}^n \beta_i^2.
$$

\item On a
$$
\frac{\partial}{\partial\beta_i}S^{\mathrm{ridge}}(\betavec)=-2(Y_i-\beta_i) +2 \lambda  \beta_i.
$$
On pose égal à 0 et on trouve
$$
Y_i-\hat \beta_i^{\mathrm{ridge}} = \lambda  \hat\beta_i^{\mathrm{ridge}} \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  \frac{Y_i}{1+\lambda}.
$$

\item On minimise, pour une valeur $\lambda>0$,
$$
S^{\mathrm{lasso}}(\betavec)=\sum_{i=1}^n (Y_i-\beta_i)^2 +\lambda \sum_{i=1}^n |\beta_i|.
$$

\item On a
$$
\frac{\partial}{\partial\beta_i}S^{\mathrm{lasso}}(\betavec)=-2(Y_i-\beta_i) + \lambda\, \mathrm{signe}(\beta_i) .
$$
On utilise les EMV trouvés en a) pour définir le signe. Supposons d'abord que $\hat\beta_i = Y_i>0$. Alors, on a aussi $\hat\beta_i^{\mathrm{lasso}}>0$ (sinon, changer le signe donnera une valeur plus petite de l'équation à minimiser). On pose la dérivée égale à 0 et on trouve
$$
2(Y_i-\hat \beta_i^{\mathrm{lasso}}) = \lambda  \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  Y_i- \lambda/2,
$$
ce qui tient seulement si $\hat\beta_i^{\mathrm{lasso}}>0$, alors on a $\hat\beta_i^{\mathrm{ridge}} =  \max(0,Y_i- \lambda/2)$.
Supposons ensuite que $\hat \beta_i = Y_i <0$. Alors, on a aussi $\hat\beta_i^{\mathrm{lasso}}<0$. On pose la dérivée égale à 0 et on trouve
$$
2(Y_i-\hat \beta_i^{\mathrm{lasso}}) = -\lambda  \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  Y_i+ \lambda/2,
$$
sous la contrainte que ce soit négatif, donc dans ce cas, $\hat\beta_i^{\mathrm{ridge}} =  \min(0,Y_i+\lambda/2)$. On combine les deux cas et on obtient l'équation donnée.

\item On peut voir que la façon de rapetisser les paramètres est bien différente pour les deux méthodes. Avec ridge, chaque coefficient des moindres carrés est réduit par la même proportion. Avec lasso, chaque coefficient des moindres carrés est réduit vers 0 d'un montant constant $\lambda/2$; ceux qui sont plus petits que $\lambda/2$ en valeur absolue sont mis exactement égaux à 0. C'est de cette façon que le lasso permet de faire la sélection des variables explicatives.

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
On considère un modèle de régression linéaire
$$
Y_i = \beta_0+\beta_1 x_i+\varepsilon_i,
$$
avec $\varepsilon_i\sim \Norm(0,\sigma^2)$ pour $i=1,\ldots,8$ pour la base de données suivante~:

\begin{center}
\begin{tabular}{ccc}
\toprule
$i$ & $x_i$ & $y_i$ \\ \midrule
1 & $-2$ & 35\\
2 & $-1$ & 40\\
3 & $-1$ & 36\\
4 & $-1$ & 38\\
5 & $0$ & 40\\
6 & $1$ & 43\\
7 & $2$ & 45\\
8 & $2$ & 43\\ \bottomrule
\end{tabular}
\end{center}

\begin{enumerate}
\item En utilisant la régression Ridge avec $\lambda=0$, estimer les paramètres $\beta_0$ et $\beta_1$.
\item En utilisant la régression Ridge avec $\lambda=4$, calculer l'erreur quadratique moyenne.
\end{enumerate}

\begin{rep}
\begin{enumerate}
\item $40$ et $2.1875$   
\item $1.8125$
\end{enumerate}
\end{rep}

\begin{sol}
\begin{enumerate}
\item Avec un paramètre de régularisation de $\lambda=0$, il s'agit d'une régression linéaire simple. On a alors, puisque $\bar{x}=0$,
\begin{align*}
\hat\beta_1 &= \frac{\sum_{i=1}^8 x_i Y_i}{\sum_{i=1}^8 x_i^2}= \frac{35}{16} = \Sexpr{round(35/16,4)}\\
\hat\beta_0 &= \bar Y = \Sexpr{mean(c(35,40,36,38,40,43,45,43))}.
\end{align*}
\item Pour la régression Ridge, on a vu que l'ordonnée à l'origine n'est pas affectée par la pénalité alors $\hat\beta_0=40$. La solution générale pour $p$ variables explicatives est
$$
\betavec_\lambda^{\mathrm{ridge}} = (\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}\Xmat^\top \yvec.
$$
En dimension $p=1$, cela se réduit à
$$
\hat\beta_1 = \frac{\sum_{i=1}^8 x_i Y_i}{\sum_{i=1}^8 x_i^2 +\lambda} = \frac{35}{16+\lambda}.
$$
Pour $\lambda=4$, on obtient $\hat\beta_1=1.75$. Cette valeur est, comme prévu, plus près de 0 que celle obtenue en a). L'équation du modèle est alors
$$
\hat{Y} = 40+ 1.75x.
$$
Par la suite, on obtient les prévisions 
\begin{align*}
\hat Y_1 & = 40+1.75\times-2 = 36.5\\
\hat Y_2 & =\hat Y_3=\hat Y_4 = 38.25\\
\hat Y_5&=40\\
\hat Y_6&=41.75\\
\hat Y_7 & =\hat Y_8= 43.50.
\end{align*}
et on calcule l'erreur quadratique moyenne
$$
\MSE = \frac{\sum_{i=1}^8 (Y_i - \hat{Y}_i)^2}{8} = 1.8125.
$$
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Sachant que l'estimateur des moindres carrés $\betavec = (\Xmat^\top\Xmat)^{-1}\Xmat^\top\yvec$ est sans biais pour $\betavec$, vérifier que, si $\lambda\ne 0$, l'estimateur du modèle ridge
$$
\betavec_\lambda^{\mathrm{ridge}} = (\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}\Xmat^\top\yvec
$$
est biaisé.

\begin{sol}
On a
\begin{align*}
\betavec_\lambda^{\mathrm{ridge}} &= (\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}\Xmat^{\top}\yvec\\
&=(\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}(\Xmat^\top\Xmat)(\Xmat^\top\Xmat)^{-1}\Xmat^{\top}\yvec\\
&=(\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}(\Xmat^\top\Xmat)\betavec.
\end{align*}
Si $\lambda=0$, on a 
$$
\Esp{\betavec_\lambda^{\mathrm{ridge}}} = \esp{\betavec} = \betavec.
$$
Sinon, on a
$$
\Esp{\betavec_\lambda^{\mathrm{ridge}}} = \Esp{(\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}(\Xmat^\top\Xmat)\betavec} = (\Xmat^\top\Xmat+\lambda \mathbf{I}_p)^{-1}(\Xmat^\top\Xmat)\betavec \ne \betavec.
$$
\end{sol}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-selection}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
