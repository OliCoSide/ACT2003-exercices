\chapter{Sélection de modèle et régression régularisée}
\label{chap:selection}

\Opensolutionfile{reponses}[reponses-selection]
\Opensolutionfile{solutions}[solutions-selection]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:selection}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:selection}}

\end{Filesave}


<<echo=FALSE>>=
options(width = 55)
@

\begin{exercice}
Est-ce que les compagnies d'assurance utilisent la race comme un facteur déterminant dans leur décision de rendre de l'assurance disponible? Fienberg (1985) a rassemblé des données d'un rapport de la \emph{U.S. Commission on Civil Rights} sur le nombre de polices d'assurance habitation émises à Chicago entre Décembre 1977 et Février 1978. Les polices d'assurance étaient placées dans 2 catégories:
\begin{itemize}
\item[$\bullet$] polices émises dans le marché standard, volontaire
\item[$\bullet$] polices émises dans le marché sous-standard, involontaire
\end{itemize}
Les polices du marché sous-standard sont émises selon un programme gouvernemental d'accès à l'assurance. Les personnes qui contractent ce type d'assurance se sont vues refuser une police d'assurance sur le marché volontaire. On s'intéresse à l'accessibilité à l'assurance selon la race et on utilise le nombre de polices émises (ou renouvellées) sur le marché sous-standard comme mesure de ``non-accessibilité''.

\medskip \noindent
La ville de Chicago a été divisée en 45 régions (selon le code postal). Pour chaque région, on a les informations suivantes:
\begin{center}
\begin{tabular}{|l|l|}
\hline
Variable & Description\\
\hline
\texttt{race} & pourcentage de la population de la région provenant d'une minorité raciale\\
\texttt{fire}& Nombre d'incendies par millier de maisons\\
\texttt{theft}& Nombre de vols par millier de maisons\\
\texttt{age} & Pourcentage des maisons construites avant 1940\\
\texttt{involact} & Nouvelles polices et renouvellements dans le marché sous-standard, par centaine de maisons\\
\texttt{income}& Revenu familial moyen\\ \hline
\end{tabular}
\end{center}
On s'intéresse majoritairement à l'effet de la variable explicative \texttt{race}, mais on veut aussi tenir compte des autres facteurs qui pourraient être en cause, et des intéractions entre ces facteurs. Les modèles considérés sont:
\begin{description}
\item[Modèle A:] \texttt{involact}$\sim$\texttt{race}
\item[Modèle B:] \texttt{involact}$\sim$\texttt{race+I(log(income))}
\item[Modèle C:] \texttt{involact}$\sim$\texttt{race+fire+age}
\item[Modèle D:] \texttt{involact}$\sim$\texttt{race+fire+theft+age}
\item[Modèle E:] \texttt{involact}$\sim$\texttt{race+I(log(income))+fire+theft+age}
\item[Modèle F:] \texttt{involact}$\sim$\texttt{race+I(log(income))*age+fire+theft}
\item[Modèle G:] \texttt{involact}$\sim$\texttt{I(log(income))*(age+race)+fire+theft}
\item[Modèle H:] \texttt{involact}$\sim$\texttt{I(log(income))*age+race*(fire+theft+I(log(income)))}
\end{description}
Note: \texttt{A*B} représente \texttt{A+B+A:B}, c'est-à-dire les effets principaux et les intéractions entre les variables explicatives \texttt{A} et \texttt{B}.

\noindent On a les informations suivantes sur les modèles A à H: 

\begin{center}
\begin{tabular}{|c|ccccccc|}
\hline
Modèle &     $p'$ &   PRESS    &   $R^2_p$ &    $C_p$ de Mallows &        AIC   &    BIC   &    $R^2_a$ \\ \hline
A&     2 &9.6344 &0.4735 &63.24 &-69.86 &-66.25 &0.5126\\
B      &3 &8.8248& 0.5177 &49.55&  -75.20& -69.78& 0.5761\\
C   &   4& 5.2083 &0.7154 & 8.58 &-103.09&-95.87 &0.7765\\
D    &  5 &4.5727&0.7501&  7.97& -103.75& -94.71& 0.7840\\
E     & 6& 4.8985 &0.7323  &9.88 &-101.84 &-91.00 &0.7790\\
F      &7& 4.8999& 0.7322& 9.64& -102.25& -89.61& 0.7850\\
G     & 8 &4.7528 &0.7403& 8.46&-103.92 &-89.47 &0.7964\\
H     &10& 5.4817& 0.7004& 10.00 &-102.98& -84.91&0.7989\\ \hline
\end{tabular}
\end{center}

\noindent Les facteurs d'inflation de la variance pour ces modèles sont présentés dans le tableau suivant:

\begin{center}
\begin{tabular}{|l|rrrrrr|}
\hline
 &  C& D& E & F&G&H\\\hline
\texttt{race}& 1.73&1.81&3.81&3.83&2191&5449\\
\texttt{fire}& 2.03&2.03&2.16 &2.48&2.50&19\\
\texttt{age}& 1.25 &  1.39 &2.08 &4070 &5247&6316\\
\texttt{theft}&& 1.23&1.63&1.64 &1.68&4.05\\
\texttt{I(log(income))}& & & 4.66&21 &21&22\\
\texttt{I(log(income)):age}& & & &3793 &4932 &5919 \\
\texttt{I(log(income)):race}&&&&&2064&5155 \\
\texttt{race:theft}&&&&&&24\\
\texttt{race:fire} &&&&&&40\\\hline
\end{tabular}
\end{center}

\noindent On sait également que les postulats de la régression linéaire multiple sont vérifiés.
        
\begin{enumerate}
\item Quel est le meilleur modèle selon 
\begin{enumerate}
\item le critère PRESS?
\item le critère du coefficient de détermination de prévision $R^2_p$?
\item le $C_p$ de Mallows?
\item le critère d'information d'Akaike?
\item le critère d'information de Bayes?
\item le coefficient de détermination ajusté $R^2_a$?
\end{enumerate}

\item Que peut-on remarquer en regardant les facteurs d'inflation de la variance pour les modèles C à H?

\item Selon vous, quel serait le meilleur modèle à utiliser pour ces données? Pourquoi?
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item 
\begin{enumerate}
\item modèle D
\item modèle D
\item modèle G
\item modèle G
\item modèle C
\item modèle H
\end{enumerate}

\item Il y a un très gros problème de multicolinéarité pour les modèle F, G et H, car certains VIFs sont beaucoup plus grands que 10. Ce problème augmente inutilement la variance des paramètres estimés.

\item On évite les modèles F G et H pour ne pas avoir de problème de multicolinéarité. Le modèle D est préférable selon les critères PRESS et $R^2_p$. De plus, ses critères AIC et BIC sont les deuxièmes plus petits. Le $C_p$ est 8, donc 8-5=3. Ce n'est pas parfait, mais ce n'est pas si mal, etc. 

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Cet exercice est inspiré de James et al. (2013). Considérons le cas simplifié où $n=p$ et la matrice d'incidence $X$ est diagonale, avec des 1 sur la diagonale et des 0 pour tous les éléments hors-diagonale.

On ajuste une régression linéaire multiple passant par l'origine avec de telles données, c'est-à-dire que $\beta_0=0$ est connu et on ne l'estime pas.

\medskip
Sous ces hypothèses,
\begin{enumerate}
\item Trouvez les estimateurs des moindres carrés $\hat\beta_1,\ldots,\hat\beta_p$.

\item Écrivez l'expression à minimiser pour trouver les estimateurs sous la régression ridge.

\item Trouvez l'expression de l'estimateur ridge.

\item Écrivez l'expression à minimiser pour trouver les estimateurs sous la régression lasso.

\item Montrez que l'estimateur lasso a la forme
$$
\hat\beta_j^{\mathrm{lasso}}=\left\{\begin{array}{ll}
y_j-\lambda/2, & \mbox{ si } y_j> \lambda/2\\
y_j+\lambda/2, & \mbox{ si } y_j < -\lambda/2\\
0, & \mbox{ si } |y_j| < \lambda/2.
\end{array}\right.
$$

\item Interprétez les effets des pénalités ridge et lasso à la lumière de vos réponses aux sous-questions précédentes.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Puisque $n=p$, $\beta_0=0$ et que la matrice d'incidence est diagonale, on a $\hat{y}_i = \hat\beta_i$ pour $i=1,\ldots,n$. On minimise $S(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2$ et on trouve pour $i\in \{1,\ldots,n\}$,
$$
\left.\frac{\partial}{\partial\beta_i}S(\bobeta)\right|_{\hat\beta_i}= -2 (y_i-\hat\beta_i) =0 \quad \Rightarrow \quad \hat\beta_i =  y_i.
$$

\item On minimise, pour une valeur $\lambda>0$,
$$
S^{\mathrm{ridge}}(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2 +\lambda \sum_{i=1}^n \beta_i^2.
$$

\item On a
$$
\frac{\partial}{\partial\beta_i}S^{\mathrm{ridge}}(\bobeta)=-2(y_i-\beta_i) +2 \lambda  \beta_i.
$$
On pose égal à 0 et on trouve
$$
y_i-\hat \beta_i^{\mathrm{ridge}} = \lambda  \hat\beta_i^{\mathrm{ridge}} \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  \frac{y_i}{1+\lambda}.
$$

\item On minimise, pour une valeur $\lambda>0$,
$$
S^{\mathrm{lasso}}(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2 +\lambda \sum_{i=1}^n |\beta_i|.
$$

\item On a
$$
\frac{\partial}{\partial\beta_i}S^{\mathrm{lasso}}(\bobeta)=-2(y_i-\beta_i) + \lambda\, \mathrm{signe}(\beta_i) .
$$
On utilise les EMV trouvés en a) pour définir le signe. Supposons d'abord que $\hat\beta_i = y_i>0$. Alors, on a aussi $\hat\beta_i^{\mathrm{lasso}}>0$ (sinon, changer le signe donnera une valeur plus petite de l'équation à minimiser). On pose la dérivée égale à 0 et on trouve
$$
2(y_i-\hat \beta_i^{\mathrm{lasso}}) = \lambda  \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  y_i- \lambda/2,
$$
ce qui tient seulement si $\hat\beta_i^{\mathrm{lasso}}>0$, alors on a $\hat\beta_i^{\mathrm{ridge}} =  \max(0,y_i- \lambda/2)$.
Supposons ensuite que $\hat \beta_i = y_i <0$. Alors, on a aussi $\hat\beta_i^{\mathrm{lasso}}<0$. On pose la dérivée égale à 0 et on trouve
$$
2(y_i-\hat \beta_i^{\mathrm{lasso}}) = -\lambda  \quad \Rightarrow \quad \hat\beta_i^{\mathrm{ridge}} =  y_i+ \lambda/2,
$$
sous la contrainte que ce soit négatif, donc dans ce cas, $\hat\beta_i^{\mathrm{ridge}} =  \min(0,y_i+\lambda/2)$. On combine les deux cas et on obtient l'équation donnée.

\item On peut voir que la façon de rapetisser les paramètres est bien différente pour les deux méthodes. Avec ridge, chaque coefficient des moindres carrés est réduit par la même proportion. Avec lasso, chaque coefficient des moindres carrés est réduit vers 0 d'un montant constant $\lambda/2$; ceux qui sont plus petits que $\lambda/2$ en valeur absolue sont mis exactement égaux à 0. C'est de cette façon que le lasso permet de faire la sélection des variables explicatives.

\end{enumerate}
\end{sol}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-selection}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
