\documentclass[11pt]{book}
\usepackage[latin1]{inputenc} % Pour pouvoir taper les accent directement et non pas passer par \'
\usepackage[T1]{fontenc} %%% Pour que les accents soient correctement traités dans le PDF
\usepackage{lmodern}     %%% Pour que les accents soient correctement traités dans le PDF
\usepackage[francais]{babel}
%\FrenchItemizeSpacingfalse
\usepackage{amsfonts} % Permet l'utilisation de plus de polices de caractères en mode mathématique
\usepackage{natbib} % Le style natbib pour les références bibliographiques
\usepackage{epsfig} % Pour utiliser la commande \epsfig
\usepackage{multirow} % Pour faire des tableaux contenant des lignes fusionnées
\usepackage{verbatim} % Permet d'insérer un fichier ASCII brut
\usepackage{rotating} % Perment l'utilisation de l'environnement sideways
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Si on crée des commandes, les ajouter ici. Exemple :

\usepackage{epstopdf} %permet d'insérer les eps avec le compilateur pdflatex
\usepackage{ccaption}
\usepackage{url}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{dsfont}
\usepackage{graphicx,color}
\usepackage{array}
\usepackage[mathscr]{eucal}

\newcommand{\betavec}{\boldsymbol{\beta}}
\newcommand{\yvec}{\mathbf{Y}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\epsvec}{\boldsymbol{\varepsilon}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\var}{\mathbb{V}\text{ar}}
\newcommand{\cov}{\mathbb{C}\text{ov}}
\newcommand{\corr}{\mathbb{C}\text{orr}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\N}{\mathcal{N}}
\def\I{\mathbb{I}}
\def\R{\mathbb{R}}
\def\P{{\mathbb P}}
\newcommand{\bm}{\boldmath}
\newcommand{\um}{\unboldmath}
\newcommand{\botheta}{\mbox{\bm$\theta$\um}}
\newcommand{\boTheta}{\mbox{\bm$\Theta$\um}}
\newcommand{\boY}{\mbox{\bm$Y$\um}}
\newcommand{\bobeta}{\mbox{\bm$\beta$}}
\newcommand{\boeps}{\mbox{\bm$\varepsilon$\um}}
\newcommand{\boldx}{\mbox{\bm$x$\um}}
\newcommand{\boR}{\mbox{\bm$R$\um}}
\newcommand{\boun}{\mbox{\bm$1$\um}}
\newcommand{\boz}{\mbox{\bm$0$\um}}
\newcommand{\bolambda}{\mbox{\bm$\lambda$\um}}
\newcommand{\bov}{\mbox{\bm$v$\um}}
\newcommand{\boalpha}{\mbox{\bm$\alpha$\um}}
\newcommand{\bo}[1]{\mbox{\bm$#1$\um}}
\newcommand{\boX}{\bo{X}}
\newcommand{\boV}{\bo{V}}
\newcommand{\hbobeta}{\hat{\bobeta}}
\newcommand{\hboeps}{\hat{\boeps}}
\newcommand{\hY}{\hat{\boY}}
\newcommand{\boH}{\bo{H}}
\newcommand{\boI}{\bo{\I}}
\newcommand{\eproof}{\hfill$\sqcap\!\!\!\!\sqcup$}
\newcommand{\bou}{\mbox{\bm$u$\um}}

\title{ACT 2003}
\author{Marie-Pier Côté}

\setlength{\oddsidemargin}{-.2in}
\setlength{\evensidemargin}{-.2in}
\setlength{\textwidth}{7in}

\makeindex

\begin{document}

\begin{center}

\textsc{\Large École d'actuariat}\\
\textsc{\Large Université Laval}\\[0.5cm]

% Title
\HRule \\
{ \LARGE \bfseries Modèles linéaires en actuariat \\ }
{ \LARGE \bfseries ACT-2003 \\ }
{ \LARGE \bfseries Exercices Chapitre 4\\ }
\HRule \\[0.4cm]

\Large Marie-Pier \textsc{Côté}\\
\textsc{Automne} 2018

\end{center}

\vfill


\noindent \textbf{Question 1.} Est-ce que les compagnies d'assurance utilisent la race comme un facteur déterminant dans leur décision de rendre de l'assurance disponible? Fienberg (1985) a rassemblé des données d'un rapport de la \emph{U.S. Commission on Civil Rights} sur le nombre de polices d'assurance habitation émises à Chicago entre Décembre 1977 et Février 1978. Les polices d'assurance étaient placées dans 2 catégories:
\begin{itemize}
\item[$\bullet$] polices émises dans le marché standard, volontaire
\item[$\bullet$] polices émises dans le marché sous-standard, involontaire
\end{itemize}
Les polices du marché sous-standard sont émises selon un programme gouvernemental d'accès à l'assurance. Les personnes qui contractent ce type d'assurance se sont vues refuser une police d'assurance sur le marché volontaire. On s'intéresse à l'accessibilité à l'assurance selon la race et on utilise le nombre de polices émises (ou renouvellées) sur le marché sous-standard comme mesure de ``non-accessibilité''.

\bigskip \noindent
La ville de Chicago a été divisée en 45 régions (selon le code postal). Pour chaque région, on a les informations suivantes:
\begin{center}
\begin{tabular}{|l|l|}
\hline
Variable & Description\\
\hline
\texttt{race} & pourcentage de la population de la région provenant d'une minorité raciale\\
\texttt{fire}& Nombre d'incendies par millier de maisons\\
\texttt{theft}& Nombre de vols par millier de maisons\\
\texttt{age} & Pourcentage des maisons construites avant 1940\\
\texttt{involact} & Nouvelles polices et renouvellements dans le marché sous-standard, par centaine de maisons\\
\texttt{income}& Revenu familial moyen\\ \hline
\end{tabular}
\end{center}
On s'intéresse majoritairement à l'effet de la variable explicative \texttt{race}, mais on veut aussi tenir compte des autres facteurs qui pourraient être en cause, et des intéractions entre ces facteurs. Les modèles considérés sont:
\begin{description}
\item[Modèle A:] \texttt{involact}$\sim$\texttt{race}
\item[Modèle B:] \texttt{involact}$\sim$\texttt{race+I(log(income))}
\item[Modèle C:] \texttt{involact}$\sim$\texttt{race+fire+age}
\item[Modèle D:] \texttt{involact}$\sim$\texttt{race+fire+theft+age}
\item[Modèle E:] \texttt{involact}$\sim$\texttt{race+I(log(income))+fire+theft+age}
\item[Modèle F:] \texttt{involact}$\sim$\texttt{race+I(log(income))*age+fire+theft}
\item[Modèle G:] \texttt{involact}$\sim$\texttt{I(log(income))*(age+race)+fire+theft}
\item[Modèle H:] \texttt{involact}$\sim$\texttt{I(log(income))*age+race*(fire+theft+I(log(income)))}
\end{description}
Note: \texttt{A*B} représente \texttt{A+B+A:B}, c'est-à-dire les effets principaux et les intéractions entre les variables explicatives \texttt{A} et \texttt{B}.

\noindent On a les informations suivantes sur les modèles A à H: 

\begin{center}
\begin{tabular}{|c|ccccccc|}
\hline
Modèle &     $p'$ &   PRESS    &   $R^2_p$ &    $C_p$ de Mallows &        AIC   &    BIC   &    $R^2_a$ \\ \hline
A&     2 &9.6344 &0.4735 &63.24 &-69.86 &-66.25 &0.5126\\
B      &3 &8.8248& 0.5177 &49.55&  -75.20& -69.78& 0.5761\\
C   &   4& 5.2083 &0.7154 & 8.58 &-103.09&-95.87 &0.7765\\
D    &  5 &4.5727&0.7501&  7.97& -103.75& -94.71& 0.7840\\
E     & 6& 4.8985 &0.7323  &9.88 &-101.84 &-91.00 &0.7790\\
F      &7& 4.8999& 0.7322& 9.64& -102.25& -89.61& 0.7850\\
G     & 8 &4.7528 &0.7403& 8.46&-103.92 &-89.47 &0.7964\\
H     &10& 5.4817& 0.7004& 10.00 &-102.98& -84.91&0.7989\\ \hline
\end{tabular}
\end{center}

\noindent Les facteurs d'inflation de la variance pour ces modèles sont présentés dans le tableau suivant:

\begin{center}
\begin{tabular}{|l|rrrrrr|}
\hline
 &  C& D& E & F&G&H\\\hline
\texttt{race}& 1.73&1.81&3.81&3.83&2191&5449\\
\texttt{fire}& 2.03&2.03&2.16 &2.48&2.50&19\\
\texttt{age}& 1.25 &  1.39 &2.08 &4070 &5247&6316\\
\texttt{theft}&& 1.23&1.63&1.64 &1.68&4.05\\
\texttt{I(log(income))}& & & 4.66&21 &21&22\\
\texttt{I(log(income)):age}& & & &3793 &4932 &5919 \\
\texttt{I(log(income)):race}&&&&&2064&5155 \\
\texttt{race:theft}&&&&&&24\\
\texttt{race:fire} &&&&&&40\\\hline
\end{tabular}
\end{center}

\noindent On sait également que les postulats de la régression linéaire multiple sont vérifiés.

\bigskip        
\begin{itemize}
\item[a)] Quel est le meilleur modèle selon 
\begin{itemize}
\item[(i)] le critère PRESS?
\item[(ii)] le critère du coefficient de détermination de prévision $R^2_p$?
\item[(iii)] le $C_p$ de Mallows?
\item[(iv)] le critère d'information d'Akaike?
\item[(v)] le critère d'information de Bayes?
\item[(vi)] le coefficient de détermination ajusté $R^2_a$?
\end{itemize}

\vspace{0.3cm}
\item[b)]  Que peut-on remarquer en regardant les facteurs d'inflation de la variance pour les modèles C à H?

\vspace{0.3cm}
\item[c)]   Selon vous, quel serait le meilleur modèle à utiliser pour ces données? Pourquoi?

\end{itemize}

\bigskip\bigskip
\noindent \textbf{Question 2. Un cas simplifié de régression ridge et lasso.} 
Cet exercice est inspiré de James et al. (2013). Considérons le cas simplifié où $n=p$ et la matrixce d'incidence $X$ est diagonale, avec des 1 sur la diagonale et des 0 pour tous les éléments hors-diagonale.

On ajuste une régression linéaire multiple passant par l'origine avec de telles données, c'est-à-dire que $\beta_0=0$ est connu et on ne l'estime pas.

\medskip
Sous ces hypothèses,
\begin{itemize}
\item [a)] Trouvez les estimateurs des moindres carrés $\hat\beta_1,\ldots,\hat\beta_p$.

\medskip
\item [b)] Écrivez l'expression à minimiser pour trouver les estimateurs sous la régression ridge.

\medskip
\item [c)] Trouvez l'expression de l'estimateur ridge.

\medskip
\item [d)] Écrivez l'expression à minimiser pour trouver les estimateurs sous la régression lasso.

\medskip
\item [e)] Montrez que l'estimateur lasso a la forme
$$
\hat\beta_j^{{\rm lasso}}=\left\{\begin{array}{ll}
y_j-\lambda/2, & \mbox{ si } y_j> \lambda/2\\
y_j+\lambda/2, & \mbox{ si } y_j < -\lambda/2\\
0, & \mbox{ si } |y_j| < \lambda/2.
\end{array}\right.
$$

\item [f)] Interprétez les effets des pénalités ridge et lasso à la lumière de vos réponses aux sous-questions précédentes.
\end{itemize}



\newpage
\begin{center}
\LARGE{\textbf{SOLUTIONS}}
\end{center}

\noindent \textbf{Question 1.} 
      
\begin{itemize}
\item[a)]  
\begin{itemize}
\item[(i)]  modèle D
\item[(ii)] modèle D
\item[(iii)] modèle G
\item[(iv)] modèle G
\item[(v)] modèle C
\item[(vi)] modèle H
\end{itemize}

\vspace{0.3cm}
\item[b)] Il y a un très gros problème de multicolinéarité pour les modèle F, G et H, car certains VIFs sont beaucoup plus grands que 10. Ce problème augmente inutilement la variance des paramètres estimés.

\vspace{0.3cm}
\item[c)]  On évite les modèles F G et H pour ne pas avoir de problème de multicolinéarité. Le modèle D est préférable selon les critères PRESS et $R^2_p$. De plus, ses critères AIC et BIC sont les deuxièmes plus petits. Le $C_p$ est 8, donc 8-5=3. Ce n'est pas parfait, mais ce n'est pas si mal, etc. 

\end{itemize}


\bigskip\bigskip
\noindent \textbf{Question 2. } 

\begin{itemize}
\item [a)] Puisque $n=p$, $\beta_0=0$ et que la matrice d'incidence est diagonale, on a $\hat{y}_i = \hat\beta_i$ pour $i=1,\ldots,n$. On minimise $S(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2$ et on trouve pour $i\in \{1,\ldots,n\}$,
$$
\left.\frac{\partial}{\partial\beta_i}S(\bobeta)\right|_{\hat\beta_i}= -2 (y_i-\hat\beta_i) =0 \quad \Rightarrow \quad \hat\beta_i =  y_i.
$$

\medskip
\item [b)] On minimise, pour une valeur $\lambda>0$,
$$
S^{{\rm ridge}}(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2 +\lambda \sum_{i=1}^n \beta_i^2.
$$

\medskip
\item [c)] On a
$$
\frac{\partial}{\partial\beta_i}S^{{\rm ridge}}(\bobeta)=-2(y_i-\beta_i) +2 \lambda  \beta_i.
$$
On pose égal à 0 et on trouve
$$
y_i-\hat \beta_i^{{\rm ridge}} = \lambda  \hat\beta_i^{{\rm ridge}} \quad \Rightarrow \quad \hat\beta_i^{{\rm ridge}} =  \frac{y_i}{1+\lambda}.
$$

\medskip
\item [d)] On minimise, pour une valeur $\lambda>0$,
$$
S^{{\rm lasso}}(\bobeta)=\sum_{i=1}^n (y_i-\beta_i)^2 +\lambda \sum_{i=1}^n |\beta_i|.
$$

\medskip
\item [e)] On a
$$
\frac{\partial}{\partial\beta_i}S^{{\rm lasso}}(\bobeta)=-2(y_i-\beta_i) + \lambda\, {\rm signe}(\beta_i) .
$$
On utilise les EMV trouvés en a) pour définir le signe. Supposons d'abord que $\hat\beta_i = y_i>0$. Alors, on a aussi $\hat\beta_i^{{\rm lasso}}>0$ (sinon, changer le signe donnera une valeur plus petite de l'équation à minimiser). On pose la dérivée égale à 0 et on trouve
$$
2(y_i-\hat \beta_i^{{\rm lasso}}) = \lambda  \quad \Rightarrow \quad \hat\beta_i^{{\rm ridge}} =  y_i- \lambda/2,
$$
ce qui tient seulement si $\hat\beta_i^{{\rm lasso}}>0$, alors on a $\hat\beta_i^{{\rm ridge}} =  \max(0,y_i- \lambda/2)$.
Supposons ensuite que $\hat \beta_i = y_i <0$. Alors, on a aussi $\hat\beta_i^{{\rm lasso}}<0$. On pose la dérivée égale à 0 et on trouve
$$
2(y_i-\hat \beta_i^{{\rm lasso}}) = -\lambda  \quad \Rightarrow \quad \hat\beta_i^{{\rm ridge}} =  y_i+ \lambda/2,
$$
sous la contrainte que ce soit négatif, donc dans ce cas, $\hat\beta_i^{{\rm ridge}} =  \min(0,y_i+\lambda/2)$. On combine les deux cas et on obtient l'équation donnée.

\medskip
\item [f)] On peut voir que la façon de rapetisser les paramètre est bien différente pour les deux méthodes. Avec ridge, chaque coefficient des moindres carrés est réduit par la même proportion. Avec lasso, chaque coefficient des moindres carrés est réduit vers 0 d'un montant constant $\lambda/2$; ceux qui sont plus petits que $\lambda/2$ en valeur absolue sont mis exactement égaux à 0. C'est de cette façon que le lasso permet de faire la sélection des variables explicatives.

\end{itemize}

\end{document}
