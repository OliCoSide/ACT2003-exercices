\documentclass[11pt]{book}
\usepackage[latin1]{inputenc} % Pour pouvoir taper les accent directement et non pas passer par \'
\usepackage[T1]{fontenc} %%% Pour que les accents soient correctement traités dans le PDF
\usepackage{lmodern}     %%% Pour que les accents soient correctement traités dans le PDF
\usepackage[francais]{babel}
%\FrenchItemizeSpacingfalse
\usepackage{amsfonts} % Permet l'utilisation de plus de polices de caractères en mode mathématique
\usepackage{natbib} % Le style natbib pour les références bibliographiques
\usepackage{epsfig} % Pour utiliser la commande \epsfig
\usepackage{multirow} % Pour faire des tableaux contenant des lignes fusionnées
\usepackage{verbatim} % Permet d'insérer un fichier ASCII brut
\usepackage{rotating} % Perment l'utilisation de l'environnement sideways
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Si on crée des commandes, les ajouter ici. Exemple :

\usepackage{epstopdf} %permet d'insérer les eps avec le compilateur pdflatex
\usepackage{ccaption}
\usepackage{url}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{dsfont}
\usepackage{graphicx,color}
\usepackage{array}
\usepackage[mathscr]{eucal}


\newtheorem{Theorem}{Théorème}[chapter]
\newtheorem{Lemme}[Theorem]{Lemme}
\newtheorem{exemple}[Theorem]{Exemple}
\newtheorem{definition}[Theorem]{Définition}
\newtheorem{exercice}[Theorem]{Exercice}
\newtheorem{corollary}[Theorem]{Corollaire}
\newtheorem{remarque}[Theorem]{Remarque}

\newcommand{\betavec}{\boldsymbol{\beta}}
\newcommand{\yvec}{\mathbf{Y}}
\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\epsvec}{\boldsymbol{\varepsilon}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\esp}{{\rm E}}
\newcommand{\var}{\rm{var}}
\newcommand{\cov}{\rm{cov}}
\newcommand{\corr}{\rm{corr}}
\newcommand{\prob}{{\rm P}}
\newcommand{\N}{\mathcal{N}}
\def\I{\mathbb{I}}
\def\R{\mathbb{R}}
\def\P{{\rm P}}
\newcommand{\bm}{\boldmath}
\newcommand{\um}{\unboldmath}
\newcommand{\botheta}{\mbox{\bm$\theta$\um}}
\newcommand{\boTheta}{\mbox{\bm$\Theta$\um}}
\newcommand{\boY}{\mbox{\bm$Y$\um}}
\newcommand{\bobeta}{\mbox{\bm$\beta$}}
\newcommand{\boeps}{\mbox{\bm$\varepsilon$\um}}
\newcommand{\boldx}{\mbox{\bm$x$\um}}
\newcommand{\boR}{\mbox{\bm$R$\um}}
\newcommand{\boun}{\mbox{\bm$1$\um}}
\newcommand{\boz}{\mbox{\bm$0$\um}}
\newcommand{\bolambda}{\mbox{\bm$\lambda$\um}}
\newcommand{\bov}{\mbox{\bm$v$\um}}
\newcommand{\boalpha}{\mbox{\bm$\alpha$\um}}
\newcommand{\bo}[1]{\mbox{\bm$#1$\um}}
\newcommand{\boX}{\bo{X}}
\newcommand{\boV}{\bo{V}}
\newcommand{\hbobeta}{\hat{\bobeta}}
\newcommand{\hboeps}{\hat{\boeps}}
\newcommand{\hY}{\hat{\boY}}
\newcommand{\boH}{\bo{H}}
\newcommand{\boI}{\bo{\I}}
\newcommand{\eproof}{\hfill$\sqcap\!\!\!\!\sqcup$}
\newcommand{\bou}{\mbox{\bm$u$\um}}

\title{ACT 2003}
\author{Marie-Pier Côté}

\setlength{\oddsidemargin}{-.2in}
\setlength{\evensidemargin}{-.2in}
\setlength{\textwidth}{7in}

\makeindex

\begin{document}

\begin{center}
\Large{ACT-2003 Modèles linéaires en actuariat}

\textbf{Exercices\\ Modèles linéaires généralisés}

Marie-Pier Côté

Automne 2018
\end{center}

\normalsize

\begin{enumerate}
\item Est-ce que les distributions suivantes font partie de la famille exponentielle linéaire? Si oui, écrire la densité sous la forme exponentielle linéaire, donner le paramètre canonique, le paramètre de dispersion, l'espérance et la variance de $Y$ en termes de la fonction $b()$ et la relation $V()$ entre la moyenne et la variance.
\smallskip
\begin{itemize}
\item[i.] Normale$(\mu,\sigma^2)$
\smallskip
\item[ii.] Uniforme$(0,\beta)$
\smallskip
\item[iii.] Poisson$(\lambda)$
\smallskip
\item[iv.] Bernoulli$(\pi)$
\smallskip
\item[v.] Binomiale$(m, \pi)$, $m>0$ est un entier et est connu (On considère $Y^*=Y/m$).
\smallskip
\item[vi.] Pareto$(\alpha,\lambda)$
\smallskip
\item[vii.] Gamma$(\alpha,\beta)$
\smallskip
\item[viii.] Binomiale négative$(r,\pi)$ avec $r$ connu (On considère $Y^*=Y/r$).
\end{itemize}

\bigskip
\item Quelles fonctions de lien peut-on utiliser pour un GLM avec une loi de Poisson?

\bigskip
\item Quel est le lien canonique pour la loi gamma? Est-ce que ce lien est toujours approprié?

\bigskip
\item On suppose que $Y_1,...,Y_n$ sont des v.a.s indépendantes et $Y_i\sim Poisson(\mu_i)$. Pour chaque observation, on a une seule variable explicative $x_i$.
\begin{itemize}
\smallskip
\item[i.] Quel est le lien canonique?
\smallskip
\item[ii.] Trouver les fonctions de score (à résoudre pour l'estimation des paramètres par maximum de vraisemblance) 
\end{itemize}

\bigskip
\item Montrer que la déviance pour le modèle binomial est $$D(y,\hat{\mu})=2\sum_{i=1}^n m_i \left[y_i\ln\left(\frac{y_i}{\hat{\mu}_i}\right)+(1-y_i)\ln\left(\frac{1-y_i}{1-\hat{\mu}_i}\right)\right].$$

\bigskip
\item Trouver les expressions des résidus de Pearson, d'Anscombe et de déviance pour la loi Gamma.

\bigskip
\item Les données suivantes représentent des données de comptage, du nombre d'échec pour trois appareils médicaux (M1, M2 et M3) lors de tests de résistance sur 1000 appareils de chaque type et pour quatre niveaux de résistance mécanique différents (I, II, III, IV).

\begin{center}
\begin{tabular}{c|cccc}
\texttt{Device}$\backslash$ \texttt{Stress Level} & \texttt{I} & \texttt{II} & \texttt{III} & \texttt{IV}\\ \hline
\texttt{M1} &6 &8& 18 &10\\
\texttt{M2}& 13& 18& 29& 20\\
\texttt{M3} &9& 8 &21& 19\\ 
\end{tabular}
\end{center}

À l'aide de la modélisation Poisson (lien canonique), évaluer s'il y a une différence significative entre les taux d'échec des appareils.

\bigskip

\item \textbf{Réclamations d'assurance automobile au Royaume-Uni.}\footnote{Source des données: McCullagh, P., and Nelder, J. A. (1989). Generalized linear models. Chapman and Hall, London.} Les données pour cet exercice sot contenues dans le fichier \texttt{BritishCar.csv (sep='';'')} disponible sur le site du cours. On y trouve les montants de réclamations moyens pour les dommages causés au véhicule du détenteur de la police pour les véhicules assurés au Royaume-Uni en 1975. Les moyennes sont en livres sterling ajustées pour l'inflation.

\begin{center}
\begin{tabular}{l|l}
\hline
Variable & Description\\ \hline
\texttt{OwnerAge} & Âge du détenteur de la police (8 catégories) \\
\texttt{Model} & Type de voiture (4 groupes) \\
\texttt{CarAge} & Âge du véhicule, en années (4 catégories) \\
\texttt{NClaims} & Nombre de réclamations\\
\texttt{AvCost} & Coût moyen par réclamation, en livres sterling\\ \hline
\end{tabular}
\end{center}

On s'intéresse à la modélisation du coût moyen par réclamation.

\bigskip
\begin{itemize}
\item[a)] Ajuster un modèle de régression Gamma avec lien inverse pour la variable endogène \texttt{AvCost}. Inclure les effets principaux \texttt{OwnerAge}, \texttt{Model} et \texttt{CarAge}.

\smallskip
\item[b)] Quelle est l'espérance du coût moyen de la réclamation pour un détenteur de police âgé entre 17 et 20 ans, avec une auto de type A âgée de moins de 3 ans ?

\smallskip
\item[c)] Interpréter brièvement les coefficients pour la variable exogène \texttt{OwnerAge}.

\smallskip
\item[d)] Interpréter brièvement les coefficients pour la variable exogène \texttt{Model}.

\smallskip
\item[e)] Interpréter brièvement les coefficients pour la variable exogène \texttt{CarAge}.

\smallskip
\item[f)] Pour quelle combinaison de variables exogènes l'espérance du coût de réclamation est-elle la plus élevée? Calculer sa valeur.

\smallskip
\item[g)] Pour quelle combinaison de variables exogènes l'espérance du coût de réclamation est-elle la plus faible? Calculer sa valeur.

\smallskip
\item[h)] Quelle est la déviance pour ce modèle? Est-ce que le modèle semble adéquat?

\smallskip
\item[i)] Tracer le graphique des résidus de Pearson en fonction des valeurs prédites, des résidus d'Ascombe en fonction des valeurs prédites et des résidus de déviance en fonction des valeurs prédites.

\smallskip
\item[j)] Obtient-on les mêmes conclusions aux sous-questions a) à h) si on utilise un lien logarithmique plutôt que le lien inverse?


\end{itemize}

\bigskip
\item On considère les données suivantes, qui contiennent le nombre $Y_i$ de turbines sur $m_i$ qui ont été fissurées après $x_i$ heures d'opération.

\begin{center}
\begin{tabular}{rrr}
\hline
$x_i$ &$m_i$ & $Y_i$\\\hline
400& 39 &2\\
1000& 53 &4\\
1400 &33& 3\\
1800& 73 &7\\
2200 &30& 5\\
2600& 39 &9\\
3000 &42& 9\\
3400& 13 &6\\
3800 &34& 22\\
4200& 40 &21\\
4600 &36& 21\\\hline
\end{tabular}
\end{center}

\smallskip
\begin{itemize}
\item[(a)] En utilisant un GLM binomial avec lien canonique, dériver les estimateurs des paramètres lorsque $x_i$ est traité comme une variable exogène dichotomique avec 11 niveaux, et lorsque le prédicteur linéaire pour la donnée $i$ est $$\eta_i=\beta_0+\beta_i, \mbox{ pour } i=1,...,11,$$ avec la contrainte d'identifiabilité que $\beta_1=0$.

\smallskip
\item[(b)] En utilisant \textsf{R} et un GLM binomial avec lien canonique, ajuster le modèle où le prédicteur linéaire est $$\eta_i=\beta_0+\beta_1 x_i, \mbox{ pour } i=1,...,11.$$ Donner les estimations des paramètres et leur écart-type.

\smallskip
\item[(c)] Refaire (b) en utilisant un lien probit. Donner les estimations des paramètres et leur écart-type.

\smallskip
\item[(d)] Refaire (b) en utilisant un lien log-log complémentaire. Donner les estimations des paramètres et leur écart-type.

\smallskip
\item[(e)] Comparer les prévisions (et leurs mesures d'incertitude) sous les trois modèles ajustés en (b), (c) et (d) pour une turbine qui était en opération pour 2000 heures.

\smallskip
\item[(f)] Tracer un graphique pour montrer si les modèles en (b), (c) et (d) ajustent bien (ou non) les données. Commenter.

\end{itemize}
\end{enumerate}

\newpage \noindent \textbf{Solutions}

\begin{enumerate}
\item 
\begin{itemize}
\item[i.] Normale$(\mu,\sigma^2)$: oui, 
\begin{align*}
f_{Y}(y)&=\frac{1}{(2\pi\sigma^2)^1/2}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right),\, y\in\mathbb{R}\\
&=\exp\left(\frac{y\mu-\mu^2/2}{\sigma^2}-\frac{y^2}{2\sigma^2}-\frac{\ln(2\pi\sigma^2)}{2}\right),\, y\in\mathbb{R}.
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\mu$
\item[$\bullet$] Paramètre de dispersion: $\phi=\sigma^2$
\item[$\bullet$] $b(\theta)=\frac{\theta^2}{2}$
\item[$\bullet$] $E[Y]=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\frac{\theta^2}{2}=\theta=\mu$
\item[$\bullet$] $\var(Y)=\phi\ddot{b}(\theta)=\sigma^2\frac{\partial}{\partial\theta}\theta=\sigma^2$
\item[$\bullet$] $V(\mu)=1$.
\end{itemize}
\item[ii.] Uniforme$(0,\beta)$: non. Le domaine dépend du paramètre $\beta$.
\item[iii.] Poisson$(\lambda)$: 
\begin{align*}
f_{Y}(y;\lambda)&=\frac{\lambda^{y}e^{-\lambda}}{y!} \mbox{, pour }y\in \mathbb{N}^{+}\\
&=\exp\{y\ln \lambda - \lambda - \ln y!\}\\
f_{Y}(y;\theta,\phi)&=\exp\left\{\frac{y\theta - e^{\theta}}{\phi} - \ln y!\right\}.
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln \lambda$
\item[$\bullet$] Paramètre de dispersion: $\phi=1$
\item[$\bullet$] $b(\theta)=e^{\theta}$
\item[$\bullet$] $E[Y]=\dot{b}(\theta)=\frac{\partial}{\partial\theta}e^{\theta}=e^{\theta}=\lambda$
\item[$\bullet$] $\var(Y)=\phi\ddot{b}(\theta)=\frac{\partial}{\partial\theta}e^{\theta}=e^{\theta}=\lambda$
\item[$\bullet$] $V(\mu)=\mu$.
\end{itemize}
\item[iv.] Bernoulli$(\pi)$
\begin{align*}
f_{Y}(y;\pi)&=\pi^{y}(1-\pi)^{1-y}1(y \in \{0,1\})\\
&=\exp\left\{y\ln\left(\frac{\pi}{1-\pi}\right)+\ln(1-\pi)\right\}1(y \in \{0,1\}).\\
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln\left(\frac{\pi}{1-\pi}\right)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1$
\item[$\bullet$] $b(\theta)=\ln(1+e^{\theta})$
\item[$\bullet$] $E[Y]=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\ln(1+e^{\theta})=\frac{e^{\theta}}{1+e^\theta}=\pi$
\item[$\bullet$] $\var(Y)=\phi\ddot{b}(\theta)=\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1+e^\theta}=\frac{e^\theta}{(1+e^\theta)^2}=\pi(1-\pi)$
\item[$\bullet$] $V(\mu)=\mu(1-\mu)$.
\end{itemize}

\item[v.] Binomiale$(m, \pi)$, $m>0$ est un entier et est connu.
\begin{align*}
f_{Y}(y;\pi)&=\begin{pmatrix} m\\y\end{pmatrix}\pi^{y}(1-\pi)^{m-y}1(y \in \{0,1,...,m\})\\
&=\exp\left\{y\ln\left(\frac{\pi}{1-\pi}\right)+m\ln(1-\pi)+\ln\begin{pmatrix} m\\y\end{pmatrix}\right\}1(y \in \{0,1,...,m\}).\\
\end{align*}
Dans cette représentation, on a $$E[Y]=m\pi \mbox{ et } Var(Y)=m\pi(1-\pi).$$ Cette forme est moins utilisée car l'espérance de $Y$ dépend de $m$, le paramètre de dispersion. Souvent, on transforme les données. On utilise plutôt $Y^{*}=Y/m$. Alors, pour ces données transformées,
\begin{align*}
f_{Y^{*}}(y;\pi)&=\exp\left\{my\ln\left(\frac{\pi}{1-\pi}\right)+m\ln(1-\pi)+\ln\begin{pmatrix} m\\my\end{pmatrix}\right\}, \,y \in \{0,1/m,...,1\}\\
&=\exp\left\{\frac{y\ln\left(\frac{\pi}{1-\pi}\right)+\ln(1-\pi)}{1/m}+\ln\begin{pmatrix} m\\my\end{pmatrix}\right\}, \,y \in \{0,1/m,...,1\}.\\
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln\left(\frac{\pi}{1-\pi}\right)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1/m$
\item[$\bullet$] $b(\theta)=\ln(1+e^{\theta})$
\item[$\bullet$] $E[Y^*]=\dot{b}(\theta)=\frac{\partial}{\partial\theta}\ln(1+e^{\theta})=\frac{e^{\theta}}{1+e^\theta}=\pi$
\item[$\bullet$] $\var(Y^*)=\phi\ddot{b}(\theta)=\frac{1}{m}\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1+e^\theta}=\frac{e^\theta}{m(1+e^\theta)^2}=\frac{\pi(1-\pi)}{m}$
\item[$\bullet$] $V(\mu)=\mu(1-\mu)$.
\end{itemize}

\item[vi.] Pareto$(\alpha,\lambda)$: non.
\item[vii.] Gamma$(\alpha,\beta)$
Soit $Y\sim Gamma(\alpha,\beta)$. Alors, avec un peu de travail, la densité peut être écrite sous la forme exponentielle linéaire.
$$f_{Y}(y;\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha-1}e^{-\beta y},$$ pour $y>0$. On reparamétrise: $\mu=\alpha/\beta=E[Y]$ et $\alpha$, on a donc $\beta=\alpha/\mu$ et $$f_{Y}(y;\alpha,\mu)=\frac{1}{y\Gamma(\alpha)}\left(\frac{\alpha y}{\mu}\right)^{\alpha}\exp\left\{-\frac{\alpha y}{\mu}\right\}.$$ Posons $\theta=-1/\mu$, et $a(\phi)=1/\alpha$, alors on trouve $$f_{Y}(y;\theta,\phi)=\exp\left\{\frac{y\theta +\ln(-\theta)}{\phi} +\alpha\ln\alpha+(\alpha-1)\ln y -\ln\Gamma(\alpha)\right\}.$$ Donc, $b(\theta)=-\ln(-\theta)$ et $a(\phi)=1/\alpha \Rightarrow \dot{b}(\theta)=\frac{-1}{\theta}=\mu$ et $\ddot{b}(\theta)=\frac{1}{\theta^{2}}=\mu^{2}$. Finalement, $$E[Y]=\frac{-1}{\theta}=\mu \mbox{ et } Var(Y)=\frac{1}{\alpha}\mu^{2}.$$
\item[viii.] Binomiale négative$(r,\pi)$ avec $r$ connu. On considère $Y^*=Y/r$:
\begin{align*}
f_Y^*(y)&=\begin{pmatrix} r+ry-1\\ry\end{pmatrix}\pi^r(1-\pi)^{ry}, \mbox{ pour } y\in \{0,\frac{1}{r},\frac{2}{r},...\}\\
&=\exp\left(ry\ln(1-\pi)+r\ln\pi+\ln\begin{pmatrix} r+ry-1\\ry\end{pmatrix}\right).
\end{align*}
\begin{itemize}
\item[$\bullet$] Paramètre canonique: $\theta=\ln(1-\pi)$
\item[$\bullet$] Paramètre de dispersion:  $\phi=1/r$
\item[$\bullet$] $b(\theta)=-\ln(1-e^\theta)$
\item[$\bullet$] $E[Y^*]=\dot{b}(\theta)=\frac{\partial}{\partial\theta}-\ln(1-e^\theta)=\frac{e^{\theta}}{1-e^\theta}=\frac{1-\pi}{\pi}$
\item[$\bullet$] $\var(Y^*)=\phi\ddot{b}(\theta)=\frac{1}{r}\frac{\partial}{\partial\theta}\frac{e^{\theta}}{1-e^\theta}=\frac{e^\theta}{r(1-e^\theta)^2}=\frac{(1-\pi)}{r\pi^2}$
\item[$\bullet$] $V(\mu)=\mu(\mu+1)$.
\end{itemize}
\item 
\end{itemize}

\item Le lien canonique est le lien log: $\eta=\ln(\mu)$. On pourrait aussi utiliser d'autres fonctions de lien, telle que le lien identité $\eta=\mu$, le lien inverse $\eta=\frac{1}{\mu}$, mais le lien log est le plus approprié parce que son utilisation garantit une moyenne $\mu$ positive, ce qui est nécessaire pour la loi de Poisson.

\item Le lien canonique pour la loi Gamma est le lien inverse $\eta=1/\mu$. Comme la moyenne d'une loi Gamma est toujours positive, ce lien n'est pas toujours approprié parce qu'il ne restreint pas le domaine de $\mu$ aux réels positifs. Le lien log serait plus approprié dans certains cas.

\item 
\begin{itemize}
\item[i.] $\eta=g(\mu)=\ln(\mu)$
\item[ii.] On a $$\ln(\mu_i)=\eta_i=\beta_0+\beta_1x_i.$$ La densité de la loi Poisson est
\begin{align*}
f_{Y_i}(y_i;\mu_i)&=\exp\left(y_i\ln\mu_i-\mu_i-\ln y_i!\right)\\
f_{Y_i}(y_i;\beta_0,\beta_1)&=\exp\left(y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}-\ln y_i!\right).
\end{align*}
La fonction de vraisemblance et la log-vraisemblance sont donc:
\begin{align*}
\mathcal{L}(\beta_0,\beta_1)&=\prod_{i=1}^n f_{Y_i}(y_i;\beta_0,\beta_1)=\prod_{i=1}^n\exp\left(y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}-\ln y_i!\right)\\
\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_i(\beta_0+\beta_1x_i)-e^{\beta_0+\beta_1x_i}+\mbox{constante}.
\end{align*}
On maximise la log-vraisemblance:
\begin{align*}
\frac{\partial}{\partial\beta_0}\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_i-e^{\beta_0+\beta_1x_i}\\
\frac{\partial}{\partial\beta_1}\ell(\beta_0,\beta_1)&=\sum_{i=1}^n y_ix_i-x_i e^{\beta_0+\beta_1x_i}\\
\end{align*}
Donc, les équations à résoudre sont
\begin{align*}
\sum_{i=1}^n y_i-e^{\beta_0+\beta_1x_i}&=0\\
\sum_{i=1}^n x_i(y_i-e^{\beta_0+\beta_1x_i})&=0.\\
\end{align*}
\end{itemize}

\item La déviance est $$D(y;\hat{\mu})=2(\ell_n(\tilde{\theta})-\ell_n(\hat{\theta})).$$ Pour le modèle Binomial, on a que
\begin{align*}
\ell_n(\theta)&=\sum_{i=1}^n\frac{y_i\ln\left(\frac{\mu_i}{1-\mu_i}\right)+\ln(1-\mu_i)}{1/m_i}.
\end{align*} Alors, dans le modèle complet, $\mu_i=y_i$ et on trouve
\begin{align*}
\ell_n(\tilde{\theta})&=\sum_{i=1}^n\frac{y_i\ln\left(\frac{y_i}{1-y_i}\right)+\ln(1-y_i)}{1/m_i}.
\end{align*}
Dans le modèle développé avec le lien log, $\mu_i=\hat{\mu}_i$ et on trouve
\begin{align*}
\ell_n(\hat{\theta})&=\sum_{i=1}^n\frac{y_i\ln\left(\frac{\hat{\mu}_i}{1-\hat{\mu}_i}\right)+\ln(1-\hat{\mu}_i)}{1/m_i}.
\end{align*}
Finalement, la déviance est
\begin{align*}
D(y;\hat{\mu})&=\sum_{i=1}^n\frac{y_i\ln\left(\frac{y_i}{1-y_i}\right)+\ln(1-y_i)}{1/m_i}-\frac{y_i\ln\left(\frac{\hat{\mu}_i}{1-\hat{\mu}_i}\right)+\ln(1-\hat{\mu}_i)}{1/m_i}\\
&=\sum_{i=1}^n m_i\left[y_i\ln\left(\frac{y_i}{\hat{\mu}_i}\right)+(1-y_i)\ln\left(\frac{1-y_i}{1-\hat{\mu}_i}\right)\right].
\end{align*}

\item Pour la distribution Gamma, on a $V(t)=t^2$ et $b(t)=-\ln(-t)$.

Résidus de Pearson: $$r_{P_i}=\frac{y_i-\hat{\mu}_i}{\sqrt{\hat{\mu}_i^2}}=\frac{y_i-\hat{\mu}_i}{\hat{\mu}_i}.$$
Résidus d'Anscombe:
\begin{align*}
A(t)&=\int_{0}^{t} \frac{\mbox{d}s}{s^{2/3}}=3t^{1/3}\\
\dot{A}(t)&=\frac{1}{s^{2/3}}\\
r_{A_i}&=\frac{A(y_i)-A(\hat{\mu}_i)}{\dot{A}(\hat{\mu}_i)\sqrt{V(\hat{\mu}_i)}}=\frac{3(y_i^{1/3}-\hat{\mu}_i^{1/3})}{\hat{\mu}_i^{1/3}}.
\end{align*}
Résidus de déviance: 
\begin{align*}
D_i&=2\left(-\frac{y_i}{y_i}-\ln(y_i)+\frac{y_i}{\hat{\mu}_i}+\ln(\hat{\mu}_i)\right)\\
&=2\left(\ln\left(\frac{\hat{\mu}_i}{y_i}\right)+\frac{y_i-\hat{\mu}_i}{\hat{\mu}_i}\right)\\
r_{D_i}&=sign(y_i-\hat{\mu}_i)\sqrt{2\left(\ln\left(\frac{\hat{\mu}_i}{y_i}\right)+\frac{y_i-\hat{\mu}_i}{\hat{\mu}_i}\right)}.
\end{align*}

\item Cette solution est en anglais, vous pouvez poser vos questions sur le forum, s'il y a lieu.

This is a two-factor model, «Device» takes three levels (M1, M2 and M3) and «Stress» takes 4 levels. The baseline group is M1 device at stress level I. An analysis of deviance is carried out to assess if the parameters for the devices are significant.
\begin{small}
\begin{verbatim}
> glm4 <- glm(Failures~Level*Machine,family=poisson,data=stresstest)
> anova(glm4)
Analysis of Deviance Table

Model: poisson, link: log

Response: Failures

Terms added sequentially (first to last)

              Df Deviance Resid. Df Resid. Dev
NULL                             11     35.844
Level          3  20.8567         8     14.987
Machine        2  12.2154         6      2.772
Level:Machine  6   2.7719         0      0.000
> qchisq(0.95,6)
[1] 12.59159
> qchisq(0.95,2) 
[1] 5.991465
\end{verbatim}
\end{small}
 The model $$\mbox{Stress+Device+Stress.Device}$$ is fitted first. The change in deviance from the simpler model $\mbox{Stress+Device}$ is 2.7719 on 6 degrees of freedom, which is not significant when compared to $\chi^{2}_{(6,0.95)}=12.59$. Hence, the model $\mbox{Stress+Device}$ is an adequate simplification of the more complex model. If we then test for the significance of the Device parameters, we find that the change in deviance from the simpler model Stress is 12.2154 on 2 degrees of freedom, which is significant because $\chi^{2}_{(2,0.95)}=5.99$. From this analysis, we can conclude that there is a significant difference between the failure rates of the different devices. 


\bigskip
\item \textbf{Réclamations d'assurance automobile au Royaume-Uni.}

\begin{itemize}
\item[a)] En \texttt{R}, on obtient

\verbatiminput{Code/GammaGLMb.txt}

\bigskip
\item[b)] On a utilisé un lien inverse, alors $E[Y_i]=\frac{1}{\eta_i}.$ Puisque les variables explicatives prennent toutes leur niveau de base, on a que $\hat{\eta}_i=\hat{\beta}_0=0.0033233$ et $$\widehat{E[Y_i]}=0.0033233^{-1}=300.91.$$

\item[c)] Puisqu'on a utilisé un lien inverse, un coefficient plus élevé implique une diminution de l'espérance du coût de la réclamation, alors qu'un coefficient négatif signifie une augmentation de cette espérance. Ici on observe que les sept coefficients sont positifs, alors la catégorie d'âge ayant une espérance de coût la plus élevée est la catégorie de base, 17-20 ans. Le coût moyen semble ensuite relativement élevé pour les jeunes entre 21 et 29 ans. La catégorie d'âge avec coût de réclamation minimal est 35-39 ans, puis la moyenne semble relativement stable pour les détenteurs de police plus âgés.

\bigskip
\item[d)] Les trois coefficients pour la variable modèle sont négatifs, ce qui signifie que les réclamations pour les véhicules de type A (niveau de base) sont moins élevées en moyenne que celles pour les autres types de véhicule. Les réclamatios pour les véhicules du modèle D semblent particulièrement coûteuse car le coefficient est beaucoup plus grand en valeur absolue que les autres.

\item[e)] De la même façon, on observe que d'augmenter l'âge du véhicule diminue le coût moyen des réclamations.

\item[f)] Pour un détenteur de police entre 17 et 20 ans, avec un véhicule de type D âgé de un à 3 ans, on trouve que $$\widehat{E[Y_i]}=\frac{1}{\hat{\beta}_0+\hat{\beta}^{MODEL}_D}=\frac{1}{0.0033233-0.0018235}=666.76.$$

\item[g)] Pour un détenteur de police entre 35 et 39 ans, avec un véhicule de type A âgé de plus de 10 ans, on trouve que $$\widehat{E[Y_i]}=\frac{1}{\hat{\beta}_0+\hat{\beta}^{OWNERAGE}_{35-39}+\hat{\beta}^{CARAGE}_{10+}}=\frac{1}{0.0033233+0.0016372+0.0033776}=119.93.$$

\item[h)] La déviance $D(y,\hat{\mu})=11.511$ est donnée dans la sortie \texttt{R} pour la sous-question a). On a que $$\frac{D(y,\hat{\mu})}{\hat{\phi}}=\frac{11.511}{0.1074529}=107.126,$$ ce qui est très près de $n-p'=109$. Le modèle semble donc adéquat.

\item[i)] Les résidus sont calculés avec les formules trouvées à la question 6. Il faut d'abord enlever les données manquantes du vecteur contenant les coûts moyens. On obtient les graphiques de la Figure~\ref{residuals}.

\begin{figure}
\begin{center}
\includegraphics[width=7.5cm]{Figures/PearsonRes.pdf}
\includegraphics[width=7.5cm]{Figures/AnscombeRes.pdf}
\includegraphics[width=7.5cm]{Figures/DevianceRes.pdf}
\caption{Résidus pour GLM Gamma}\label{residuals}
\end{center}
\end{figure}

\item[j)] a. Le modèle avec le lien logarithmique est 

\verbatiminput{Code/GammaGLMi.txt}

\bigskip
b. Avec ce modèle $E[Y_i]=e^{\eta_i}.$ Puisque les variables explicatives prennent toutes leur niveau de base, on a que $\hat{\eta}_i=\hat{\beta}_0=5.711739$ et $$\widehat{E[Y_i]}=e^5.711739=302.39.$$ Cela ne diffère pas beaucoup du résultat trouvé en b).

\bigskip
c-d-e. Puisqu'on a utilisé un lien logarithmique, on a un modèle multiplicatif. Si $e^\beta>1$, alors l'espérance du coût augmente, alors que si $e^\beta<1$ alors l'espérance du coût diminue. On peut donc tirer des conclusions similaires à celles en c), d) et e).

\bigskip
f. Pour un détenteur de police entre 17 et 20 ans, avec un véhicule de type D âgé de un à 3 ans, on trouve que $$\widehat{E[Y_i]}=\exp{\hat{\beta}_0+\hat{\beta}^{MODEL}_D}=\exp{5.711739+0.472290}=484.94.$$ On note que cette valeur est beaucoup moins élevée que celle obtenue en f).

\bigskip
g. Pour un détenteur de police entre 35 et 39 ans, avec un véhicule de type A âgé de plus de 10 ans, on trouve que $$\widehat{E[Y_i]}=\exp{\hat{\beta}_0+\hat{\beta}^{OWNERAGE}_{35-39}+\hat{\beta}^{CARAGE}_{10+}}=\exp{5.711739-0.331420-0.735513}=104.04.$$

\bigskip
h. La déviance $D(y,\hat{\mu})=11.263$ est donnée dans la sortie \texttt{R} pour la sous-question a). On a que $$\frac{D(y,\hat{\mu})}{\hat{\phi}}=\frac{11.263}{0.0910768}=123.66,$$ ce qui est moins près de $n-p'=109$ que pour le modèle avec le lien inverse. Le modèle semble donc moins adéquat.
\end{itemize}

\bigskip
\item (a)  If $x_{i}$ is treated as a factor predictor with 11 levels, the linear predictor is written as $$\eta_{i}=\beta_{0}+\beta_{i} \mbox{,  } i=1,...,11$$ and $\beta_{1}=0$. The binomial density is the following: $$f_{Y}(y_{i})=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\pi_{i}^{y_{i}}(1-\pi_{i})^{m_{i}-y_{i}},$$ which can be rewritten in a exponential family representation as: $$f_{Y}(y_{i})=\exp\left[y_{i}\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)+m\ln(1-\pi_{i})+\ln\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\right].$$ Hence, the canonical parameter is $\theta_{i}=\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)$ and the canonical link is the logit link. Thus, 
\begin{align*}
\eta_{i}&=\ln\left(\frac{\pi_{i}}{1-\pi_{i}}\right)=\beta_{0}+\beta_{i} \\
\pi_{i}&=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}= \frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\\
\end{align*}
The expression of the density in the reparametrization is then 
\begin{align*}
f_{Y}(y_{i})&=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\left(\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\right)^{y_{i}}\left(\frac{1}{1+e^{\beta_{0}+\beta_{i}}}\right)^{m_{i}-y_{i}}\\
&=\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\frac{e^{y_{i}(\beta_{0}+\beta_{i})}}{(1+e^{\beta_{0}+\beta_{i}})^{m_{i}}}\\
\end{align*} The likelihood $L$ and the log-likelihood $l$ are shown below:
\begin{align*}
L(\beta_{0},...,\beta_{11};y_{1},...,y_{11})&=\prod_{i=1}^{11} \begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}\frac{e^{y_{i}(\beta_{0}+\beta_{i})}}{(1+e^{\beta_{0}+\beta_{i}})^{m_{i}}}\\
\ell(\beta_{0},...,\beta_{11};y_{1},...,y_{11})&=\sum_{i=1}^{11} \left[\ln\begin{pmatrix} m_{i} \\ y_{i} \end{pmatrix}+y_{i}(\beta_{0}+\beta_{i})-m_{i}\ln(1+e^{\beta_{0}+\beta_{i}})\right]\\
\end{align*} We have 
\begin{align*}
\frac{\partial \ell}{\partial \beta_{0}}&=\sum_{i=1}^{11} \left[y_{i}-m_{i}\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}}\right]\\
\frac{\partial \ell}{\partial \beta_{i}}&=y_{i}-m_{i}\frac{e^{\beta_{0}+\beta_{i}}}{1+e^{\beta_{0}+\beta_{i}}} \mbox{,  } i=2,...,11,\\
\end{align*} and $\beta_{1}=0$ by constraint of the model. The maximum likelihood estimators for the parameters are derived by solving the system of equations $\frac{\partial \ell}{\partial \beta_{i}}=0,$ $i=0,...,11$:
\begin{align*}
\sum_{i=1}^{11} \left[y_{i}-m_{i}\frac{e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}{1+e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}\right]&=0\\
y_{i}-m_{i}\frac{e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}{1+e^{\hat{\beta}_{0}+\hat{\beta}_{i}}}&=0 \mbox{,  } i=2,...,11,\\
\Rightarrow \hat{\beta}_{0}+\hat{\beta}_{i}=\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)& \mbox{,  } i=2,...,11,\\
\end{align*}  Using the first equation and replacing $\hat{\beta}_{0}+\hat{\beta}_{i}$ by $\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)$,
\begin{align*}
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}+\sum_{i=2}^{11} \left[y_{i}-m_{i}\frac{\left(\frac{y_{i}}{m_{i}-y_{i}}\right)}{1+\left(\frac{y_{i}}{m_{i}-y_{i}}\right)}\right]=0\\
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}+\sum_{i=2}^{11} \left[y_{i}-m_{i}\frac{y_{i}}{m_{i}}\right]=0\\
&y_{1}-m_{1}\frac{e^{\hat{\beta}_{0}}}{1+e^{\hat{\beta}_{0}}}=0\\
&\hat{\beta}_{0}=\ln\left(\frac{y_{1}}{m_{1}-y_{1}}\right)\\
&\hat{\beta}_{i}=\ln\left(\frac{y_{i}}{m_{i}-y_{i}}\right)-\hat{\beta}_{0}=\ln\left(\frac{y_{i}/(m_{i}-y_{i})}{y_{1}/(m_{1}-y_{1})}\right)  \mbox{,  } i=2,...,11.\\
\end{align*}
The estimates of the model parameters are easily found in \verb'R' as follows:
\begin{small}
\begin{verbatim}
> (beta0 <- log(y[1]/(m[1]-y[1])))
[1] -2.917771
> (beta <- c(0,log(y[-1]/(m[-1]-y[-1]))-beta0))
[1] 0.0000000 0.4122448 0.6151856 0.6740261 1.3083328 1.7137979 1.6184877
[8] 2.7636201 3.5239065 3.0178542 3.2542430
\end{verbatim}
\end{small}
Hence, here $$\hat{\beta}=(-2.9178, 0, 0.4122, 0.6152, 0.6740, 1.3083, 1.7138, 1.6185, 2.7636, 3.5239, 3.0179, 3.2542)^{T}.$$


As a consistency check following from the invariance property of maximum likelihood estimation, we can verify that the estimates of $\pi_{i}$ using the expit function are equal to the MLE estimates $\hat{\pi}_{i}=\frac{y_{i}}{m_{i}}$:
\begin{small}
\begin{verbatim}
> (pi <- exp(beta0+beta)/(1+exp(beta0+beta)))
 [1] 0.05128205 0.07547170 0.09090909 0.09589041 0.16666667 0.23076923
 [7] 0.21428571 0.46153846 0.64705882 0.52500000 0.58333333
> y/m
 [1] 0.05128205 0.07547170 0.09090909 0.09589041 0.16666667 0.23076923
 [7] 0.21428571 0.46153846 0.64705882 0.52500000 0.58333333
\end{verbatim}
\end{small}

\vspace{0.3cm}
(b) The Binomial GLM model with logit link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \verb'R' and the command:
\begin{center}
\begin{verbatim}
glm(cbind(y,m-y)~x,family=binomial). 
\end{verbatim}
\end{center}

The estimates of the parameters are $\hat{\beta}_{0}=-3.6070615$ and $\hat{\beta}_{1}=0.0009121$, with standard error $SE(\hat{\beta}_{0})=0.3533875$ and $SE(\hat{\beta}_{1})=0.0001084$.

\vspace{0.3cm}
(c) The Binomial GLM model with probit link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \verb'R' and the command:
\begin{center}
\begin{verbatim}
 glm(cbind(y,m-y)~x,family=binomial(link=probit)). 
\end{verbatim}
\end{center}

The estimates of the parameters are $\hat{\beta}_{0}=-2.080$ and $\hat{\beta}_{1}=5.230\times10^{-4}$, with standard error $SE(\hat{\beta}_{0})=0.1852$ and $SE(\hat{\beta}_{1})=5.973\times10^{-5}$.

\vspace{0.3cm}
(d) The Binomial GLM model with complementary log-log link and the linear predictor $\eta_{i}=\beta_{0}+\beta_{1}x_{i}$, $i=1,...,11$ is fitted to the data using \verb'R' and the command:
\begin{center}
\begin{verbatim}
glm(cbind(y,m-y)~x,family=binomial(link=cloglog)). 
\end{verbatim}
\end{center}

The estimates of the parameters are $\hat{\beta}_{0}=-3.360$ and $\hat{\beta}_{1}=7.480\times10^{-4}$, with standard error $SE(\hat{\beta}_{0})=0.3061$ and $SE(\hat{\beta}_{1})=8.622\times10^{-5}$.

\vspace{0.3cm}
(e) Predictions can be found using the inverse of the link function. For the model with canonical link (model from b), we find that $$\hat{y}_{2000}=\frac{e^{\hat{\beta}_{0}+2000\hat{\beta}_{1}}}{1+e^{\hat{\beta}_{0}+2000\hat{\beta}_{1}}}=0.1439472.$$ Alternatively, the command \verb'predict(modelb,data.frame(x=2000),type="response",se.fit=TRUE)' can be used to calculate the predictions and associated standard errors. The resulting predictions and standard errors are presented in Table~\ref{pred}. The probability of developing fissures after 2000 hours of operations is 14.39\% according to model b, and the standard deviation is 2.08\%. This prediciton is quite comparable with the complementary log-log model (d), for which the estimated probability of developing fissures is 14.36\%, with a standard error of 2.03\%. The precision is slightly better in this model than the two others due to a smaller variance. The estimated probability with Model c, using the probit link, is higher at 15.06\%, with standard error of 2.06\%. 
\begin{table}[htbp]
\begin{center}
\begin{tabular}{|l||c|c|c|}
  \hline
   & Model b (logit link) & Model c (probit link) & Model d (compl. log-log link) \\
  \hline
  \hline
	 $\hat{y}_{2000}$ &0.1439472 & 0.150615 & 0.1436447\\ \hline
	 $SE(\hat{y}_{2000})$ &0.02080256 & 0.02062154 & 0.02030803 \\ \hline
\end{tabular}
\caption{Predictions and Standard Errors for Predictions for the 3 Models} \label{pred}
\end{center}
\end{table}

\vspace{0.3cm}
(f) Figure~\ref{modfit} shows a plot of the data points along with the three fitted lines. It was obtained using the following code in \verb'R', where \verb'ilogit, iprobit' and \verb'icloglog' are the inverse of the corresponding link functions:
\begin{small}
\begin{verbatim}
plot(x,y/m,pch=19,xlab="Number of Hours of Operation (x)",
  ylab="Prob of Developing Fissures")
j <- seq(0,4800,1)
lines(j,ilogit(coef(modelb)[1]+coef(modelb)[2]*j),lwd=2)
lines(j,iprobit(coef(modelc)[1]+coef(modelc)[2]*j),lty=2,col=2,lwd=2)
lines(j,icloglog(coef(modeld)[1]+coef(modeld)[2]*j),lty=3,col=4,lwd=2)
legend("topleft",legend=c("Logit","Probit","Complementary log-log"),
lty=c(1,2,3),col=c(1,2,4),lwd=rep(2,3))
\end{verbatim}
\end{small}
It is easy to observe that the fit is better when the number of hours of operations is lower, it seems that the variance of the observations is increasing with the predictor. This is expected in a generalized linear model framework. The three fitted lines are slightly different. The probit link produces lower estimates in the tails and higher estimates in the middle of the range of the predictors. It seems like this model is less representative of the data than the others. The complementary log-log model (d) predicts higher probabilities of failures in the extremes of the range of the predictors. This seems to fit the data well, and recall that the variance of the predictions where also smaller than other models in this case, which is a desirable property. The line for the model with canonical link is between the two others. It could also be a reasonable model for the data.


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=11cm]{Figures/turbinesf.jpeg}
\end{center}
\caption{Logistic, Probit and Complementary Log-Log Model Fit} \label{modfit}
\end{figure}


\end{enumerate}

\end{document}