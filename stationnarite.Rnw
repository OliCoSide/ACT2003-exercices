\chapter[Stationnarité et modèles stochastiques de séries chronologiques]%
  [Stationnarité et modèles stochastiques]%
  {Stationnarité et modèles stochastiques de séries chronologiques}
\label{chap:stationnarite}

\Opensolutionfile{reponses}[reponses-stationnarite]
\Opensolutionfile{solutions}[solutions-stationnarite]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:stationnarite}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:stationnarite}}

\end{Filesave}


<<echo=FALSE>>=
options(width = 52)
@

\begin{exercice}
  Soit $\{Z_t\}$ une suite de variables aléatoires indépendantes
  distribuées selon une loi normale de moyenne 0 et de variance
  $\sigma^2$, et soit $a$, $b$ et $c$ des constantes. Déterminer
  lequel ou lesquels des processus ci-dessous sont stationnaires. Pour
  chaque processus stationnaire, calculer la moyenne et la fonction
  d'autocovariance.
  \begin{enumerate}
    \item $X_t = a + b Z_t + c Z_{t-2}$
    \item $X_t = Z_1 \cos(ct) + Z_2 \sin(ct)$
    \item $X_t = Z_t \cos(ct) + Z_{t-1} \sin(ct)$
    \item $X_t = a + b Z_0$
    \item $X_t = Z_t Z_{t-1}$
  \end{enumerate}
  \emph{Astuce}: $\cos(u+v) = \cos u \cos v - \sin u \sin v$ et
  $\sin(u+v) = \sin u \cos v + \cos u \sin v$.
  \begin{rep}
    Sont stationnaires: a), b), d), e).
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a
      \begin{align*}
        \esp{X_t}
        &= a \quad \mbox{est indépendant de }t \\
        \gamma_X(t, t + h)
        &= b^2 \gamma_Z(h) + bc \gamma_Z(h - 2) \\
        &\phantom{=}
        + bc \gamma_Z(h + 2) +  c^2 \gamma_Z(h) \\
        &=
        \begin{cases}
          (b^2 + c^2) \sigma^2, & h = 0 \\
          bc \sigma^2,          & h = \pm 2 \\
          0,                    & \text{ailleurs}
        \end{cases} \\
        &= \gamma_X(h).
      \end{align*}
      Le processus $X_t = a + b Z_t + c Z_{t-2}$ est donc
      stationnaire.
    \item On a
      \begin{align*}
        \esp{X_t}
        &= 0 \\
        \gamma_X(t, t + h) &= \cos (ct) \cos (ct + ch) \sigma^2 +
        \sin (ct) \sin (ct + ch) \sigma^2 \\
        &= \sigma^2
        [\cos^2 (ct) \cos (ch) - \cos (ct) \sin (ct) \sin (ch) \\
        &\phantom{=}
        + \sin^2 (ct) \cos (ch) + \cos (ct) \sin (ch) \sin (ct)] \\
        &= \cos (ch) \sigma^2 \\
        &= \gamma_X(h).
      \end{align*}
      Le processus $X_t = Z_1 \cos(ct) + Z_2 \sin(ct)$ est donc
      stationnaire.
    \item On a
      \begin{align*}
        \esp{X_t}
        &= 0 \\
        \gamma_X(t, t + h)
        &= \cos (ct) \cos (ct + ch) \gamma_Z(h) \\
        &\phantom{=}
        + \cos (ct) \sin (ct + ch) \gamma_Z(h - 1) \\
        &\phantom{=}
        + \sin (ct) \cos (ct + ch) \gamma_Z(h + 1) \\
        &\phantom{=}
        + \sin (ct) \sin (ct + ch) \gamma_Z(h) \\
        &=
        \begin{cases}
          \sigma^2,                          & h = 0 \\
          \cos (ct) \sin (ct + ch) \sigma^2, & h = 1 \\
          \sin (ct) \cos (ct + ch) \sigma^2, & h = -1 \\
          0, & \text{ailleurs}.
        \end{cases}
      \end{align*}
      Le processus $X_t = Z_t \cos(ct) + Z_{t-1} \sin(ct)$ n'est donc
      pas stationnaire.
    \item On a
      \begin{align*}
        \esp{X_t}
        &= a \\
        \gamma_X(t, t + h)
        &= b^2 \Var{Z_0} \\
        &= b^2 \sigma^2 \\
        &= \gamma_X(h).
      \end{align*}
      Le processus $X_t = a + b Z_0$ est donc stationnaire.
    \item On a
      \begin{align*}
        \esp{X_t}
        &= \gamma_Z(1) = 0 \\
        \gamma_X(t, t + h)
        &= \esp{X_t X_{t+h}} - \esp{X_t} \esp{X_{t+h}} \\
        &= \esp{Z_t Z_{t-1} Z_{t+h} Z_{t+h-1}} \\
        &=
        \begin{cases}
          \sigma^2 \sigma^2 = \sigma^4, & h = 0 \\
          0, & h = \pm 1 \\
          0, & h \neq 0
        \end{cases} \\
        &= \gamma_X(h).
      \end{align*}
      Le processus $X_t = Z_t Z_{t-1}$ est donc stationnaire.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\{X_t\}$ une série chronologique stationnaire de moyenne
  nulle et $a$, $b$, des constantes.
  \begin{enumerate}
  \item Si $Y_t = a + bt + s_t + X_t$, où $s_t$ est une composante de
    saisonnalité de période 12, démontrer que $\nabla \nabla_{12} Y_t =
    (1 - B)(1 - B^{12}) Y_t$ est stationnaire.
  \item Si $Y_t = (a + bt)s_t + X_t$, où $s_t$ est toujours une
    composante de saisonnalité de période 12, démontrer que
    $\nabla_{12}^2 Y_t = (1 - B^{12})^2\, Y_t$ est stationnaire.
  \end{enumerate}
  \begin{sol}
    On sait que $\nabla_k X_t = X_t - X_{t-k}$, $k = 1, 2, \dots$ et
    $\gamma_X(h) = \Cov(X_t, X_{t+h})$.
    \begin{enumerate}
    \item On a $s_t = s_{t-12}$. Ainsi, avec $Y_t = a + bt + s_t +
      X_t$,
      \begin{align*}
        \nabla_{12} Y_t
        &= Y_t - Y_{t - 12} \\
        &= 12b + X_t - X_{t-12} \\
        \intertext{et}
        \nabla \nabla_{12} Y_t
        &= X_t - X_{t-1} - X_{t-12} + X_{t-13} = W_t.
      \end{align*}
      Maintenant, il est clair que l'espérance du processus
      $\{W_t\}$ est indépendante de $t$, car celle du processus $\{X_t\}$ l'est.
      En outre,
      \begin{align*}
        \gamma_W(h)
        &= \Cov(X_t - X_{t-1} - X_{t-12} + X_{t-13}, \\
        & \quad\;
        X_{t+h} - X_{t+h-1} - X_{t+h-12} + X_{t+h-13}) \\
        &= 4 \gamma_X(h) - 2 \gamma_X(h-1) - 2 \gamma_X(h-12) \\
        & \quad\;
        + \gamma_X(h-13) - 2 \gamma_X(h+1) + \gamma_X(h-11) \\
        & \quad\; - 2\gamma_X(h+12) + \gamma_X(h+11) + \gamma_X(h+13),
      \end{align*}
      ce qui est clairement indépendant de $t$. Le processus $\{W_t\}$
      est donc stationnaire.
    \item Avec $Y_t = (a + bt)s_t + X_t$ et, encore une fois, $s_t = s_{t-12}$,
      \begin{align*}
        \nabla_{12} Y_t
        &= 12bs_t + X_t - X_{t-12} \\
        \intertext{et}
        \nabla_{12}^2 Y_t
        &= X_t - 2 X_{t-12} + X_{t-24} = W_t.
      \end{align*}
      Avec une démarche semblable à celle effectuée en a), on démontre
      alors que le processus $\{W_t\}$ est stationnaire.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\{X_t\}$ et $\{Y_t\}$ deux séries stationnaires et non
  corrélées, c'est-à-dire que $\Cov(X_r, Y_s) = 0$ pour tous $r$ et
  $s$. Démontrer que $\{X_t + Y_t\}$ est stationnaire avec fonction
  d'autocovariance égale à la somme des fonctions d'autocovariance de
  $\{X_t\}$ et $\{Y_t\}$.
  \begin{sol}
    Soit $\mu_X$ la moyenne et $\gamma_X(h)$ la FACV du processus
    stationnaire $\{X_t\}$, puis $\mu_Y$ et $\gamma_Y(h)$ les
    fonctions correspondantes pour le processus $\{Y_t\}$. Les deux
    processus sont indépendants. Par conséquent,
    \begin{align*}
      \esp{X_t + Y_t}
      &= \mu_X + \mu_Y \quad \mbox{est indépendant de }t, \\
      \intertext{et} \gamma_{X+Y}(t, t+h)
      &= \Cov(X_t + Y_t, X_{t+h} + Y_{t+h}) \\
      &= \Cov(X_t, X_{t+h}) + \Cov(Y_t, Y_{t+h}) \\
      &= \gamma_X(h) + \gamma_Y(h),
    \end{align*}
    ce qui complète la preuve.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:stationnarite:lake}
  Les données \texttt{lake.dat} donnent le niveau du Lac Huron moins
  570 pieds entre les années 1875 et 1972. L'ensemble contient donc 98
  données. Faire une modélisation préliminaire (à être complétée plus
  tard) de cette série en suivant les étapes suivantes.
  \begin{enumerate}[i)]
  \item Tracer le graphique de la série et identifier visuellement une
    tendance et/ou de la saisonnalité.
  \item Si nécessaire, estimer la tendance par régression et éliminer
    la saisonnalité à l'aide de différences.
  \item Proposer un modèle pour les résidus obtenus en ii). Justifier
    votre réponse à l'aide du corrélogramme des résidus et des
    résultats des tests de détection du bruit blanc.
  \end{enumerate}
<<echo=FALSE>>=
lake <- ts(scan("lake.dat", comment.char = "#"), start = 1875)
fit <- lm(lake ~ time(lake))
b <- round(coef(fit), 3)
res <- residuals(fit)
n <- length(res)
Q <- round(Box.test(res, lag = 40)$statistic, 2)
TP <- sum(max.col(embed(res, 3)) == 2, na.rm = TRUE) +
    sum(max.col(embed(-res, 3)) == 2, na.rm = TRUE)
@
  \begin{rep}
    $\hat{m}_t =
    \Sexpr{format(b[1], dec = ",")}
    \Sexpr{format(b[2], dec = ",")} t$,
    $Q = \Sexpr{format(Q, dec = ",")}$,
    $T = \Sexpr{TP}$.
  \end{rep}
  \begin{sol}
    La série \texttt{lake.dat} est représentée à la figure
    \ref{fig:stationnarite:lake1}.
    \begin{figure}[htbp]
      \begin{minipage}[t]{0.48\textwidth}
        \centering
<<echo=FALSE,fig=TRUE>>=
plot(lake)
abline(fit)
@
        \subcaption{Série originale avec la droite de régression \label{fig:stationnarite:lake1}}
      \end{minipage}
      \hfill
      \begin{minipage}[t]{0.48\textwidth}
        \centering
<<echo=FALSE,fig=TRUE>>=
plot(residuals(fit), type="l")
@
        \subcaption{Résidus de régression \label{fig:stationnarite:lake2}}
      \end{minipage}
      \newline
      \centering
      \begin{minipage}[t]{0.48\textwidth}
        \centering
<<echo=FALSE,fig=TRUE>>=
acf(residuals(fit), lag.max = 40)
@
        \subcaption{FAC des résidus de régression \label{fig:stationnarite:lake3}}
      \end{minipage}
      \caption{Graphiques reliés à la série \texttt{lake} (exercice
        \ref{chap:stationnarite}.\ref{ex:stationnarite:lake})}
    \end{figure}
    Il n'y a pas de composante de saisonnalité apparente dans cette
    série, mais il y a une tendance linéaire décroissante. On peut
    alors postuler le modèle suivant pour la série \texttt{lake}
    $\{Y_t\}$:
    \begin{displaymath}
      Y_t = m_t + X_t, \quad t = 1875, \dots, 1972,
    \end{displaymath}
    où $m_t = \beta_0 + \beta_1 t$ et le modèle des résidus $\{X_t\}$
    est à déterminer. La valeur de $\beta_0$ et $\beta_1$ est estimée
    par les moindres carrés en utilisant la fonction \texttt{lm} dans
    \textsf{R}:
<<echo=TRUE>>=
( fit <- lm(lake ~ time(lake)) )
@
    Cette droite de régression est incorporée au graphique de la série
    à la figure \ref{fig:stationnarite:lake1}. Le graphique de la
    série des résidus de la régression
    \begin{displaymath}
      \hat{X}_t = Y_t - \hat{\beta}_0 - \hat{\beta}_1 t
    \end{displaymath}
    se trouve à la figure \ref{fig:stationnarite:lake2}. La fonction
    d'autocorrélation empirique correspondante $\hat{\rho}(h)$ se
    trouve, quant à elle, à la figure \ref{fig:stationnarite:lake3}
    pour $h = 0, \dots, 40$.

    Puisque plus de $0,95(40) = 2$ valeurs excèdent les bornes de
    l'intervalle de confiance à 95~\%, il est clair que les résidus
    $\{\hat{X}_t\}$ ne proviennent pas d'un bruit blanc. Cette
    assertion est confirmée par les tests portmanteau et des
    changements de direction, qui tous les deux rejettent l'hypothèse
    d'un bruit blanc:
<<echo=TRUE>>=
res <- residuals(fit)
n <- length(res)
Box.test(res, lag = 40)
TP <- sum(max.col(embed(res, 3)) == 2, na.rm = TRUE) +
    sum(max.col(embed(-res, 3)) == 2, na.rm = TRUE)
abs((TP - 2 * (n - 2)/3)/sqrt((16 * n - 29)/90)) > 1.96
@
    La forme générale de la FAC empirique suggère que des modèles
    potentiels pour les résidus $\{\hat{X}_t\}$ seraient un AR$(1)$ ou
    un AR$(2)$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:stationnarite:ma2}
  Considérer le processus à moyenne mobile $\{X_t\}$ suivant:
  \begin{displaymath}
    X_t = Z_t + \theta Z_{t-2},
  \end{displaymath}
  où $\{Z_t\} \sim \text{WN}(0, 1)$.
  \begin{enumerate}
  \item Calculer les fonctions d'autocovariance et d'autocorrélation
    de ce processus.
  \item À l'aide de la fonction \texttt{arima.sim} de \textsf{R},
    simuler 300 observations du processus ci-dessus avec $\theta =
    0,8$. Calculer et tracer le corrélogramme du processus ainsi
    obtenu.
  \item Répéter la partie b) avec $\theta = -0,8$.
  \item Les corrélogrammes obtenus en b) et c) correspondent-ils à
    la fonction d'autocorrélation théorique calculée en a)?
  \item On remarquera que la série en b) fluctue moins rapidement que
    celle en c). Expliquer cet état de fait à l'aide de la fonction
    d'autocorrélation.
  \end{enumerate}
  \begin{rep}
    $\gamma_X(0) = 1 + \theta^2$, $\gamma_X(\pm 2) = \theta$,
    $\gamma_X(h) = 0$ ailleurs.
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item La FACV est donnée par
      \begin{align*}
        \gamma_X(h)
        &= \Cov(Z_t + \theta Z_{t - 2},
        Z_{t + h} + \theta Z_{t + h - 2}) \\
        &= \gamma_Z(h) + \theta \gamma_Z(h - 2) +
        \theta \gamma_Z(h + 2) + \theta^2 \gamma_Z(h) \\
        &= \begin{cases}
          1 + \theta^2, & h = 0 \\
          \theta,      & h = \pm 2 \\
          0,           & \mathrm{ailleurs}
        \end{cases}
      \end{align*}
      puisque $\{Z_t\} \sim \text{WN}(0, \sigma^2)$. La FAC est alors
      \begin{displaymath}
        \rho_X(h) =
        \begin{cases}
          1,                          & h = 0 \\
          \frac{\theta}{1 + \theta^2}, & h = \pm 2 \\
          0,                          & \mathrm{ailleurs}.
        \end{cases}
      \end{displaymath}
    \item Le modèle de la série $\{X_t\}$ est simplement un processus
      MA(2) avec $\theta_1 = 0$, un modèle simple à simuler avec la
      fonction \texttt{arima.sim}:
<<echo=TRUE, eval=TRUE>>=
x <- arima.sim(list(ma=c(0, 0.8)), n=300)
@
      La série simulée ainsi que sa FAC empirique se trouvent à la
      figure \ref{fig:stationnarite:ma2-1}. Afin d'améliorer la
      lisibilité du graphique, seules les 100 dernières valeurs de la
      série sont affichées.
      \begin{figure}
        \begin{minipage}{0.48\textwidth}
<<echo=FALSE, fig=TRUE>>=
plot(window(x, start = 200))
@
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
<<echo=FALSE, fig=TRUE>>=
acf(x)
@
        \end{minipage}
        \caption{Simulation (100 dernières valeurs) et FAC empirique
          de la série $\{X_t\}$ de l'exercice
          \ref{chap:stationnarite}.\ref{ex:stationnarite:ma2}~b)}
        \label{fig:stationnarite:ma2-1}
      \end{figure}
    \item La fonction \texttt{arima.sim} est maintenant appelée avec $\theta_2
      = -0.8$:
<<echo=TRUE, eval=TRUE>>=
x <- arima.sim(list(ma=c(0, -0.8)), n = 300)
@
      La série simulée ainsi que sa FAC empirique se retrouvent à la
      figure \ref{fig:stationnarite:prob2b}.
      \begin{figure}
        \begin{minipage}{0.48\textwidth}
<<echo=FALSE,fig=TRUE>>=
plot(window(x, start = 200))
@
        \end{minipage}
        \hfill
        \begin{minipage}{0.48\textwidth}
<<echo=FALSE,fig=TRUE>>=
acf(x)
@
        \end{minipage}
        \caption{Simulation (100 dernières valeurs) et FAC empirique
          de la série $\{X_t\}$ de l'exercice
          \ref{chap:stationnarite}.\ref{ex:stationnarite:ma2}~c)}
        \label{fig:stationnarite:prob2b}
      \end{figure}
    \item Oui, les corrélogrammes obtenus en b) et c) correspondent à
      la fonction d'autocorrélation théorique calculée en a).
    \item La série avec $\theta = 0.8$ fluctue moins rapidement
      puisque les observations distantes d'un pas de 2 sont corrélées
      positivement. Elles ont donc tendance à aller dans la même
      direction.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\{X_t\}$ un processus AR(1).
  \begin{enumerate}
  \item Calculer la variance de $(X_1 + X_2 + X_3 + X_4)/4$ quand $\phi
    = 0,9$ et $\sigma^2 = 1$.
  \item Répéter la partie a) avec $\phi = -0,9$ et comparer le
    résultat avec celui obtenu en a). Interpréter.
  \end{enumerate}
  \begin{rep}
    $\var{\frac{1}{4}(X_1 + X_2 + X_3 + X_4)} = \sigma^2(\frac{1}{4} +
    \frac{3}{8} \phi + \frac{1}{4} \phi^2 + \frac{1}{8} \phi^3)/(1 -
    \phi^2)$.
  \end{rep}
  \begin{sol}
    On a $X_t = \phi X_{t - 1} + Z_t$, $\{Z_t\} \sim \text{WN}(0,
    \sigma^2)$ et $\gamma_X(h) = \phi^h \gamma_X(0)$, $\gamma_X(0) =
    \sigma^2/(1 - \phi^2)$.
    \begin{enumerate}
    \item Des hypothèses ci-dessus,
      \begin{align*}
        \Var{\frac{X_1 + X_2 + X_3 + X_4}{4}}
        &= \frac{1}{16}(4 \Var{X_1} + 6 \gamma_X(1) + \\
        &\phantom{=}
        + 4 \gamma_X(2) + 2 \gamma_X(3)) \\
        &= \frac{\sigma^2}{1 - \phi^2} \left[\frac{1}{4} + \frac{3}{8}
          \phi + \frac{1}{4} \phi^2 + \frac{1}{8} \phi^3 \right].
      \end{align*}
      Si $\phi = 0.9$ et $\sigma^2 = 1$, alors la variance est égale à
      $4,638$.
    \item Maintenant, si $\phi = -0.9$, alors la variance est égale à
      $0,126$. L'explication de cette plus faible variance lorsque
      $\phi < 0$ est similaire à celle donnée en
      \ref{chap:stationnarite}.\ref{ex:stationnarite:ma2}~e).
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\{Z_t\}$ un bruit IID avec $Z_t \sim N(0, 1)$. On
  définit
  \begin{displaymath}
    X_t =
    \begin{cases}
      Z_t, & \text{si $t$ est pair,} \\
      \D \frac{Z_{t-1}^2 - 1}{\sqrt{2}}, & \text{si $t$ est impair.}
    \end{cases}
  \end{displaymath}
  Démontrer que $\{X_t\}$ est WN$(0, 1)$ mais non IID$(0, 1)$.
  \begin{sol}
    Tout d'abord, puisque $Z_t \sim N(0, 1)$, alors $Z_t^2 \sim
    \chi^2(1)$, d'où $\esp{Z_t^2} = 1$ et $\var{Z_t^2} = 2$.  On a
    donc
    \begin{align*}
      \esp{X_t}
      &=
      \begin{cases}
        \esp{Z_t}, & \text{$t$ pair} \\
        \Esp{\frac{Z_{t-1}^2 - 1}{\sqrt{2}}}, & \text{$t$ impair}
      \end{cases} \\
      &= 0
      \intertext{et}
      \var{X_t}
      &=
      \begin{cases}
        \var{Z_t}, & \text{$t$ pair} \\
        \Var{\frac{Z_{t-1}^2 - 1}{\sqrt{2}}}, & \text{$t$ impair}
      \end{cases} \\
      &= 1.
    \end{align*}
    De plus,
    \begin{align*}
      \gamma_X(1)
      &= \Cov\left(Z_t, \frac{Z_t^2 - 1}{\sqrt{2}} \right) \\
      &= \frac{1}{\sqrt{2}} \Cov(Z_t, Z_t^2) \\
      &= \frac{1}{\sqrt{2}} (\esp{Z_t^3} - \esp{Z_t} \esp{Z_t^2}) \\
      &= 0
    \end{align*}
    car tous les moments impairs d'une variable aléatoire normale sont
    nuls. Il est clair que $\gamma_X(h) = 0$ pour $|h| > 1$. Par
    conséquent,
    \begin{displaymath}
      \mu_X = 0 \quad \text{et} \quad
      \gamma_X(h) =
      \begin{cases}
        1, & h = 0 \\
        0, & h \neq 0.
      \end{cases}
    \end{displaymath}
    Ainsi, $\{X_t\}$ est WN$(0, 1)$.  Cependant, $X_t = Z_t$ et
    $X_{t+1} = (Z_t^2 - 1)/\sqrt{2}$ ne sont pas des variables
    aléatoires indépendantes, d'où $\{X_t\}$ n'est pas un bruit IID.
  \end{sol}
\end{exercice}

\begin{exercice}
  On vous donne les cinq valeurs suivantes d'un bruit blanc de moyenne
  0 et de variance 1:
  \begin{center}
    $0,18$\quad $-1,61$\quad $3,00$\quad $1,33$\quad $0,37$.
  \end{center}
  Calculer quatre valeurs des processus ci-dessous.
  \begin{enumerate}
  \item AR$(1)$ avec $\phi = 0,6$.
  \item MA$(1)$ avec $\theta = -0,4$.
  \item ARMA$(1, 1)$ avec $\phi = 0,6$ et $\theta = -0,4$.
  \end{enumerate}
<<echo=FALSE>>=
Z <- c(0.18, -1.61, 3, 1.33, 0.37)
X1 <- numeric(4)
X1[1] <- Z[1]
for (i in 2:4) X1[i] <- 0.6 * X1[i - 1] + Z[i]
X2 <- c(Z[1], Z[2:5] - 0.4 * Z[1:4])
X3 <- numeric(4)
X3[1] <- Z[1]
for (i in 2:4) X3[i] <- 0.6 * X3[i - 1] + Z[i] - 0.4 * Z[i - 1]
Z.f <- format(Z, digits = 3, dec = ",")
X1.f <- format(X1, digits = 4, dec = ",")
X2.f <- format(X2, digits = 4, dec = ",")
X3.f <- format(X3, digits = 4, dec = ",")
@
  \begin{rep}
    \begin{enumerate}
    \item $\{ \Sexpr{paste(X1.f, collapse = ", ")} \}$
    \item $\{ \Sexpr{paste(X2.f, collapse = ", ")} \}$
    \item $\{ \Sexpr{paste(X3.f, collapse = ", ")} \}$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    Cet exercice vise simplement à illustrer comment diverses
    combinaisons de valeurs d'un processus de bruit blanc peuvent
    générer des observations de processus ARMA.
    \begin{enumerate}
    \item On peut calculer les valeurs de $\{X_t\}$ à partir de la
      définition $X_t = 0,6 X_{t - 1} + Z_t$ avec $X_1 = Z_1$. On a donc
      \begin{align*}
        X_1 &= Z_1 = \Sexpr{X1.f[1]} \\
        X_2 &= 0,6 X_1 + Z_2 = \Sexpr{X1.f[2]} \\
        X_3 &= 0,6 X_2 + Z_3 = \Sexpr{X1.f[3]} \\
        X_4 &= 0,6 X_3 + Z_4 = \Sexpr{X1.f[4]}.
      \end{align*}
      De manière équivalente, la solution de l'équation
      caractéristique d'un processus AR$(1)$ est $X_t = \sum_{j =
        0}^\infty \phi^j Z_{t - j}$. On a donc
      \begin{align*}
        X_1 &= Z_1 = \Sexpr{X1.f[1]} \\
        X_2 &= Z_2 + 0,6 Z_1 = \Sexpr{X1.f[2]} \\
        X_3 &= Z_3 + 0,6 Z_2 + 0,36 Z_1 = \Sexpr{X1.f[3]} \\
        X_4 &= Z_4 + 0,6 Z_3 + 0,36 Z_2 + 0,216 Z_3 = \Sexpr{X1.f[3]}.
      \end{align*}
    \item On a $X_t = Z_t - 0,4 Z_{t - 1}$, donc
      \begin{align*}
        X_1 &= Z_1 = \Sexpr{X2.f[1]} \\
        X_2 &= Z_2 - 0,4 Z_1 = \Sexpr{X2.f[2]} \\
        X_3 &= Z_3 - 0,4 Z_2 = \Sexpr{X2.f[3]} \\
        X_4 &= Z_4 - 0,4 Z_3 = \Sexpr{X2.f[4]}.
      \end{align*}
    \item Ici, $X_t = 0,6 X_{t - 1} + Z_t - 0,4 Z_{t - 1}$. Par
      conséquent,
      \begin{align*}
        X_1 &= Z_1 = \Sexpr{X3.f[1]} \\
        X_2 &= 0,6 X_1 + Z_2 - 0,4 Z_1 = \Sexpr{X3.f[2]} \\
        X_3 &= 0,6 X_2 + Z_3 - 0,4 Z_2 = \Sexpr{X3.f[3]} \\
        X_4 &= 0,6 X_3 + Z_4 - 0,4 Z_3 = \Sexpr{X3.f[4]}.
      \end{align*}
      On peut également démontrer que la réprésentation MA$(\infty)$
      d'un processus ARMA$(1, 1)$ est $X_t = Z_t + (\phi + \theta) \sum_{j =
        1}^\infty \phi^{j - 1} Z_{t - j}$. On a donc, de manière
      équivalente,
      \begin{align*}
        X_1 &= Z_1 = \Sexpr{X3.f[1]} \\
        X_2 &= Z_2 + 0,2 Z_1 = \Sexpr{X3.f[2]} \\
        X_3 &= Z_3 + 0,2 (Z_2 + 0,6 Z_1) = \Sexpr{X3.f[3]} \\
        X_4 &= Z_4 + 0,2 (Z_3 + 0,6 Z_2 + 0,36 Z_3) = \Sexpr{X3.f[4]}.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}
\enlargethispage{10mm}

\begin{exercice}
  \begin{enumerate}
  \item Vérifier que le processus stationnaire (causal)
    \begin{displaymath}
      X_t = \sum_{j=0}^\infty \phi^j Z_{t-j} = Z_t + \phi Z_{t-1} +
      \phi^2 Z_{t-2} + \dots,
    \end{displaymath}
    où $|\phi| < 1$, est bien une solution de l'équation $X_t - \phi
    X_{t-1} = Z_t$ définissant le processus AR(1).
  \item Vérifier que le processus
    \begin{displaymath}
      X_t = - \sum_{j=1}^\infty \phi^{-j} Z_{t+j} =
      - \phi^{-1} Z_{t+1} - \phi^{-2} Z_{t+2} - \phi^{-3} Z_{t+3} -
      \dots,
  \end{displaymath}
  où $|\phi| > 1$ est aussi une solution de l'équation ci-dessus, mais
  que cette solution n'est pas causale.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item On a $X_t = \sum_{j=0}^\infty \phi^j Z_{t-j}$, d'où
      \begin{align*}
        X_t - \phi X_{t-1}
        &= \sum_{j=0}^\infty \phi^j Z_{t-j} -
        \phi \sum_{j=0}^\infty \phi^j Z_{t-1-j} \\
        &= \sum_{j=0}^\infty \phi^j Z_{t-j} -
        \sum_{j=0}^\infty \phi^{j+1} Z_{t-1-j} \\
        &= \sum_{j=0}^\infty \phi^j Z_{t-j} -
        \sum_{j=1}^\infty \phi^j Z_{t-j} \\
        &= Z_t.
      \end{align*}
    \item On a cette fois $X_t = - \sum_{j=1}^\infty \phi^{-j} Z_{t+j}$, d'où
      \begin{align*}
        X_t - \phi X_{t-1}
        &= - \sum_{j=1}^\infty \phi^{-j} Z_{t+j} +
        \phi \sum_{j=1}^\infty \phi^{-j} Z_{t-1+j} \\
        &= - \sum_{j=1}^\infty \phi^{-j} Z_{t-j} +
        \sum_{j=1}^\infty \phi^{-j+1} Z_{t-1+j} \\
        &= - \sum_{j=1}^\infty \phi^{-j} Z_{t-j} +
        \sum_{j=0}^\infty \phi^{-j} Z_{t-j} \\
        &= Z_t.
      \end{align*}
      La solution n'est évidemment pas causale puisque les valeurs du
      processus $\{X_t\}$ sont déterminées par des valeurs futures du
      processus $\{Z_t\}$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Pour chaque processus ARMA ci-dessous, déterminer si le processus
  est stationnaire et s'il est réversible. (Dans chaque cas $\{Z_t\}$
  est un bruit blanc.)
  \begin{enumerate}
  \item $X_t + 0,2 X_{t-1} - 0,48 X_{t-2} = Z_t$.
  \item $X_t + 1,9 X_{t-1} - 0,88 X_{t-2} = Z_t + 0,2 Z_{t-1} + 0,7 Z_{t-2}$.
  \item $X_t + 0,6 X_{t-1} = Z_t + 1,2 Z_{t-1}$.
  \item $X_t + 1,8 X_{t-1} - 0,81 X_{t-2} = Z_t$.
  \item $X_t + 1,6 X_{t-1} = Z_t - 0,4 Z_{t-1} + 0,04 Z_{t-2}$.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item Stationnaire et réversible
    \item Réversible seulement
    \item Stationnaire seulement
    \item Réversible seulement
    \item Réversible seulement
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Dans ce qui suit, $z_k$, $k = 1, 2$ représentent les racines du
    polynôme en $z$.
    \begin{enumerate}
    \item Processus AR(2): $X_t + 0.2 X_{t-1} - 0.48 X_{t-2} = Z_t
      \Rightarrow \phi(z) = 1 + 0.2z - 0.48z^2 \Rightarrow |z_1| =
      |5/3| > 1$ et $|z_2| = |-1.25| > 1$. De plus, $\theta(z)
      \equiv 1$. Le processus $\{X_t\}$ est donc stationnaire et réversible.
    \item Processus $\text{ARMA}(2, 2)$: $X_t + 1.9 X_{t-1} - 0.88
      X_{t-2} = Z_t + 0.2 Z_{t-1} + 0.7 Z_{t-2} \Rightarrow \phi(z) =
      1 + 1.9z - 0.88z^2 \Rightarrow |z_1| = |2.5967| > 1$ et $|z_2| =
      |-0.4376| < 1$.  De plus, $\theta(z) = 1 + 0.2z + 0.7z^2
      \Rightarrow z_{1,2} = -0.1429 \pm 1.1867 i \Rightarrow |z_{1,2}|
      = \sqrt{(-0.1429)^2 + (1.1867)^2} > 1$. Le processus $\{X_t\}$
      n'est donc pas stationnaire, mais il est réversible.
    \item Processus $\text{ARMA}(1, 1)$: $X_t + 0.6 X_{t-1} = Z_t +
      1.2 Z_{t-1} \Rightarrow \phi(z) = 1 + 0.6z \Rightarrow |\phi| =
      |-0.6| < 1$. De plus, $\theta(z) = 1 + 1.2z \Rightarrow |\theta|
      = |1.2| > 1$. Le processus $\{X_t\}$ est alors stationnaire,
      mais non réversible.
    \item Processus AR(2): $X_t + 1.8 X_{t-1} - 0.81 X_{t-2} = Z_t
      \Rightarrow \phi(z) = 1 + 1.8z - 0.81z^2 \Rightarrow |z_1| =
      |-0.4602| < 1$ et $|z_2| = |2.6825| > 1$. De plus, $\theta(z)
      \equiv 1$. Le processus $\{X_t\}$ n'est donc pas stationnaire,
      mais il est réversible.
    \item Processus $\text{ARMA}(1, 2)$: $X_t + 1.6 X_{t-1} = Z_t -
      0.4 Z_{t-1} + 0.04 Z_{t-2} \Rightarrow \phi(z) = 1 + 1.6z
      \Rightarrow |\phi| = |-1.6| > 1$.  De plus, $\theta(z) = 1 -
      0.4z + 0.04z^2 \Rightarrow |z_1| = |z_2| = |5| > 1$. Le
      processus $\{X_t\}$ n'est donc pas stationnaire, mais il est
      réversible.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\{Y_t\}$ la somme d'un processus AR(1) et d'un bruit
  blanc, c'est-à-dire
  \begin{displaymath}
    Y_t = X_t + W_t,
  \end{displaymath}
  où $\{W_t\} \sim \text{WN}(0, \sigma_W^2)$ et $\{X_t\}$ est le
  processus AR(1) avec $|\phi| < 1$
  \begin{displaymath}
    X_t - \phi X_{t-1} = Z_t, \quad
    \{Z_t\} \sim \text{WN}(0, \sigma_Z^2).
  \end{displaymath}
  On suppose de plus que $\Esp{W_s Z_t} = 0$ pour tous $s$ et $t$.
  \begin{enumerate}
  \item Démontrer que $\{Y_t\}$ est stationnaire et calculer sa fonction
    d'autocovariance.
  \item Démontrer que la série chronologique $U_t \equiv Y_t - \phi
    Y_{t-1}$ est 1--corrélée (c'est-à-dire que $\gamma_U(h) = 0$ pour
    tout $|h| > 1$) et que, par conséquent, elle peut s'écrire comme
    un processus MA(1).
  \item Conclure de b) que $\{Y_t\}$ est un processus ARMA$(1, 1)$ et
    exprimer les trois paramètres de ce modèle en fonction de $\phi$,
    $\sigma_W^2$ et $\sigma_Z^2$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Pour démontrer que la série $\{Y_t\}$ est stationnaire, on
      doit connaître la covariance entre $X_t$ et $W_s$ pour tout $t$
      et $s$. Comme $\{X_t\}$ est un processus AR(1) stationnaire, on
      peut l'écrire sous la forme $X_t = \sum_{j=0}^\infty \phi^j
      Z_{t-j}$. On constate donc que
      \begin{displaymath}
        \Cov(X_t, W_s) = \sum_{j=0}^\infty \phi^j \Cov(Z_{t-j}, W_s) = 0,
      \end{displaymath}
      car $\Cov(Z_t, W_s) = \esp{Z_t W_s} = 0$. Par conséquent,
      $\esp{X_t} = \esp{Y_t + W_t} = 0$ et
      \begin{align*}
        \gamma_Y(h)
        &= \Cov(X_{t+h} + W_{t+h}, X_t + W_t) \\
        &= \gamma_X(h) + \gamma_W(h) \\
        &= \begin{cases} \frac{\displaystyle \sigma_Z^2}{\displaystyle
            1 - \phi^2}
          + \sigma_W^2, & h = 0 \\
          \frac{\displaystyle \phi^{|h|}}{\displaystyle 1 - \phi^2}
          \sigma_Z^2, & h \neq 0,
        \end{cases}
      \end{align*}
      car $\{X_t\} \sim \text{AR}(1)$ et $\{W_t\} \sim \text{WN}(0,
      \sigma_W^2)$. Puisque les fonctions $\esp{Y_t}$ et $\gamma_Y(h)$
      ne dépendent pas du temps, on conclut que le processus
      $\{Y_t\}$ est stationnaire.
    \item On a $U_t = Y_t - \phi Y_{t-1}$, d'où
      \begin{align*}
        \gamma_U(h)
        &= \Cov(Y_t - \phi Y_{t-1}, Y_{t+h} - \phi Y_{t+h-1}) \\
        &= (1 + \phi^2) \gamma_Y(h) - \phi \gamma_Y(h-1) - \phi
        \gamma_Y(h+1) \\
        &= \begin{cases}
          (1 + \phi^2) \sigma_W^2 + \sigma_Z^2, & h = 0 \\
          - \phi \sigma_W^2,                   & h = \pm 1 \\
          0, & |h| > 1.
        \end{cases}
      \end{align*}
      Ainsi, $\{U_t\}$ est 1--corrélée et, en raison de la
      correspondance biunivoque entre les modèles ARMA et leur FACV,
      il s'agit d'un processus MA(1) avec paramètres $\theta$ et
      $\sigma^2$.
    \item Si $\{Y_t - \phi Y_{t-1}\} \sim \text{MA}(1)$, alors
      $\{Y_t\} \sim \text{ARMA}(1, 1)$, car la série $\{Y_t\}$ peut
      être exprimée comme étant la solution de $Y_t - \phi Y_{t-1} =
      V_t + \theta V_{t-1}$, où $\{V_t\} \sim \text{WN}(0, \sigma^2)$.
      Les paramètres de ce modèle ARMA$(1, 1)$ sont $\phi$, $\theta$
      et $\sigma^2$, où $\theta$ et $\sigma^2$ sont les solutions du
      système d'équations non linéaires
      \begin{align*}
        (1 + \phi^2) \sigma_W^2 + \sigma_Z^2,
        &= (1 + \theta^2) \sigma^2 \\
        - \phi \sigma_W^2
        &= \theta \sigma^2.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Les équations de Yule--Walker généralisées sont obtenues en
    multipliant
    \begin{displaymath}
      X_t - \phi_1 X_{t-1} - \dots - \phi_p X_{t-p} =
      Z_t + \theta_1 Z_{t-1} + \dots + \theta_q Z_{t-q}
    \end{displaymath}
    de part et d'autre par $X_{t-h}$ et en prenant par la suite
    l'espérance. Démontrer que les équations ainsi obtenues sont les
    suivantes: pour $0 \leq h \leq q$
    \begin{align*}
      \gamma_X(h) - \phi_1 \gamma_X(h-1) - \dots - \phi_p \gamma_X(h -
      p)
      &= \sigma^2 \sum_{j=0}^\infty \psi_j \theta_{h+j}, \\
      \intertext{et, pour $h > q$,}
      \gamma_X(h) - \phi_1 \gamma_X(h-1) - \dots - \phi_p \gamma_X(h - p)
      &= 0.
    \end{align*}
  \item Utiliser les équations de Yule--Walker généralisées ci-dessus
    pour calculer la fonction d'autocovariance d'un modèle ARMA$(1,
    1)$.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item En multipliant l'équation caractéristique d'un processus
      ARMA$(p, q)$ de part et d'autre par $X_{t-h}$, puis en prenant
      l'espérance, on obtient
      \begin{displaymath}
        \sum_{k=0}^p \phi_k \esp{X_{t-k} X_{t-h}} =
        \sum_{k=0}^p \theta_k \esp{Z_{t-k} X_{t-h}},
      \end{displaymath}
      avec $\phi_0 = \theta_0 = 1$. Or, $\esp{X_{t-k} X_{t-h}} =
      \gamma_X(h-k)$ et, en utilisant l'identité $X_{t-h} =
      \sum_{j=0}^\infty \psi_j Z_{t-h-j}$,
      \begin{align*}
        \esp{Z_{t-k} X_{t-h}}
        &= \sum_{j=0}^\infty \psi_j \esp{Z_{t-k} Z_{t-h-j}} \\
        &= \sum_{j=0}^\infty \psi_j \delta_{k, h+j} \sigma^2.
      \end{align*}
      On a donc
      \begin{align*}
        \sum_{k=0}^p \phi_k \gamma_X(h - k) &= \sum_{k=0}^q
        \sum_{j=0}^\infty \theta_k \psi_j \delta_{k, h+j}
        \sigma^2 \\
        &= \sigma^2 \sum_{j=0}^\infty \psi_j \theta_{h+j}
      \end{align*}
      pour $h = 0, \dots, q$. Lorsque $h > q$, le côté droit de
      l'équation est égal à $0$.
    \item Pour le processus ARMA$(1, 1)$, on a, pour $h = 0$ et $h =
      1$,
      \begin{align*}
        \gamma_X(0) - \phi \gamma_X(1) &= \sigma^2(1 + \psi_1 \theta) \\
        \gamma_X(1) - \phi \gamma_X(0) &= \sigma^2 \theta.
      \end{align*}
      Or, on sait que, pour le processus ARMA$(1, 1)$, $\psi_1 = \phi
      + \theta$. La solution de ce système d'équations est, par
      conséquent,
      \begin{align*}
        \gamma_X(0)
        &= \sigma^2
        \left(
          \frac{1 + \psi_1 \theta + \phi \theta}{1 - \phi^2}
        \right) \\
        &= \sigma^2
        \left(
          1 + \frac{(\phi + \theta)^2}{1 - \phi^2}
        \right) \\
        \intertext{et}
        \gamma_X(1)
        &= \sigma^2 \theta + \phi \gamma_X(0) \\
        &= \sigma^2
        \left(
          \phi + \theta + \frac{(\phi + \theta)^2 \phi}{1 - \phi^2}
        \right).
      \end{align*}
      Pour $h > 1$, on a
      \begin{align*}
        \gamma_X(2)
        &= \phi \gamma_X(1) \\
        \gamma_X(3)
        &= \phi \gamma_X(2) \\
        &= \phi^2 \gamma_X(1) \\
        \intertext{soit, de manière générale,}
        \gamma_X(h)
        &= \phi^{h - 1} \gamma_X(1), \quad h > 1.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

% \begin{exercice}
%   Inverser le processus ARIMA$(0,1,1)$ et démontrer que
%   \begin{displaymath}
%     X_t = \alpha \sum_{j=1}^\infty (1-\alpha)^{j-1} X_{t-j} + Z_t,
%   \end{displaymath}
%   où $\alpha = 1 + \theta$. Si l'on note le premier terme du côté
%   droit de l'équation ci-dessus $\tilde{X}_{t-1}$, on a
%   \begin{displaymath}
%     X_t = \tilde{X}_{t-1} + Z_t
%   \end{displaymath}
%   et donc le processus ARIMA$(0, 1, 1)$ se comporte à peu de choses
%   près comme une marche aléatoire, à savoir que l'observation au temps
%   $t$ du processus est égale à la moyenne pondérée des observations
%   précédentes ($\tilde{X}_{t-1}$) à laquelle on ajoute un «choc»
%   aléatoire ($Z_t$).
% \end{exercice}

\begin{exercice}
  Pour chaque modèle ci-dessous:
  \begin{enumerate}[i)]
  \item classer le modèle parmi la famille des processus
    $\text{ARIMA}(p, d, q)$;
  \item calculer les quatre premiers coefficients $\psi_0$, $\psi_1$,
    $\psi_2$ et $\psi_3$ de la représentation $\text{MA}(\infty)$ de
    $\{X_t\}$;
  \item calculer les quatre premiers coefficients $\pi_0$, $\pi_1$,
    $\pi_2$ et $\pi_3$ de la représentation $\text{AR}(\infty)$ de
    $\{Z_t\}$.
  \end{enumerate}
  Dans tous les cas, $\{Z_t\}$ est un bruit blanc.
  \begin{enumerate}
  \item $X_t - 0,5 X_{t-1} = Z_t$
  \item $X_t = Z_t - 1,3 Z_{t-1} + 0,4 Z_{t-2}$
  \item $X_t - 0,5 X_{t-1} = Z_t - 1,3 Z_{t-1} + 0,4 Z_{t-2}$
  \item $X_t - 1,2 X_{t-1} + 0,2 X_{t-2} = Z_t - 0,5 Z_{t-1}$
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item AR$(1)$
    \item MA$(2)$
    \item ARMA$(1, 2)$
    \item ARIMA$(1, 1, 1)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il s'agit d'un processus ARMA$(1, 0)$, ou plus simplement
      AR$(1)$. Les coefficients $\psi_1$, $\psi_2$ et $\psi_3$
      satisfont l'égalité
      \begin{displaymath}
        (1 + \psi_1 z + \psi_2 z^2 + \psi_3 z^3 + \dots)(1 - 0,5 z) = 1,
      \end{displaymath}
      soit
      \begin{align*}
        \psi_1 &= 0,5 \\
        \psi_2 &= 0,5 \psi_1 = 0,25 \\
        \psi_3 &= 0,5 \psi_2 = 0,125.
      \end{align*}
      On confirme cette réponse avec la fonction \texttt{ARMAtoMA} de
      \textsf{R}:
<<echo=TRUE>>=
ARMAtoMA(ar = 0.5, lag.max = 3)
@
      Puisque le processus est déjà inversé, on sait que les
      coefficients de la représentation AR$(\infty)$ sont simplement
      $\pi_1 = -0,5$ et $\pi_j = 0$, $j > 1$.
    \item On a un processus MA$(2)$ avec $\psi_1 = -1,3$, $\psi_2 =
      0,4$ et $\psi_j = 0$, $j > 2$. Les trois premiers coefficients
      de la représentation AR$(\infty)$ satisfont l'équation
      \begin{displaymath}
        (1 + \pi_1 z + \pi_2 z^2 + \pi_3 z^3 + \dots)%
        (1 - 1,3 z + 0,4 z^2) = 1,
      \end{displaymath}
      soit
      \begin{align*}
        \pi_1 &= 1,3 \\
        \pi_2 &= -0,4 + 1,3 \pi_1 = 1,29 \\
        \pi_3 &= -0,4 \pi_1 + 1,3 \pi_2 = 1,157.
      \end{align*}
      On peut calculer les coefficients $\pi_j$ avec la fonction
      \texttt{ARMAtoMA} dans \textsf{R} en inversant simplement le
      rôle des coefficients $\phi_j$ et $\theta_j$ (ainsi que leur
      signe). En d'autres mots, trouver les coefficients $\pi_j$ du
      processus $X_t = Z_t - 1,3 Z_{t-1} + 0,4 Z_{t-2}$ est en tous
      points équivalent à trouver les coefficients $\psi_j$ du
      processus $X_t - 1,3 X_{t-1} + 0,4 X_{t-2} = Z_t$. On a donc
<<echo=TRUE>>=
ARMAtoMA(ar = c(1.3, -0.4), lag.max = 3)
@
    \item On a un processus ARMA$(1, 2)$. Les trois premiers coefficients
      $\psi_j$ sont obtenus en égalant les coefficients des puissances
      de $z$ dans
      \begin{displaymath}
        (1 + \psi_1 z + \psi_2 z^2 + \psi_3 z^3 + \dots)%
        (1 - 0,5 z) = 1 - 1,3 z + 0,4 z^2.
      \end{displaymath}
      On obtient
      \begin{align*}
        \psi_1 &= 0,5 - 1,3 = -0,8 \\
        \psi_2 &= 0,5 \psi_1 + 0,4 = 0 \\
        \psi_3 &= 0,5 \psi_2 = 0.
      \end{align*}
      Confirmation avec \textsf{R}:
<<echo=TRUE>>=
 ARMAtoMA(ar = 0.5, ma = c(-1.3, 0.4), lag.max = 3)
@
      Les trois premiers coefficients $\pi_j$ sont obtenus à partir de
      l'équation
      \begin{displaymath}
        (1 + \pi_1 z + \pi_2 z^2 + \pi_3 z^3 + \dots)%
        (1 - 1,3 z + 0,4 z^2) = 1 - 0,5 z,
      \end{displaymath}
      soit
      \begin{align*}
        \pi_1 &= 1,3 - 0,5 = 0,8 \\
        \pi_2 &= -0,4 + 1,3 \pi_1 = 0,64 \\
        \pi_3 &= -0,4 \pi_1 + 1,3 \pi_2 = 0,512.
      \end{align*}
      En effet,
<<echo=TRUE>>=
ARMAtoMA(ar = c(1.3, -0.4), ma = -0.5, lag.max = 3)
@
    \item On a en fait $(1 - 0,2B)(1 - B) X_t = Z_t - 0,5 Z_{t-1}$,
      soit un processus ARIMA$(1, 1, 1)$. Les trois premiers
      coefficients $\psi_j$ sont obtenus à partir de l'équation
      \begin{displaymath}
        (1 + \psi_1 z + \psi_2 z^2 + \psi_3 z^3 + \dots)%
        (1 - 1,2 z + 0,2 z^2) = 1 - 0,5 z,
      \end{displaymath}
      d'où
      \begin{align*}
        \psi_1 &= 1,2 - 0,5 = 0,7 \\
        \psi_2 &= 1,2 \psi_1 - 0,2 = 0,640 \\
        \psi_3 &= 1,2 \psi_2 - 0,2 \psi_1 = 0,628.
      \end{align*}
      On obtient le même résultat avec \texttt{ARMAtoMA}:
<<echo=TRUE>>=
ARMAtoMA(ar = c(1.2, -0.2), ma = -0.5, lag.max = 3)
@
      Pour les trois premiers coefficients $\pi_j$, on résout
      \begin{displaymath}
        (1 + \pi_1 z + \pi_2 z^2 + \pi_3 z^3 + \dots)%
        (1 - 0,5 z) = 1 - 1,2 z + 0,2 z^2,
      \end{displaymath}
      ce qui donne
      \begin{align*}
        \pi_1 &= 0,5 - 1,2 = -0,7 \\
        \pi_2 &= 0,5 \pi_1 + 0,2 = -0,15 \\
        \pi_3 &= 0,5 \pi_2 = -0,075.
      \end{align*}
      En effet:
<<echo=TRUE>>=
ARMAtoMA(ar = 0.5, ma = c(-1.2, 0.2), lag.max = 3)
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que la valeur à $h = 2$ de la fonction d'autocorrélation
  partielle d'un modèle MA(1)
  \begin{displaymath}
    X_t = Z_1 + \theta Z_{t-1}, \quad \{Z_t\} \sim \text{WN}(0, \sigma^2),
  \end{displaymath}
  est
  \begin{displaymath}
    \phi_{22} = - \frac{\theta^2}{1 + \theta^2 + \theta^4}.
  \end{displaymath}
  \begin{sol}
    L'autocorrélation partielle de pas 2, $\phi_{22}$, est donnée par
    \begin{align*}
      \phi_{21} + \phi_{22} \rho(1) &= \rho(1) \\
      \phi_{21} \rho(1) + \phi_{22} &= \rho(2).
    \end{align*}
    Or, pour un processus MA$(1)$, $\rho(1) = \theta/(1 + \theta^2)$
    et $\rho(2) = 0$. On a donc
    \begin{align*}
      \phi_{22}
      &= - \frac{\rho(1)^2}{1 - \rho(1)^2} \\
      &= - \frac{\theta^2}{1 + \theta^2 + \theta^4}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On souhaite ajuster le modèle de régression suivant à un ensemble de
  $n$ données:
  \begin{displaymath}
    Y_t = \beta_0 + \beta_1 X_{t1} + \beta_2 X_{t2} + \varepsilon_t,
  \end{displaymath}
  où $\{\varepsilon_t\} \sim \text{AR}(1)$. Suite à de longues
  incantations proférées dans une langue gutturale proche de celle du
  Mordor, on a appris que les paramètres du processus AR$(1)$ sont
  $\phi = 0,8$ et $\sigma^2 = 9$.
  \begin{enumerate}
  \item Expliquer brièvement pourquoi l'emploi des moindres carrés
    généralisés s'avère approprié pour l'estimation des paramètres de la
    régression $\beta_0$, $\beta_1$ et $\beta_2$.
  \item Préciser la forme de la matrice $\mat{V} =
    \varmat{\vepsb}$ à utiliser dans les moindres carrés
    généralisés.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Puisque $\varmat{\vepsb} \neq \sigma^2 \mat{I}$, l'estimateur
      des moindres carrés généralisés a une plus faible variance que
      l'estimateur des moindres carrés ordinaires.
    \item On sait que $\{\varepsilon_t\} \sim \text{AR}(1)$ avec $\phi
      = 0,8$ et $\sigma^2 = 9$. Par conséquent,
      \begin{align*}
        \gamma_{\varepsilon}(h)
        &= \frac{\sigma^2}{1 - \phi^2}\, \phi^h \\
        &= 25 (0,8)^h, \quad h = 0, 1, 2, \dots
      \end{align*}
      d'où la matrice $\mat{V} = \varmat{\vepsb}$ à utiliser dans les
      moindres carrés généralisés est
      \begin{displaymath}
        \mat{V} = 25
        \begin{bmatrix}
          1 & 0,8 & 0,8^2 & \dots & 0,8^{n-1} \\
          0,8 & 1 & 0,8 & \dots & 0,8^{n-2} \\
          \vdots & & \ddots & & \vdots \\
          0,8^{n-1} & 0,8^{n-2} & 0,8^{n-3} & \dots & 1
        \end{bmatrix}.
      \end{displaymath}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Le processus $\text{SARIMA}(p, d, q) \times (P, D, Q)_s$ est défini
  comme la solution stationnaire $\{X_t\}$ des équations
  \begin{displaymath}
    \phi(B) \Phi(B^s) W_t = \theta(B) \Theta(B^s) Z_t, \quad
    W_t = \nabla^d \nabla_s^D X_t,
  \end{displaymath}
  où $\{Z_t\} \sim \text{WN}(0, \sigma^2)$ et $\phi(z)$, $\Phi(z)$,
  $\theta(z)$ et $\Theta(z)$ sont des polynômes de degré $p$, $P$, $q$
  et $Q$, respectivement. Ainsi, si le processus $\{X_t\}$ une
  tendance et de la saisonnalité de période $s$, alors $\{W_t\}$ est
  le processus stationnaire obtenu en éliminant la tendance puis la
  saisonnalité à l'aide de $d$ différences (d'ordre 1) et $D$
  (normalement $D = 1$) différences d'ordre $s$. De plus,
  \begin{displaymath}
    \Phi(B^s) = 1 - \Phi_1 B^s - \Phi_2 B^{2s} - \dots - \Phi_P B^{Ps}
  \end{displaymath}
  est une composante AR saisonnière (entre les années) et
  \begin{displaymath}
    \Theta(B^s) = 1 - \Theta_1 B^s - \Theta_2 B^{2s} - \dots - \Theta_Q B^{Qs}
  \end{displaymath}
  est une composante MA saisonnière. Remarquer que l'on peut obtenir
  un modèle saisonnier même si $D = 0$, en autant que $P > 0$ ou $Q >
  0$.

  Trouver l'ordre des modèles SARIMA ci-dessous.
  \begin{enumerate}
  \item $(1 - B)(1 - B^{12}) X_t = (1 + 0,5 B)(1 - 0,6 B^{12}) Z_t$
  \item $(1 - 0,7 B)(1 - B^{12}) X_t = (1 + 0,45 B^{12}) Z_t$
  \item $(1 - 0,7 B)(1 + 0,3 B^4)(1 - B) X_t = (1 + 0,37 B^4) Z_t$
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item SARIMA$(0, 1, 1) \times (0, 1, 1)_{12}$
    \item SARIMA$(1, 0, 0) \times (0, 1, 1)_{12}$
    \item SARIMA$(1, 1, 0) \times (1, 0, 1)_4$
    \end{enumerate}
  \end{rep}
\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%%%
%%% Insérer les réponses
%%%
\input{reponses-stationnarite}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_methodes_statistiques"
%%% End:
